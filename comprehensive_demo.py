#!/usr/bin/env python3
"""
å…¨é¢çš„å¤šæä¾›å•†AIæœåŠ¡æ¼”ç¤º
Comprehensive Multi-Provider AI Service Demo
"""

import asyncio
import os
import sys
import time
from typing import List, Dict, Any
from dotenv import load_dotenv

# Add src to path
sys.path.insert(0, 'src')

from multi_agent_service.models.model_service import ModelConfig, ModelRequest
from multi_agent_service.models.enums import ModelProvider
from multi_agent_service.services.model_router import ModelRouter, LoadBalancingStrategy
from multi_agent_service.services import providers


def print_header(title: str, width: int = 80):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{'='*width}")
    print(f"ğŸš€ {title.center(width-4)}")
    print(f"{'='*width}")


def print_section(title: str, width: int = 60):
    """æ‰“å°ç« èŠ‚"""
    print(f"\n{'-'*width}")
    print(f"ğŸ“‹ {title}")
    print(f"{'-'*width}")


async def setup_router() -> ModelRouter:
    """è®¾ç½®è·¯ç”±å™¨"""
    load_dotenv()
    
    configs = [
        ModelConfig(
            provider=ModelProvider.QWEN,
            model_name="qwen-turbo",
            api_key=os.getenv("QWEN_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            priority=1,
            enabled=True
        ),
        ModelConfig(
            provider=ModelProvider.DEEPSEEK,
            model_name="deepseek-chat",
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1",
            priority=2,
            enabled=True
        ),
        ModelConfig(
            provider=ModelProvider.GLM,
            model_name="glm-4-flash",
            api_key=os.getenv("GLM_API_KEY"),
            base_url="https://open.bigmodel.cn/api/paas/v4",
            priority=3,
            enabled=True
        )
    ]
    
    return ModelRouter(configs, LoadBalancingStrategy.PRIORITY)


async def demo_provider_comparison():
    """æ¼”ç¤ºæä¾›å•†å¯¹æ¯”"""
    print_section("AIæä¾›å•†èƒ½åŠ›å¯¹æ¯”")
    
    router = await setup_router()
    
    try:
        questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è§£é‡Šæ·±åº¦å­¦ä¹ çš„æ¦‚å¿µ",
            "äººå·¥æ™ºèƒ½çš„åº”ç”¨é¢†åŸŸ"
        ]
        
        for i, question in enumerate(questions, 1):
            print(f"\nğŸ” é—®é¢˜ {i}: {question}")
            print("=" * 60)
            
            # ä¸ºæ¯ä¸ªæä¾›å•†å•ç‹¬æµ‹è¯•
            for client_id, client in router.clients.items():
                provider_name = client_id.split(':')[0].upper()
                
                try:
                    request = ModelRequest(
                        messages=[{"role": "user", "content": question}],
                        max_tokens=100,
                        temperature=0.7
                    )
                    
                    start_time = time.time()
                    response = await client.chat_completion(request)
                    response_time = time.time() - start_time
                    
                    if response.choices:
                        content = response.choices[0].get("message", {}).get("content", "")
                        # æˆªæ–­é•¿å›å¤
                        if len(content) > 150:
                            content = content[:150] + "..."
                    
                    print(f"\nğŸ¤– {provider_name}:")
                    print(f"   å›å¤: {content}")
                    print(f"   å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                    
                    if response.usage:
                        usage = response.usage
                        total_tokens = usage.get('total_tokens', 0)
                        print(f"   Tokenä½¿ç”¨: {total_tokens}")
                    
                except Exception as e:
                    print(f"\nâŒ {provider_name}: {str(e)}")
    
    finally:
        await router.close()


async def demo_load_balancing():
    """æ¼”ç¤ºè´Ÿè½½å‡è¡¡"""
    print_section("æ™ºèƒ½è´Ÿè½½å‡è¡¡æ¼”ç¤º")
    
    strategies = [
        (LoadBalancingStrategy.PRIORITY, "ä¼˜å…ˆçº§ç­–ç•¥"),
        (LoadBalancingStrategy.ROUND_ROBIN, "è½®è¯¢ç­–ç•¥"),
        (LoadBalancingStrategy.RESPONSE_TIME, "å“åº”æ—¶é—´ç­–ç•¥")
    ]
    
    for strategy, name in strategies:
        print(f"\nğŸ¯ {name}")
        
        router = await setup_router()
        router.set_strategy(strategy)
        
        try:
            # å‘é€å¤šä¸ªè¯·æ±‚
            for i in range(3):
                request = ModelRequest(
                    messages=[{"role": "user", "content": f"è¿™æ˜¯ç¬¬{i+1}ä¸ªæµ‹è¯•è¯·æ±‚"}],
                    max_tokens=30,
                    temperature=0.7
                )
                
                response = await router.chat_completion(request)
                print(f"   è¯·æ±‚{i+1} â†’ {response.provider.value.upper()}")
        
        finally:
            await router.close()


async def demo_failover():
    """æ¼”ç¤ºæ•…éšœè½¬ç§»"""
    print_section("æ•…éšœè½¬ç§»æœºåˆ¶æ¼”ç¤º")
    
    router = await setup_router()
    
    try:
        print("ğŸ”§ åˆå§‹çŠ¶æ€: æ‰€æœ‰æä¾›å•†å¯ç”¨")
        
        # æ­£å¸¸è¯·æ±‚
        request = ModelRequest(
            messages=[{"role": "user", "content": "æµ‹è¯•æ­£å¸¸è¯·æ±‚"}],
            max_tokens=30
        )
        
        response = await router.chat_completion(request)
        print(f"âœ… æ­£å¸¸è¯·æ±‚æˆåŠŸï¼Œä½¿ç”¨: {response.provider.value.upper()}")
        
        # ç¦ç”¨ç¬¬ä¸€ä¸ªæä¾›å•†
        first_client_id = list(router.clients.keys())[0]
        first_provider = first_client_id.split(':')[0].upper()
        
        print(f"\nğŸš« ç¦ç”¨ {first_provider}")
        router.configs[first_client_id].enabled = False
        
        # æ•…éšœè½¬ç§»è¯·æ±‚
        response = await router.chat_completion(request)
        print(f"ğŸ”„ æ•…éšœè½¬ç§»æˆåŠŸï¼Œä½¿ç”¨: {response.provider.value.upper()}")
        
        # æ£€æŸ¥æ•…éšœè½¬ç§»äº‹ä»¶
        events = router.get_failover_events()
        if events:
            latest = events[-1]
            print(f"ğŸ“ è®°å½•: {latest.original_provider.value} â†’ {latest.fallback_provider.value}")
        
        # æ¢å¤æä¾›å•†
        print(f"\nğŸ”„ æ¢å¤ {first_provider}")
        router.configs[first_client_id].enabled = True
        
        response = await router.chat_completion(request)
        print(f"âœ… æ¢å¤æˆåŠŸï¼Œä½¿ç”¨: {response.provider.value.upper()}")
    
    finally:
        await router.close()


async def demo_performance():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
    print_section("æ€§èƒ½ç›‘æ§æ¼”ç¤º")
    
    router = await setup_router()
    
    try:
        # å‘é€å¤šä¸ªè¯·æ±‚æ”¶é›†æ€§èƒ½æ•°æ®
        questions = [
            "ä»€ä¹ˆæ˜¯äº‘è®¡ç®—ï¼Ÿ",
            "è§£é‡ŠåŒºå—é“¾æŠ€æœ¯",
            "äººå·¥æ™ºèƒ½çš„æœªæ¥",
            "é‡å­è®¡ç®—åŸç†",
            "5GæŠ€æœ¯ä¼˜åŠ¿"
        ]
        
        print("ğŸ“Š å‘é€æµ‹è¯•è¯·æ±‚æ”¶é›†æ€§èƒ½æ•°æ®...")
        
        for question in questions:
            request = ModelRequest(
                messages=[{"role": "user", "content": question}],
                max_tokens=50,
                temperature=0.7
            )
            
            response = await router.chat_completion(request)
            print(f"   âœ“ {response.provider.value.upper()}")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"{'æä¾›å•†':<12} {'è¯·æ±‚æ•°':<8} {'æˆåŠŸæ•°':<8} {'å¹³å‡å“åº”æ—¶é—´':<12} {'å¯ç”¨æ€§':<8}")
        print("-" * 60)
        
        metrics = router.get_metrics()
        for client_id, client_metrics in metrics.items():
            provider = client_id.split(':')[0].upper()
            total = client_metrics.get('total_requests', 0)
            success = client_metrics.get('successful_requests', 0)
            avg_time = client_metrics.get('average_response_time', 0)
            availability = client_metrics.get('availability', 0)
            
            print(f"{provider:<12} {total:<8} {success:<8} {avg_time:<12.2f} {availability:<8.1%}")
    
    finally:
        await router.close()


async def demo_health_check():
    """æ¼”ç¤ºå¥åº·æ£€æŸ¥"""
    print_section("å¥åº·æ£€æŸ¥æ¼”ç¤º")
    
    router = await setup_router()
    
    try:
        print("ğŸ¥ æ£€æŸ¥æ‰€æœ‰æä¾›å•†å¥åº·çŠ¶æ€...")
        
        health_status = await router.health_check()
        
        print(f"\nğŸ“Š å¥åº·æ£€æŸ¥ç»“æœ:")
        for client_id, is_healthy in health_status.items():
            provider = client_id.split(':')[0].upper()
            status = "âœ… å¥åº·" if is_healthy else "âŒ ä¸å¥åº·"
            print(f"   {provider}: {status}")
        
        healthy_count = sum(1 for status in health_status.values() if status)
        total_count = len(health_status)
        
        print(f"\nğŸ“ˆ å¥åº·ç»Ÿè®¡: {healthy_count}/{total_count} ä¸ªæä¾›å•†å¥åº·")
        
        if healthy_count == total_count:
            print("ğŸ‰ æ‰€æœ‰æä¾›å•†éƒ½å¤„äºå¥åº·çŠ¶æ€ï¼")
        else:
            print(f"âš ï¸  æœ‰ {total_count - healthy_count} ä¸ªæä¾›å•†éœ€è¦æ£€æŸ¥")
    
    finally:
        await router.close()


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print_header("å¤šæä¾›å•†AIæœåŠ¡å…¨é¢æ¼”ç¤º")
    
    # æ£€æŸ¥é…ç½®
    load_dotenv()
    
    providers_info = [
        ("Qwen", os.getenv("QWEN_API_KEY")),
        ("DeepSeek", os.getenv("DEEPSEEK_API_KEY")),
        ("GLM", os.getenv("GLM_API_KEY"))
    ]
    
    print("ğŸ”‘ APIé…ç½®æ£€æŸ¥:")
    valid_count = 0
    for name, key in providers_info:
        if key and key != f"your_{name.lower()}_api_key_here":
            print(f"   âœ… {name}: {key[:10]}...")
            valid_count += 1
        else:
            print(f"   âŒ {name}: æœªé…ç½®")
    
    if valid_count == 0:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„APIé…ç½®")
        return
    
    print(f"\nğŸ¯ æ‰¾åˆ° {valid_count} ä¸ªæœ‰æ•ˆé…ç½®ï¼Œå¼€å§‹æ¼”ç¤º...")
    
    # æ¼”ç¤ºåˆ—è¡¨
    demos = [
        ("AIæä¾›å•†èƒ½åŠ›å¯¹æ¯”", demo_provider_comparison),
        ("æ™ºèƒ½è´Ÿè½½å‡è¡¡", demo_load_balancing),
        ("æ•…éšœè½¬ç§»æœºåˆ¶", demo_failover),
        ("æ€§èƒ½ç›‘æ§", demo_performance),
        ("å¥åº·æ£€æŸ¥", demo_health_check)
    ]
    
    results = []
    total_start_time = time.time()
    
    for demo_name, demo_func in demos:
        try:
            print_header(demo_name)
            
            start_time = time.time()
            await demo_func()
            duration = time.time() - start_time
            
            print(f"\nâœ… {demo_name} å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)")
            results.append((demo_name, True, duration))
            
        except Exception as e:
            print(f"\nâŒ {demo_name} å¤±è´¥: {str(e)}")
            results.append((demo_name, False, 0))
    
    # æ€»ç»“
    total_duration = time.time() - total_start_time
    
    print_header("æ¼”ç¤ºæ€»ç»“")
    
    successful = sum(1 for _, success, _ in results if success)
    total = len(results)
    
    print(f"ğŸ“Š æ¼”ç¤ºç»“æœ: {successful}/{total} æˆåŠŸ")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’")
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for demo_name, success, duration in results:
        status = "âœ…" if success else "âŒ"
        time_info = f"({duration:.2f}s)" if success else ""
        print(f"   {status} {demo_name} {time_info}")
    
    if successful == total:
        print(f"\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºæˆåŠŸï¼å¤šæä¾›å•†AIæœåŠ¡ç³»ç»Ÿå®Œå…¨å°±ç»ªï¼")
        print(f"\nğŸš€ ç³»ç»Ÿç‰¹æ€§:")
        print(f"   â€¢ æ”¯æŒ Qwenã€DeepSeekã€GLM ä¸‰å¤§AIæä¾›å•†")
        print(f"   â€¢ OpenAIå…¼å®¹APIæ ¼å¼")
        print(f"   â€¢ æ™ºèƒ½è´Ÿè½½å‡è¡¡ (ä¼˜å…ˆçº§/è½®è¯¢/å“åº”æ—¶é—´)")
        print(f"   â€¢ è‡ªåŠ¨æ•…éšœè½¬ç§»æœºåˆ¶")
        print(f"   â€¢ å®æ—¶æ€§èƒ½ç›‘æ§")
        print(f"   â€¢ å¥åº·æ£€æŸ¥åŠŸèƒ½")
        print(f"   â€¢ ç»Ÿä¸€é”™è¯¯å¤„ç†")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æ¼”ç¤ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’Œé…ç½®")


if __name__ == "__main__":
    asyncio.run(main())