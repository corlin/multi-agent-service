# Patent MVP System æ•…éšœæ’é™¤å’Œè¿ç»´æŒ‡å—

## ç›®å½•

1. [å¸¸è§é—®é¢˜è¯Šæ–­](#å¸¸è§é—®é¢˜è¯Šæ–­)
2. [ç³»ç»Ÿç›‘æ§](#ç³»ç»Ÿç›‘æ§)
3. [æ—¥å¿—åˆ†æ](#æ—¥å¿—åˆ†æ)
4. [æ€§èƒ½è°ƒä¼˜](#æ€§èƒ½è°ƒä¼˜)
5. [å¤‡ä»½å’Œæ¢å¤](#å¤‡ä»½å’Œæ¢å¤)
6. [å®‰å…¨ç»´æŠ¤](#å®‰å…¨ç»´æŠ¤)
7. [å‡çº§æŒ‡å—](#å‡çº§æŒ‡å—)
8. [åº”æ€¥å“åº”](#åº”æ€¥å“åº”)

## å¸¸è§é—®é¢˜è¯Šæ–­

### 1. å¯åŠ¨é—®é¢˜

#### é—®é¢˜: uvå‘½ä»¤æœªæ‰¾åˆ°

**ç—‡çŠ¶**:
```bash
bash: uv: command not found
```

**è¯Šæ–­æ­¥éª¤**:
1. æ£€æŸ¥uvæ˜¯å¦å·²å®‰è£…
2. éªŒè¯PATHç¯å¢ƒå˜é‡
3. æ£€æŸ¥shellé…ç½®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®‰è£…uv (Linux/macOS)
curl -LsSf https://astral.sh/uv/install.sh | sh

# å®‰è£…uv (Windows)
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# é‡æ–°åŠ è½½shellé…ç½®
source ~/.bashrc  # æˆ– ~/.zshrc

# éªŒè¯å®‰è£…
uv --version
```

#### é—®é¢˜: Pythonç‰ˆæœ¬ä¸å…¼å®¹

**ç—‡çŠ¶**:
```
ERROR: Python 3.11 is not supported. Python 3.12+ is required.
```

**è¯Šæ–­æ­¥éª¤**:
```bash
python --version
python3 --version
which python
which python3
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨pyenvå®‰è£…Python 3.12
pyenv install 3.12.1
pyenv global 3.12.1

# æˆ–ä½¿ç”¨ç³»ç»ŸåŒ…ç®¡ç†å™¨
# Ubuntu/Debian
sudo apt update
sudo apt install python3.12

# macOS
brew install python@3.12
```

#### é—®é¢˜: ä¾èµ–å®‰è£…å¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: Failed to resolve dependencies
```

**è¯Šæ–­æ­¥éª¤**:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. éªŒè¯pyproject.tomlæ–‡ä»¶
3. æ£€æŸ¥uvç¼“å­˜

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ¸…ç†uvç¼“å­˜
uv cache clean

# é‡æ–°åŒæ­¥ä¾èµ–
uv sync --reinstall

# å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨å®‰è£…
uv add fastapi uvicorn
```

### 2. æ•°æ®åº“è¿æ¥é—®é¢˜

#### é—®é¢˜: æ•°æ®åº“è¿æ¥å¤±è´¥

**ç—‡çŠ¶**:
```
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not connect to server
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
sudo systemctl status postgresql

# æµ‹è¯•è¿æ¥
psql -h localhost -U patent_user -d patent_db

# æ£€æŸ¥ç«¯å£
netstat -tuln | grep 5432
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯åŠ¨PostgreSQLæœåŠ¡
sudo systemctl start postgresql
sudo systemctl enable postgresql

# åˆ›å»ºæ•°æ®åº“å’Œç”¨æˆ·
sudo -u postgres psql
CREATE DATABASE patent_db;
CREATE USER patent_user WITH PASSWORD 'patent_pass';
GRANT ALL PRIVILEGES ON DATABASE patent_db TO patent_user;
\q

# æ›´æ–°è¿æ¥å­—ç¬¦ä¸²
export DATABASE_URL="postgresql://patent_user:patent_pass@localhost:5432/patent_db"
```

#### é—®é¢˜: æ•°æ®åº“è¿ç§»å¤±è´¥

**ç—‡çŠ¶**:
```
alembic.util.exc.CommandError: Can't locate revision identified by 'head'
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åˆå§‹åŒ–Alembic
uv run alembic init alembic

# åˆ›å»ºåˆå§‹è¿ç§»
uv run alembic revision --autogenerate -m "Initial migration"

# åº”ç”¨è¿ç§»
uv run alembic upgrade head
```

### 3. Redisè¿æ¥é—®é¢˜

#### é—®é¢˜: Redisè¿æ¥å¤±è´¥

**ç—‡çŠ¶**:
```
redis.exceptions.ConnectionError: Error 111 connecting to localhost:6379
```

**è¯Šæ–­æ­¥éª¤**:
```bash
# æ£€æŸ¥RedisæœåŠ¡çŠ¶æ€
sudo systemctl status redis

# æµ‹è¯•è¿æ¥
redis-cli ping

# æ£€æŸ¥é…ç½®
redis-cli config get "*"
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å¯åŠ¨RedisæœåŠ¡
sudo systemctl start redis
sudo systemctl enable redis

# å¦‚æœä½¿ç”¨Docker
docker run -d -p 6379:6379 redis:alpine

# æµ‹è¯•è¿æ¥
redis-cli ping
# åº”è¯¥è¿”å› PONG
```

### 4. Agentåˆå§‹åŒ–é—®é¢˜

#### é—®é¢˜: Agentæ³¨å†Œå¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: Failed to register agent 'patent_coordinator_agent'
```

**è¯Šæ–­æ­¥éª¤**:
1. æ£€æŸ¥Agenté…ç½®æ–‡ä»¶
2. éªŒè¯Agentç±»è·¯å¾„
3. æ£€æŸ¥ä¾èµ–é¡¹

**è§£å†³æ–¹æ¡ˆ**:
```bash
# éªŒè¯é…ç½®æ–‡ä»¶è¯­æ³•
python -m json.tool config/patent_agents.json

# æ£€æŸ¥Agentç±»æ˜¯å¦å­˜åœ¨
uv run python -c "
from src.multi_agent_service.agents.patent.coordinator_agent import PatentCoordinatorAgent
print('Agent class found')
"

# é‡æ–°åŠ è½½é…ç½®
curl -X POST http://localhost:8000/api/v1/config/reload
```

### 5. å¤–éƒ¨APIé—®é¢˜

#### é—®é¢˜: CNKI APIè°ƒç”¨å¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: CNKI API request failed with status 401
```

**è¯Šæ–­æ­¥éª¤**:
1. æ£€æŸ¥APIå¯†é’¥
2. éªŒè¯ç½‘ç»œè¿æ¥
3. æ£€æŸ¥APIé…é¢

**è§£å†³æ–¹æ¡ˆ**:
```bash
# éªŒè¯APIå¯†é’¥
curl -H "Authorization: Bearer $CNKI_API_KEY" \
     "https://api.cnki.net/v1/test"

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $CNKI_API_KEY

# æ›´æ–°é…ç½®
export CNKI_API_KEY="your_valid_api_key"
```

## ç³»ç»Ÿç›‘æ§

### 1. å¥åº·æ£€æŸ¥

#### åŸºç¡€å¥åº·æ£€æŸ¥

```bash
# ç³»ç»Ÿå¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# è¯¦ç»†ç³»ç»ŸçŠ¶æ€
curl http://localhost:8000/api/v1/system/status

# AgentçŠ¶æ€æ£€æŸ¥
curl http://localhost:8000/api/v1/agents/status
```

#### è‡ªåŠ¨åŒ–å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# health_monitor.sh

HEALTH_URL="http://localhost:8000/health"
STATUS_URL="http://localhost:8000/api/v1/system/status"
LOG_FILE="/var/log/patent-mvp/health.log"

check_health() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # åŸºç¡€å¥åº·æ£€æŸ¥
    if curl -f -s "$HEALTH_URL" > /dev/null; then
        echo "$timestamp - Health check: OK" >> "$LOG_FILE"
    else
        echo "$timestamp - Health check: FAILED" >> "$LOG_FILE"
        send_alert "Health check failed"
        return 1
    fi
    
    # ç³»ç»ŸçŠ¶æ€æ£€æŸ¥
    local status=$(curl -s "$STATUS_URL" | jq -r '.status')
    if [ "$status" = "healthy" ]; then
        echo "$timestamp - System status: $status" >> "$LOG_FILE"
    else
        echo "$timestamp - System status: $status (WARNING)" >> "$LOG_FILE"
        send_alert "System status warning: $status"
    fi
}

send_alert() {
    local message="$1"
    # å‘é€é‚®ä»¶æˆ–Slacké€šçŸ¥
    echo "$message" | mail -s "Patent MVP Alert" admin@company.com
}

# æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
while true; do
    check_health
    sleep 300
done
```

### 2. æ€§èƒ½ç›‘æ§

#### ç³»ç»Ÿèµ„æºç›‘æ§

```bash
#!/bin/bash
# resource_monitor.sh

LOG_FILE="/var/log/patent-mvp/resources.log"

monitor_resources() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # CPUä½¿ç”¨ç‡
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    
    # å†…å­˜ä½¿ç”¨ç‡
    local mem_usage=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
    
    # ç£ç›˜ä½¿ç”¨ç‡
    local disk_usage=$(df -h / | awk 'NR==2 {print $5}' | cut -d'%' -f1)
    
    # è®°å½•åˆ°æ—¥å¿—
    echo "$timestamp,CPU:${cpu_usage}%,MEM:${mem_usage}%,DISK:${disk_usage}%" >> "$LOG_FILE"
    
    # æ£€æŸ¥é˜ˆå€¼
    if (( $(echo "$cpu_usage > 80" | bc -l) )); then
        send_alert "High CPU usage: ${cpu_usage}%"
    fi
    
    if (( $(echo "$mem_usage > 85" | bc -l) )); then
        send_alert "High memory usage: ${mem_usage}%"
    fi
    
    if [ "$disk_usage" -gt 90 ]; then
        send_alert "High disk usage: ${disk_usage}%"
    fi
}

# æ¯åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
while true; do
    monitor_resources
    sleep 60
done
```

#### åº”ç”¨æ€§èƒ½ç›‘æ§

```python
# performance_monitor.py
import asyncio
import aiohttp
import time
import json
from datetime import datetime

class PerformanceMonitor:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.metrics = []
    
    async def check_api_performance(self):
        """æ£€æŸ¥APIå“åº”æ—¶é—´"""
        endpoints = [
            "/health",
            "/api/v1/system/status",
            "/api/v1/agents/status"
        ]
        
        async with aiohttp.ClientSession() as session:
            for endpoint in endpoints:
                start_time = time.time()
                try:
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        response_time = (time.time() - start_time) * 1000
                        status = response.status
                        
                        metric = {
                            "timestamp": datetime.now().isoformat(),
                            "endpoint": endpoint,
                            "response_time": response_time,
                            "status_code": status,
                            "success": status == 200
                        }
                        
                        self.metrics.append(metric)
                        print(f"{endpoint}: {response_time:.2f}ms (HTTP {status})")
                        
                        # è­¦å‘Šé˜ˆå€¼
                        if response_time > 5000:  # 5ç§’
                            self.send_alert(f"Slow response: {endpoint} took {response_time:.2f}ms")
                            
                except Exception as e:
                    print(f"Error checking {endpoint}: {e}")
                    self.send_alert(f"API error: {endpoint} - {e}")
    
    def send_alert(self, message):
        """å‘é€è­¦æŠ¥"""
        print(f"ALERT: {message}")
        # å®ç°é‚®ä»¶æˆ–Slacké€šçŸ¥
    
    async def run_monitoring(self):
        """è¿è¡Œç›‘æ§å¾ªç¯"""
        while True:
            await self.check_api_performance()
            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    monitor = PerformanceMonitor()
    asyncio.run(monitor.run_monitoring())
```

## æ—¥å¿—åˆ†æ

### 1. æ—¥å¿—æ–‡ä»¶ä½ç½®

```
logs/
â”œâ”€â”€ patent-mvp.log          # ä¸»åº”ç”¨æ—¥å¿—
â”œâ”€â”€ patent-mvp-error.log    # é”™è¯¯æ—¥å¿—
â”œâ”€â”€ patent-mvp-access.log   # è®¿é—®æ—¥å¿—
â”œâ”€â”€ agent-coordinator.log   # åè°ƒAgentæ—¥å¿—
â”œâ”€â”€ agent-analysis.log      # åˆ†æAgentæ—¥å¿—
â””â”€â”€ performance.log         # æ€§èƒ½æ—¥å¿—
```

### 2. æ—¥å¿—åˆ†æè„šæœ¬

#### é”™è¯¯åˆ†æè„šæœ¬

```bash
#!/bin/bash
# analyze_errors.sh

LOG_FILE="logs/patent-mvp-error.log"
REPORT_FILE="logs/error_report_$(date +%Y%m%d).txt"

echo "Patent MVP Error Analysis Report - $(date)" > "$REPORT_FILE"
echo "================================================" >> "$REPORT_FILE"

# ç»Ÿè®¡é”™è¯¯ç±»å‹
echo -e "\n1. Error Types:" >> "$REPORT_FILE"
grep "ERROR" "$LOG_FILE" | awk -F' - ' '{print $3}' | sort | uniq -c | sort -nr >> "$REPORT_FILE"

# æœ€è¿‘24å°æ—¶çš„é”™è¯¯
echo -e "\n2. Recent Errors (Last 24 hours):" >> "$REPORT_FILE"
grep "$(date -d '1 day ago' '+%Y-%m-%d')" "$LOG_FILE" | grep "ERROR" >> "$REPORT_FILE"

# é«˜é¢‘é”™è¯¯
echo -e "\n3. Most Frequent Errors:" >> "$REPORT_FILE"
grep "ERROR" "$LOG_FILE" | awk -F' - ' '{print $3}' | sort | uniq -c | sort -nr | head -10 >> "$REPORT_FILE"

# Agentç›¸å…³é”™è¯¯
echo -e "\n4. Agent Errors:" >> "$REPORT_FILE"
grep "Agent" "$LOG_FILE" | grep "ERROR" >> "$REPORT_FILE"

echo "Error analysis complete. Report saved to: $REPORT_FILE"
```

#### æ€§èƒ½åˆ†æè„šæœ¬

```python
# analyze_performance.py
import re
import json
from datetime import datetime, timedelta
from collections import defaultdict

class PerformanceAnalyzer:
    def __init__(self, log_file="logs/patent-mvp-access.log"):
        self.log_file = log_file
        self.metrics = defaultdict(list)
    
    def parse_access_logs(self):
        """è§£æè®¿é—®æ—¥å¿—"""
        with open(self.log_file, 'r') as f:
            for line in f:
                # è§£ææ—¥å¿—æ ¼å¼: timestamp - client_addr - "request" status response_time
                match = re.match(
                    r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) - ([\d.]+) - "([^"]+)" (\d+) ([\d.]+)',
                    line
                )
                if match:
                    timestamp, client, request, status, response_time = match.groups()
                    
                    # æå–ç«¯ç‚¹
                    endpoint = request.split()[1] if len(request.split()) > 1 else "unknown"
                    
                    self.metrics[endpoint].append({
                        'timestamp': datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S'),
                        'status': int(status),
                        'response_time': float(response_time),
                        'client': client
                    })
    
    def analyze_performance(self):
        """åˆ†ææ€§èƒ½æŒ‡æ ‡"""
        report = {}
        
        for endpoint, requests in self.metrics.items():
            if not requests:
                continue
                
            response_times = [r['response_time'] for r in requests]
            statuses = [r['status'] for r in requests]
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_response_time = sum(response_times) / len(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
            
            # æˆåŠŸç‡
            success_count = sum(1 for s in statuses if 200 <= s < 400)
            success_rate = success_count / len(statuses) * 100
            
            # æœ€è¿‘1å°æ—¶çš„è¯·æ±‚
            recent_requests = [
                r for r in requests 
                if r['timestamp'] > datetime.now() - timedelta(hours=1)
            ]
            
            report[endpoint] = {
                'total_requests': len(requests),
                'recent_requests': len(recent_requests),
                'avg_response_time': round(avg_response_time, 2),
                'max_response_time': round(max_response_time, 2),
                'min_response_time': round(min_response_time, 2),
                'success_rate': round(success_rate, 2)
            }
        
        return report
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        self.parse_access_logs()
        analysis = self.analyze_performance()
        
        print("Performance Analysis Report")
        print("=" * 50)
        
        for endpoint, metrics in analysis.items():
            print(f"\nEndpoint: {endpoint}")
            print(f"  Total Requests: {metrics['total_requests']}")
            print(f"  Recent Requests (1h): {metrics['recent_requests']}")
            print(f"  Avg Response Time: {metrics['avg_response_time']}ms")
            print(f"  Max Response Time: {metrics['max_response_time']}ms")
            print(f"  Success Rate: {metrics['success_rate']}%")
            
            # æ€§èƒ½è­¦å‘Š
            if metrics['avg_response_time'] > 1000:
                print(f"  âš ï¸  WARNING: High average response time")
            if metrics['success_rate'] < 95:
                print(f"  âš ï¸  WARNING: Low success rate")

if __name__ == "__main__":
    analyzer = PerformanceAnalyzer()
    analyzer.generate_report()
```

### 3. æ—¥å¿—è½®è½¬é…ç½®

```bash
# /etc/logrotate.d/patent-mvp
/var/log/patent-mvp/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    create 644 patent-mvp patent-mvp
    postrotate
        systemctl reload patent-mvp
    endscript
}
```

## æ€§èƒ½è°ƒä¼˜

### 1. æ•°æ®åº“ä¼˜åŒ–

#### PostgreSQLé…ç½®ä¼˜åŒ–

```sql
-- postgresql.conf ä¼˜åŒ–å»ºè®®

-- å†…å­˜è®¾ç½®
shared_buffers = 256MB                  -- ç³»ç»Ÿå†…å­˜çš„25%
effective_cache_size = 1GB              -- ç³»ç»Ÿå†…å­˜çš„75%
work_mem = 4MB                          -- æ¯ä¸ªæŸ¥è¯¢æ“ä½œçš„å†…å­˜
maintenance_work_mem = 64MB             -- ç»´æŠ¤æ“ä½œå†…å­˜

-- è¿æ¥è®¾ç½®
max_connections = 100                   -- æœ€å¤§è¿æ¥æ•°
shared_preload_libraries = 'pg_stat_statements'

-- æ—¥å¿—è®¾ç½®
log_min_duration_statement = 1000      -- è®°å½•æ…¢æŸ¥è¯¢(>1ç§’)
log_checkpoints = on
log_connections = on
log_disconnections = on

-- æ€§èƒ½è®¾ç½®
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100
```

#### ç´¢å¼•ä¼˜åŒ–

```sql
-- ä¸ºä¸“åˆ©è¡¨åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY idx_patents_application_date 
ON patents(application_date);

CREATE INDEX CONCURRENTLY idx_patents_keywords_gin 
ON patents USING gin(keywords);

CREATE INDEX CONCURRENTLY idx_patents_applicants_gin 
ON patents USING gin(applicants);

CREATE INDEX CONCURRENTLY idx_patents_ipc_classes 
ON patents(ipc_classes);

-- åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE patents;

-- æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT schemaname, tablename, indexname, idx_tup_read, idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_tup_read DESC;
```

### 2. Redisä¼˜åŒ–

#### Redisé…ç½®ä¼˜åŒ–

```bash
# redis.conf ä¼˜åŒ–è®¾ç½®

# å†…å­˜è®¾ç½®
maxmemory 512mb
maxmemory-policy allkeys-lru

# æŒä¹…åŒ–è®¾ç½®
save 900 1
save 300 10
save 60 10000

# ç½‘ç»œè®¾ç½®
tcp-keepalive 300
timeout 0

# æ€§èƒ½è®¾ç½®
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
set-max-intset-entries 512
```

### 3. åº”ç”¨å±‚ä¼˜åŒ–

#### uvå’ŒPythonä¼˜åŒ–

```bash
# ç¯å¢ƒå˜é‡ä¼˜åŒ–
export PYTHONOPTIMIZE=1                 # å¯ç”¨å­—èŠ‚ç ä¼˜åŒ–
export PYTHONUNBUFFERED=1               # ç¦ç”¨è¾“å‡ºç¼“å†²
export UV_COMPILE_BYTECODE=1            # uvç¼–è¯‘å­—èŠ‚ç 
export UV_LINK_MODE=copy                # uvé“¾æ¥æ¨¡å¼

# å†…å­˜ä¼˜åŒ–
export PYTHONMALLOC=malloc
export MALLOC_ARENA_MAX=2
```

#### Uvicorné…ç½®ä¼˜åŒ–

```python
# ç”Ÿäº§ç¯å¢ƒUvicorné…ç½®
uvicorn_config = {
    "host": "0.0.0.0",
    "port": 8000,
    "workers": 4,                       # CPUæ ¸å¿ƒæ•°
    "worker_class": "uvicorn.workers.UvicornWorker",
    "loop": "uvloop",                   # é«˜æ€§èƒ½äº‹ä»¶å¾ªç¯
    "http": "httptools",                # é«˜æ€§èƒ½HTTPè§£æå™¨
    "access_log": True,
    "log_level": "info",
    "keepalive": 2,
    "max_requests": 1000,               # å·¥ä½œè¿›ç¨‹é‡å¯é˜ˆå€¼
    "max_requests_jitter": 100,
    "preload_app": True,                # é¢„åŠ è½½åº”ç”¨
}
```

## å¤‡ä»½å’Œæ¢å¤

### 1. æ•°æ®åº“å¤‡ä»½

#### è‡ªåŠ¨å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup_database.sh

DB_NAME="patent_db"
DB_USER="patent_user"
BACKUP_DIR="/var/backups/patent-mvp"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/patent_db_$DATE.sql"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR"

# æ‰§è¡Œå¤‡ä»½
pg_dump -U "$DB_USER" -h localhost "$DB_NAME" > "$BACKUP_FILE"

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip "$BACKUP_FILE"

# åˆ é™¤7å¤©å‰çš„å¤‡ä»½
find "$BACKUP_DIR" -name "*.sql.gz" -mtime +7 -delete

echo "Database backup completed: ${BACKUP_FILE}.gz"
```

#### æ•°æ®åº“æ¢å¤

```bash
#!/bin/bash
# restore_database.sh

BACKUP_FILE="$1"
DB_NAME="patent_db"
DB_USER="patent_user"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file>"
    exit 1
fi

# åœæ­¢åº”ç”¨æœåŠ¡
systemctl stop patent-mvp

# åˆ é™¤ç°æœ‰æ•°æ®åº“
dropdb -U "$DB_USER" "$DB_NAME"

# åˆ›å»ºæ–°æ•°æ®åº“
createdb -U "$DB_USER" "$DB_NAME"

# æ¢å¤æ•°æ®
if [[ "$BACKUP_FILE" == *.gz ]]; then
    gunzip -c "$BACKUP_FILE" | psql -U "$DB_USER" "$DB_NAME"
else
    psql -U "$DB_USER" "$DB_NAME" < "$BACKUP_FILE"
fi

# å¯åŠ¨åº”ç”¨æœåŠ¡
systemctl start patent-mvp

echo "Database restore completed"
```

### 2. åº”ç”¨æ•°æ®å¤‡ä»½

```bash
#!/bin/bash
# backup_application.sh

APP_DIR="/opt/patent-mvp"
BACKUP_DIR="/var/backups/patent-mvp"
DATE=$(date +%Y%m%d_%H%M%S)

# å¤‡ä»½é…ç½®æ–‡ä»¶
tar -czf "$BACKUP_DIR/config_$DATE.tar.gz" \
    "$APP_DIR/config/" \
    "$APP_DIR/.env.production" \
    "$APP_DIR/logging.conf"

# å¤‡ä»½æŠ¥å‘Šæ–‡ä»¶
tar -czf "$BACKUP_DIR/reports_$DATE.tar.gz" \
    "$APP_DIR/reports/"

# å¤‡ä»½æ—¥å¿—æ–‡ä»¶
tar -czf "$BACKUP_DIR/logs_$DATE.tar.gz" \
    "$APP_DIR/logs/"

echo "Application backup completed"
```

## å®‰å…¨ç»´æŠ¤

### 1. å®‰å…¨æ£€æŸ¥æ¸…å•

#### å®šæœŸå®‰å…¨æ£€æŸ¥

```bash
#!/bin/bash
# security_check.sh

echo "Patent MVP Security Check - $(date)"
echo "=================================="

# 1. æ£€æŸ¥å¼€æ”¾ç«¯å£
echo "1. Open Ports:"
netstat -tuln | grep LISTEN

# 2. æ£€æŸ¥è¿è¡Œè¿›ç¨‹
echo -e "\n2. Running Processes:"
ps aux | grep -E "(uvicorn|python|redis|postgres)"

# 3. æ£€æŸ¥æ–‡ä»¶æƒé™
echo -e "\n3. File Permissions:"
ls -la /opt/patent-mvp/config/
ls -la /opt/patent-mvp/.env*

# 4. æ£€æŸ¥æ—¥å¿—ä¸­çš„å¯ç–‘æ´»åŠ¨
echo -e "\n4. Suspicious Activities:"
grep -i "failed\|error\|unauthorized" logs/patent-mvp-access.log | tail -10

# 5. æ£€æŸ¥ç£ç›˜ä½¿ç”¨
echo -e "\n5. Disk Usage:"
df -h

# 6. æ£€æŸ¥å†…å­˜ä½¿ç”¨
echo -e "\n6. Memory Usage:"
free -h
```

### 2. å®‰å…¨æ›´æ–°

#### ä¾èµ–æ›´æ–°è„šæœ¬

```bash
#!/bin/bash
# update_dependencies.sh

echo "Updating Patent MVP dependencies..."

# å¤‡ä»½å½“å‰ç¯å¢ƒ
cp uv.lock uv.lock.backup

# æ›´æ–°ä¾èµ–
uv sync --upgrade

# è¿è¡Œæµ‹è¯•
uv run pytest tests/ -v

if [ $? -eq 0 ]; then
    echo "Dependencies updated successfully"
else
    echo "Tests failed, rolling back..."
    cp uv.lock.backup uv.lock
    uv sync
    exit 1
fi
```

## å‡çº§æŒ‡å—

### 1. åº”ç”¨å‡çº§æµç¨‹

```bash
#!/bin/bash
# upgrade_application.sh

NEW_VERSION="$1"
BACKUP_DIR="/var/backups/patent-mvp/upgrade_$(date +%Y%m%d_%H%M%S)"

if [ -z "$NEW_VERSION" ]; then
    echo "Usage: $0 <new_version>"
    exit 1
fi

echo "Upgrading Patent MVP to version $NEW_VERSION"

# 1. åˆ›å»ºå¤‡ä»½
mkdir -p "$BACKUP_DIR"
cp -r /opt/patent-mvp "$BACKUP_DIR/"

# 2. åœæ­¢æœåŠ¡
systemctl stop patent-mvp

# 3. å¤‡ä»½æ•°æ®åº“
pg_dump -U patent_user patent_db > "$BACKUP_DIR/database.sql"

# 4. æ›´æ–°ä»£ç 
cd /opt/patent-mvp
git fetch origin
git checkout "v$NEW_VERSION"

# 5. æ›´æ–°ä¾èµ–
uv sync

# 6. è¿è¡Œè¿ç§»
uv run alembic upgrade head

# 7. è¿è¡Œæµ‹è¯•
uv run pytest tests/test_startup.py -v

# 8. å¯åŠ¨æœåŠ¡
systemctl start patent-mvp

# 9. éªŒè¯å‡çº§
sleep 10
curl -f http://localhost:8000/health

if [ $? -eq 0 ]; then
    echo "Upgrade completed successfully"
else
    echo "Upgrade failed, rolling back..."
    systemctl stop patent-mvp
    cp -r "$BACKUP_DIR/patent-mvp" /opt/
    psql -U patent_user patent_db < "$BACKUP_DIR/database.sql"
    systemctl start patent-mvp
    exit 1
fi
```

## åº”æ€¥å“åº”

### 1. æœåŠ¡ä¸­æ–­å“åº”

#### å¿«é€Ÿæ¢å¤è„šæœ¬

```bash
#!/bin/bash
# emergency_recovery.sh

echo "Emergency Recovery Procedure Started - $(date)"

# 1. æ£€æŸ¥æœåŠ¡çŠ¶æ€
systemctl status patent-mvp

# 2. æ£€æŸ¥ç«¯å£å ç”¨
netstat -tuln | grep 8000

# 3. æ£€æŸ¥è¿›ç¨‹
ps aux | grep uvicorn

# 4. å¼ºåˆ¶é‡å¯æœåŠ¡
systemctl stop patent-mvp
sleep 5
pkill -f uvicorn
sleep 2
systemctl start patent-mvp

# 5. éªŒè¯æ¢å¤
sleep 10
curl -f http://localhost:8000/health

if [ $? -eq 0 ]; then
    echo "Service recovered successfully"
else
    echo "Service recovery failed, escalating..."
    # å‘é€ç´§æ€¥é€šçŸ¥
    echo "Patent MVP service recovery failed" | mail -s "URGENT: Service Down" admin@company.com
fi
```

### 2. æ•°æ®åº“ç´§æ€¥æ¢å¤

```bash
#!/bin/bash
# emergency_db_recovery.sh

echo "Database Emergency Recovery - $(date)"

# 1. æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
systemctl status postgresql

# 2. å°è¯•è¿æ¥æ•°æ®åº“
pg_isready -h localhost -p 5432

# 3. å¦‚æœæ•°æ®åº“æ— å“åº”ï¼Œé‡å¯
if [ $? -ne 0 ]; then
    echo "Database not responding, restarting..."
    systemctl restart postgresql
    sleep 10
fi

# 4. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
psql -U patent_user -d patent_db -c "SELECT COUNT(*) FROM patents;"

# 5. å¦‚æœæ•°æ®æŸåï¼Œä»æœ€æ–°å¤‡ä»½æ¢å¤
if [ $? -ne 0 ]; then
    echo "Database corruption detected, restoring from backup..."
    LATEST_BACKUP=$(ls -t /var/backups/patent-mvp/patent_db_*.sql.gz | head -1)
    
    if [ -n "$LATEST_BACKUP" ]; then
        dropdb -U patent_user patent_db
        createdb -U patent_user patent_db
        gunzip -c "$LATEST_BACKUP" | psql -U patent_user patent_db
        echo "Database restored from: $LATEST_BACKUP"
    else
        echo "No backup found, manual intervention required"
        exit 1
    fi
fi
```

### 3. ç›‘æ§å’Œå‘Šè­¦

#### å‘Šè­¦è„šæœ¬

```python
# alert_system.py
import smtplib
import requests
from email.mime.text import MIMEText
from datetime import datetime

class AlertSystem:
    def __init__(self):
        self.smtp_server = "smtp.company.com"
        self.smtp_port = 587
        self.email_user = "alerts@company.com"
        self.email_pass = "password"
        self.admin_emails = ["admin@company.com"]
        self.slack_webhook = "https://hooks.slack.com/services/..."
    
    def send_email_alert(self, subject, message):
        """å‘é€é‚®ä»¶å‘Šè­¦"""
        try:
            msg = MIMEText(message)
            msg['Subject'] = subject
            msg['From'] = self.email_user
            msg['To'] = ', '.join(self.admin_emails)
            
            server = smtplib.SMTP(self.smtp_server, self.smtp_port)
            server.starttls()
            server.login(self.email_user, self.email_pass)
            server.send_message(msg)
            server.quit()
            
            print(f"Email alert sent: {subject}")
        except Exception as e:
            print(f"Failed to send email alert: {e}")
    
    def send_slack_alert(self, message):
        """å‘é€Slackå‘Šè­¦"""
        try:
            payload = {
                "text": f"ğŸš¨ Patent MVP Alert: {message}",
                "username": "Patent MVP Monitor",
                "icon_emoji": ":warning:"
            }
            
            response = requests.post(self.slack_webhook, json=payload)
            if response.status_code == 200:
                print("Slack alert sent")
            else:
                print(f"Failed to send Slack alert: {response.status_code}")
        except Exception as e:
            print(f"Failed to send Slack alert: {e}")
    
    def critical_alert(self, message):
        """å‘é€ä¸¥é‡å‘Šè­¦"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] CRITICAL: {message}"
        
        self.send_email_alert("CRITICAL: Patent MVP System Alert", full_message)
        self.send_slack_alert(full_message)
    
    def warning_alert(self, message):
        """å‘é€è­¦å‘Šå‘Šè­¦"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        full_message = f"[{timestamp}] WARNING: {message}"
        
        self.send_slack_alert(full_message)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    alert_system = AlertSystem()
    alert_system.critical_alert("Service is down")
    alert_system.warning_alert("High memory usage detected")
```

---

## è”ç³»æ”¯æŒ

å¦‚æœé‡åˆ°æœ¬æŒ‡å—æœªæ¶µç›–çš„é—®é¢˜ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒï¼š

- **é‚®ç®±**: support@patent-mvp.com
- **ç”µè¯**: +86-xxx-xxxx-xxxx
- **åœ¨çº¿æ”¯æŒ**: https://support.patent-mvp.com
- **ç´§æ€¥çƒ­çº¿**: +86-xxx-xxxx-xxxx (24/7)

---

**ç‰ˆæœ¬**: 1.0.0  
**æ›´æ–°æ—¥æœŸ**: 2024å¹´1æœˆ  
**ç»´æŠ¤å›¢é˜Ÿ**: Patent MVP Operations Team