#!/usr/bin/env python3
"""å‚è€ƒ google_patent_scraper çš„æ–¹æ³•è®¿é—® Google Patents."""

import asyncio
import logging
import requests
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import re
from urllib.parse import quote_plus, urljoin
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class GooglePatentsScraper:
    """åŸºäº google_patent_scraper æ–¹æ³•çš„ Google Patents è®¿é—®å™¨."""
    
    def __init__(self):
        self.base_url = "https://patents.google.com"
        self.session = requests.Session()
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def search_patents(self, query: str, num_results: int = 10) -> List[Dict[str, Any]]:
        """æœç´¢ä¸“åˆ©."""
        try:
            logger.info(f"æœç´¢ä¸“åˆ©: {query}")
            
            # æ„é€ æœç´¢URL
            search_url = f"{self.base_url}/?q={quote_plus(query)}"
            logger.info(f"æœç´¢URL: {search_url}")
            
            # å‘é€è¯·æ±‚
            response = self.session.get(search_url, timeout=30)
            response.raise_for_status()
            
            logger.info(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            logger.info(f"å“åº”å†…å®¹é•¿åº¦: {len(response.text)}")
            
            # è§£ææœç´¢ç»“æœ
            patents = self._parse_search_results(response.text, num_results)
            
            logger.info(f"æ‰¾åˆ° {len(patents)} ä¸ªä¸“åˆ©")
            return patents
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def _parse_search_results(self, html_content: str, num_results: int) -> List[Dict[str, Any]]:
        """è§£ææœç´¢ç»“æœé¡µé¢."""
        patents = []
        
        try:
            # æŸ¥æ‰¾ä¸“åˆ©é“¾æ¥æ¨¡å¼
            patent_link_pattern = r'href="(/patent/[^"]+)"'
            patent_links = re.findall(patent_link_pattern, html_content)
            
            logger.info(f"æ‰¾åˆ° {len(patent_links)} ä¸ªä¸“åˆ©é“¾æ¥")
            
            # æŸ¥æ‰¾ä¸“åˆ©æ ‡é¢˜æ¨¡å¼
            title_pattern = r'<h3[^>]*>([^<]+)</h3>'
            titles = re.findall(title_pattern, html_content)
            
            logger.info(f"æ‰¾åˆ° {len(titles)} ä¸ªæ ‡é¢˜")
            
            # ç»„åˆç»“æœ
            for i, link in enumerate(patent_links[:num_results]):
                patent = {
                    "patent_url": urljoin(self.base_url, link),
                    "patent_id": self._extract_patent_id_from_url(link),
                    "title": titles[i] if i < len(titles) else "æœªçŸ¥æ ‡é¢˜",
                    "source": "google_patents_scraper",
                    "collected_at": datetime.now().isoformat()
                }
                
                # å°è¯•è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯
                try:
                    patent_details = self.get_patent_details(patent["patent_url"])
                    patent.update(patent_details)
                except Exception as e:
                    logger.warning(f"è·å–ä¸“åˆ©è¯¦æƒ…å¤±è´¥: {e}")
                
                patents.append(patent)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™åˆ¶
                time.sleep(1)
            
            return patents
            
        except Exception as e:
            logger.error(f"è§£ææœç´¢ç»“æœå¤±è´¥: {e}")
            return []
    
    def _extract_patent_id_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–ä¸“åˆ©ID."""
        match = re.search(r'/patent/([^/\?]+)', url)
        return match.group(1) if match else "æœªçŸ¥ID"
    
    def get_patent_details(self, patent_url: str) -> Dict[str, Any]:
        """è·å–ä¸“åˆ©è¯¦ç»†ä¿¡æ¯."""
        try:
            logger.info(f"è·å–ä¸“åˆ©è¯¦æƒ…: {patent_url}")
            
            response = self.session.get(patent_url, timeout=30)
            response.raise_for_status()
            
            return self._parse_patent_details(response.text)
            
        except Exception as e:
            logger.error(f"è·å–ä¸“åˆ©è¯¦æƒ…å¤±è´¥: {e}")
            return {}
    
    def _parse_patent_details(self, html_content: str) -> Dict[str, Any]:
        """è§£æä¸“åˆ©è¯¦æƒ…é¡µé¢."""
        details = {}
        
        try:
            # æå–ä¸“åˆ©å·
            patent_number_pattern = r'<span[^>]*>([A-Z]{2}\d+[A-Z]\d*)</span>'
            patent_number_match = re.search(patent_number_pattern, html_content)
            if patent_number_match:
                details["patent_number"] = patent_number_match.group(1)
            
            # æå–æ ‡é¢˜
            title_pattern = r'<h1[^>]*>([^<]+)</h1>'
            title_match = re.search(title_pattern, html_content)
            if title_match:
                details["title"] = title_match.group(1).strip()
            
            # æå–æ‘˜è¦
            abstract_pattern = r'<div[^>]*abstract[^>]*>([^<]+)</div>'
            abstract_match = re.search(abstract_pattern, html_content, re.IGNORECASE)
            if abstract_match:
                details["abstract"] = abstract_match.group(1).strip()
            
            # æå–ç”³è¯·äºº
            assignee_pattern = r'<dd[^>]*>([^<]+)</dd>'
            assignee_matches = re.findall(assignee_pattern, html_content)
            if assignee_matches:
                details["applicants"] = [match.strip() for match in assignee_matches[:3]]
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_pattern = r'(\d{4}-\d{2}-\d{2})'
            date_match = re.search(date_pattern, html_content)
            if date_match:
                details["publication_date"] = date_match.group(1)
            
            return details
            
        except Exception as e:
            logger.error(f"è§£æä¸“åˆ©è¯¦æƒ…å¤±è´¥: {e}")
            return {}


async def test_google_patents_scraper():
    """æµ‹è¯• Google Patents çˆ¬è™«."""
    logger.info("æµ‹è¯• Google Patents çˆ¬è™«...")
    
    scraper = GooglePatentsScraper()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "artificial intelligence",
        "machine learning",
        "blockchain"
    ]
    
    all_results = {}
    
    for query in test_queries:
        logger.info(f"\n--- æµ‹è¯•æŸ¥è¯¢: {query} ---")
        
        patents = scraper.search_patents(query, num_results=3)
        all_results[query] = patents
        
        if patents:
            logger.info(f"âœ“ æ‰¾åˆ° {len(patents)} ä¸ªä¸“åˆ©")
            for i, patent in enumerate(patents, 1):
                logger.info(f"\nä¸“åˆ© {i}:")
                logger.info(f"  ID: {patent.get('patent_id', 'N/A')}")
                logger.info(f"  æ ‡é¢˜: {patent.get('title', 'N/A')}")
                logger.info(f"  ä¸“åˆ©å·: {patent.get('patent_number', 'N/A')}")
                logger.info(f"  URL: {patent.get('patent_url', 'N/A')}")
                if patent.get('applicants'):
                    logger.info(f"  ç”³è¯·äºº: {', '.join(patent['applicants'])}")
                if patent.get('abstract'):
                    logger.info(f"  æ‘˜è¦: {patent['abstract'][:100]}...")
        else:
            logger.warning(f"âœ— æœªæ‰¾åˆ° '{query}' ç›¸å…³ä¸“åˆ©")
        
        # æŸ¥è¯¢é—´éš”
        await asyncio.sleep(2)
    
    # ä¿å­˜ç»“æœ
    result_file = f"google_patents_scraper_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        logger.info(f"\nç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    # ç»Ÿè®¡
    total_patents = sum(len(patents) for patents in all_results.values())
    successful_queries = sum(1 for patents in all_results.values() if patents)
    
    logger.info(f"\n=== æµ‹è¯•ç»Ÿè®¡ ===")
    logger.info(f"æ€»æŸ¥è¯¢æ•°: {len(test_queries)}")
    logger.info(f"æˆåŠŸæŸ¥è¯¢: {successful_queries}")
    logger.info(f"æ€»ä¸“åˆ©æ•°: {total_patents}")
    logger.info(f"æˆåŠŸç‡: {successful_queries/len(test_queries)*100:.1f}%")
    
    return successful_queries > 0


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°."""
    logger.info("=== Google Patents Scraper æ–¹æ³•æµ‹è¯• ===")
    logger.info(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    success = await test_google_patents_scraper()
    
    if success:
        logger.info("\nğŸ‰ Google Patents çˆ¬è™«æ–¹æ³•æµ‹è¯•æˆåŠŸ!")
        return 0
    else:
        logger.error("\nâŒ Google Patents çˆ¬è™«æ–¹æ³•æµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)