"""Patent data collection agent implementation."""

import asyncio
import aiohttp
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

from .base import PatentBaseAgent
from ...models.base import UserRequest, AgentResponse, Action
from ...models.config import AgentConfig
from ...models.enums import AgentType
from ...services.model_client import BaseModelClient

# å¯¼å…¥ PatentsView ç›¸å…³æ¨¡å—
from ...patent.services.patentsview_service import PatentsViewService
from ...patent.config.patentsview_config import PatentsViewAPIConfig
from ...patent.models.patentsview_data import PatentsViewSearchResult, PatentRecord
from ...patent.models.requests import PatentAnalysisRequest


logger = logging.getLogger(__name__)


class PatentDataCollectionAgent(PatentBaseAgent):
    """ä¸“åˆ©æ•°æ®æ”¶é›†Agentï¼Œè´Ÿè´£ä»PatentsView APIæ”¶é›†ä¸“åˆ©ä¿¡æ¯."""
    
    agent_type = AgentType.PATENT_DATA_COLLECTION
    
    def __init__(self, config: AgentConfig, model_client: BaseModelClient):
        """åˆå§‹åŒ–ä¸“åˆ©æ•°æ®æ”¶é›†Agent."""
        super().__init__(config, model_client)
        
        # æ•°æ®æ”¶é›†ç›¸å…³å…³é”®è¯
        self.collection_keywords = [
            "æ”¶é›†", "è·å–", "é‡‡é›†", "æŠ“å–", "ä¸‹è½½", "å¯¼å…¥", "åŒæ­¥",
            "collect", "gather", "fetch", "retrieve", "download", "import", "sync"
        ]
        
        # åˆå§‹åŒ– PatentsView æœåŠ¡
        self.patentsview_config = PatentsViewAPIConfig.from_env()
        self.patentsview_service = None
        
        # æ•°æ®æºé…ç½® - ä¸“æ³¨äº PatentsView API
        self.data_sources_config = {
            'patentsview_api': {
                'base_url': self.patentsview_config.base_url,
                'rate_limit': 5,
                'timeout': self.patentsview_config.timeout,
                'enabled': True,
                'service_class': PatentsViewService
            }
        }
        
        # æ•°æ®è´¨é‡é…ç½®
        self.quality_config = {
            "min_title_length": 5,
            "required_fields": ["patent_title", "patent_id"],
            "max_duplicates_ratio": 0.1
        }
        
        # åˆå§‹åŒ–ä¸“åˆ©æ•°æ®æº
        self._patent_data_sources = {}
        self._patent_data_sources['patentsview_api'] = {
            'enabled': True,
            'priority': 1,
            'rate_limit': 5,
            'timeout': self.patentsview_config.timeout
        }
    
    async def can_handle_request(self, request: UserRequest) -> float:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†æ•°æ®æ”¶é›†ç›¸å…³è¯·æ±‚."""
        # å…ˆè°ƒç”¨çˆ¶ç±»çš„ä¸“åˆ©ç›¸å…³åˆ¤æ–­
        base_confidence = await super().can_handle_request(request)
        
        content = request.content.lower()
        
        # æ£€æŸ¥æ•°æ®æ”¶é›†å…³é”®è¯
        collection_matches = sum(1 for keyword in self.collection_keywords if keyword in content)
        collection_score = min(collection_matches * 0.3, 0.6)
        
        # æ£€æŸ¥æ•°æ®æ”¶é›†ç‰¹å®šæ¨¡å¼
        collection_patterns = [
            r"(æ”¶é›†|è·å–|é‡‡é›†).*?(ä¸“åˆ©|æ•°æ®|ä¿¡æ¯)",
            r"(ä¸‹è½½|å¯¼å…¥|åŒæ­¥).*?(ä¸“åˆ©|æ–‡ä»¶|æ•°æ®)",
            r"(æŠ“å–|çˆ¬å–).*?(ä¸“åˆ©|ç½‘ç«™|æ•°æ®)",
            r"(collect|gather|fetch).*?(patent|data|information)",
            r"(download|import|sync).*?(patent|file|data)"
        ]
        
        pattern_score = 0
        for pattern in collection_patterns:
            if re.search(pattern, content):
                pattern_score += 0.25
        
        # ç»¼åˆè¯„åˆ†
        total_score = min(base_confidence + collection_score + pattern_score, 1.0)
        
        return max(total_score, 0.0)
    
    async def get_capabilities(self) -> List[str]:
        """è·å–æ•°æ®æ”¶é›†Agentçš„èƒ½åŠ›åˆ—è¡¨."""
        base_capabilities = await super().get_capabilities()
        collection_capabilities = [
            "PatentsView API ä¸“åˆ©æ•°æ®æ”¶é›†",
            "å¤šç«¯ç‚¹æ•°æ®æ•´åˆ",
            "ä¸“åˆ©åŸºç¡€ä¿¡æ¯æ”¶é›†",
            "ä¸“åˆ©æ‘˜è¦å’Œæƒåˆ©è¦æ±‚æ”¶é›†",
            "ä¸“åˆ©æƒäººå’Œå‘æ˜äººä¿¡æ¯æ”¶é›†",
            "ä¸“åˆ©åˆ†ç±»ä¿¡æ¯æ”¶é›†",
            "æ•°æ®æ ‡å‡†åŒ–å’Œæ¸…æ´—",
            "æ•°æ®å»é‡å’Œè´¨é‡æ§åˆ¶",
            "æ‰¹é‡æ•°æ®å¤„ç†",
            "å¢é‡æ•°æ®åŒæ­¥"
        ]
        return base_capabilities + collection_capabilities
    
    async def estimate_processing_time(self, request: UserRequest) -> int:
        """ä¼°ç®—æ•°æ®æ”¶é›†å¤„ç†æ—¶é—´."""
        content = request.content.lower()
        
        # å°é‡æ•°æ®ï¼š20-30ç§’
        if any(word in content for word in ["å°‘é‡", "å‡ ä¸ª", "10", "20"]):
            return 25
        
        # ä¸­é‡æ•°æ®ï¼š45-60ç§’
        if any(word in content for word in ["ä¸­ç­‰", "100", "200"]):
            return 50
        
        # å¤§é‡æ•°æ®ï¼š90-120ç§’
        if any(word in content for word in ["å¤§é‡", "å…¨éƒ¨", "1000", "æ‰¹é‡"]):
            return 105
        
        # é»˜è®¤æ”¶é›†æ—¶é—´
        return 40
    
    async def _process_request_specific(self, request: UserRequest) -> AgentResponse:
        """å¤„ç†æ•°æ®æ”¶é›†ç›¸å…³çš„å…·ä½“è¯·æ±‚."""
        start_time = datetime.now()
        
        try:
            # è§£ææ”¶é›†è¯·æ±‚
            collection_params = self._parse_collection_request(request.content)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_collection_cache_key(collection_params)
            cached_result = await self.get_from_cache(cache_key)
            
            if cached_result:
                self.logger.info("Returning cached collection results")
                return AgentResponse(
                    agent_id=self.agent_id,
                    agent_type=self.agent_type,
                    response_content=cached_result["response_content"],
                    confidence=0.9,
                    metadata={
                        **cached_result.get("metadata", {}),
                        "from_cache": True
                    }
                )
            
            # æ‰§è¡Œæ•°æ®æ”¶é›†
            collection_results = await self._execute_data_collection(collection_params)
            
            # æ•°æ®å¤„ç†å’Œè´¨é‡æ§åˆ¶
            processed_data = await self._process_and_validate_data(collection_results)
            
            # ç”Ÿæˆå“åº”å†…å®¹
            response_content = await self._generate_collection_response(
                collection_params, processed_data
            )
            
            # ç”Ÿæˆåç»­åŠ¨ä½œ
            next_actions = self._generate_collection_actions(collection_params, processed_data)
            
            # ç¼“å­˜ç»“æœ
            result_data = {
                "response_content": response_content,
                "metadata": {
                    "collection_params": collection_params,
                    "total_collected": len(processed_data.get("patents", [])),
                    "processing_time": (datetime.now() - start_time).total_seconds(),
                    "sources_used": list(collection_results.keys()),
                    "data_quality_score": processed_data.get("quality_score", 0.0)
                }
            }
            await self.save_to_cache(cache_key, result_data)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("data_collection", duration, True)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=response_content,
                confidence=0.85,
                next_actions=next_actions,
                collaboration_needed=False,
                metadata=result_data["metadata"]
            )
            
        except Exception as e:
            self.logger.error(f"Error processing collection request: {str(e)}")
            
            # è®°å½•å¤±è´¥æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("data_collection", duration, False)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=f"æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
                confidence=0.0,
                collaboration_needed=True,
                metadata={
                    "error": str(e),
                    "processing_time": duration
                }
            )
    
    def _parse_collection_request(self, content: str) -> Dict[str, Any]:
        """è§£ææ•°æ®æ”¶é›†è¯·æ±‚å‚æ•°."""
        params = {
            "keywords": [],
            "sources": list(self.data_sources_config.keys()),
            "limit": 100,
            "date_range": None,
            "collection_type": "comprehensive",
            "quality_level": "standard"
        }
        
        content_lower = content.lower()
        
        # æå–å…³é”®è¯
        keywords = []
        
        # æŸ¥æ‰¾å¼•å·ä¸­çš„å…³é”®è¯
        quoted_keywords = re.findall(r'["""\'](.*?)["""\']', content)
        keywords.extend(quoted_keywords)
        
        # æŸ¥æ‰¾å¸¸è§çš„æŠ€æœ¯æœ¯è¯­
        tech_patterns = [
            r'(äººå·¥æ™ºèƒ½|AI|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ )',
            r'(åŒºå—é“¾|blockchain)',
            r'(ç‰©è”ç½‘|IoT)',
            r'(5G|é€šä¿¡æŠ€æœ¯)',
            r'(æ–°èƒ½æº|ç”µæ± æŠ€æœ¯)',
            r'(ç”Ÿç‰©æŠ€æœ¯|åŸºå› )',
            r'(èŠ¯ç‰‡|åŠå¯¼ä½“)',
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
        if not keywords:
            stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "ç­‰", "æ”¶é›†", "ä¸“åˆ©"}
            words = content.split()
            keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        params["keywords"] = keywords[:5]  # é™åˆ¶å…³é”®è¯æ•°é‡
        
        # åˆ¤æ–­æ”¶é›†ç±»å‹
        if any(word in content_lower for word in ["å¿«é€Ÿ", "ç®€å•", "åŸºç¡€"]):
            params["collection_type"] = "quick"
            params["limit"] = 50
        elif any(word in content_lower for word in ["å…¨é¢", "è¯¦ç»†", "å®Œæ•´"]):
            params["collection_type"] = "comprehensive"
            params["limit"] = 200
        elif any(word in content_lower for word in ["æ·±åº¦", "ä¸“ä¸š", "é«˜è´¨é‡"]):
            params["collection_type"] = "deep"
            params["quality_level"] = "high"
        
        # åˆ¤æ–­æ•°æ®æºåå¥½ - é»˜è®¤ä½¿ç”¨ PatentsView
        if "google" in content_lower:
            params["sources"] = ["google_patents"]
        elif "cnipa" in content_lower or "ä¸­å›½" in content_lower:
            params["sources"] = ["cnipa_api"]
        elif "patentsview" in content_lower or "ç¾å›½" in content_lower:
            params["sources"] = ["patentsview_api"]
        else:
            # é»˜è®¤ä½¿ç”¨ PatentsView API
            params["sources"] = ["patentsview_api"]
        
        # æå–æ•°é‡é™åˆ¶
        limit_match = re.search(r'(\d+).*?(ä¸ª|æ¡|ä»¶)', content)
        if limit_match:
            params["limit"] = min(int(limit_match.group(1)), 500)  # æœ€å¤§é™åˆ¶500
        
        # æå–æ—¶é—´èŒƒå›´
        year_matches = re.findall(r'(\d{4})', content)
        if len(year_matches) >= 2:
            years = [int(y) for y in year_matches if 2000 <= int(y) <= 2024]
            if len(years) >= 2:
                params["date_range"] = {
                    "start_year": min(years),
                    "end_year": max(years)
                }
        
        return params
    
    def _generate_collection_cache_key(self, collection_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ”¶é›†ç¼“å­˜é”®."""
        key_parts = [
            "collection",
            "_".join(sorted(collection_params["keywords"])),
            "_".join(sorted(collection_params["sources"])),
            str(collection_params["limit"]),
            collection_params["collection_type"]
        ]
        
        if collection_params.get("date_range"):
            date_range = collection_params["date_range"]
            key_parts.append(f"{date_range['start_year']}-{date_range['end_year']}")
        
        return "_".join(key_parts).replace(" ", "_")
    
    async def _execute_data_collection(self, collection_params: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """æ‰§è¡Œæ•°æ®æ”¶é›†."""
        results = {}
        
        # åˆå§‹åŒ– PatentsView æœåŠ¡
        if not self.patentsview_service:
            self.patentsview_service = PatentsViewService(
                api_key=self.patentsview_config.api_key,
                base_url=self.patentsview_config.base_url
            )
        
        try:
            # ä½¿ç”¨ PatentsView API è¿›è¡Œæ•°æ®æ”¶é›†
            patentsview_data = await self._collect_from_patentsview(collection_params)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            patents_data = self._convert_patentsview_to_standard_format(patentsview_data)
            results["patentsview_api"] = patents_data
            
            # å¦‚æœå¯ç”¨äº†å…¶ä»–æ•°æ®æºï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            for source in collection_params["sources"]:
                if (source != "patentsview_api" and 
                    source in self.data_sources_config and 
                    self.data_sources_config[source]["enabled"]):
                    
                    # ä¸ºå…¶ä»–æ•°æ®æºä¿ç•™åŸæœ‰çš„æ”¶é›†é€»è¾‘
                    try:
                        source_data = await self._collect_from_source(source, collection_params)
                        results[source] = source_data or []
                    except Exception as e:
                        self.logger.error(f"Collection failed for {source}: {str(e)}")
                        results[source] = []
            
            return results
            
        except Exception as e:
            self.logger.error(f"PatentsView collection failed: {str(e)}")
            # å¦‚æœ PatentsView å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
            results["patentsview_api"] = []
            return results
    
    async def _collect_from_source(self, source: str, collection_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»æŒ‡å®šæ•°æ®æºæ”¶é›†æ•°æ®."""
        try:
            keywords = collection_params["keywords"]
            limit = collection_params["limit"]
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶
            return await self.with_retry(
                self._fetch_from_source,
                max_retries=3,
                delay=1.0,
                source=source,
                keywords=keywords,
                limit=limit
            )
            
        except Exception as e:
            self.logger.error(f"Error collecting from {source}: {str(e)}")
            return []
    
    async def _collect_from_patentsview(self, collection_params: Dict[str, Any]) -> Dict[str, Any]:
        """ä» PatentsView API æ”¶é›†æ•°æ®."""
        try:
            keywords = collection_params.get("keywords", [])
            limit = collection_params.get("limit", 100)
            date_range = collection_params.get("date_range")
            
            self.logger.info(f"Collecting from PatentsView with keywords: {keywords}, limit: {limit}")
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            query = await self._build_patentsview_query(keywords, date_range)
            
            # æ‰§è¡Œå¤šä¸ªæœç´¢ä»»åŠ¡
            search_tasks = []
            
            # åŸºç¡€ä¸“åˆ©æœç´¢
            search_tasks.append(self._search_patents_direct(query, limit))
            
            # ä¸“åˆ©æ–‡æœ¬æœç´¢
            search_tasks.append(self._search_patent_texts_direct(query))
            
            # æ‰§è¡Œå¹¶è¡Œæœç´¢
            results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # æ•´åˆæœç´¢ç»“æœ
            integrated_data = await self._integrate_patentsview_results(results)
            
            self.logger.info(f"Collected {len(integrated_data.get('patents', []))} patents from PatentsView")
            return integrated_data
            
        except Exception as e:
            self.logger.error(f"Error collecting from PatentsView: {str(e)}")
            # è¿”å›ç©ºçš„ç»“æœ
            return {"patents": [], "patent_texts": {"summaries": [], "claims": []}, "errors": [str(e)]}
    
    def _convert_patentsview_to_standard_format(self, integrated_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å°† PatentsView æœç´¢ç»“æœè½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼."""
        standard_patents = []
        
        try:
            patents = integrated_data.get("patents", [])
            patent_texts = integrated_data.get("patent_texts", {})
            summaries = patent_texts.get("summaries", [])
            claims = patent_texts.get("claims", [])
            
            # åˆ›å»ºæ‘˜è¦å’Œæƒåˆ©è¦æ±‚çš„å­—å…¸
            summary_dict = {summary.get("patent_id"): summary.get("summary_text", "") 
                          for summary in summaries if summary.get("patent_id")}
            
            claims_dict = {}
            for claim in claims:
                patent_id = claim.get("patent_id")
                if patent_id:
                    if patent_id not in claims_dict:
                        claims_dict[patent_id] = []
                    claims_dict[patent_id].append({
                        "sequence": claim.get("claim_sequence", 0),
                        "text": claim.get("claim_text", "")
                    })
            
            for patent in patents:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                patent_id = patent.get("patent_id", "")
                standard_patent = {
                    "patent_id": patent_id,
                    "patent_number": patent.get("patent_number") or patent_id,
                    "title": patent.get("patent_title") or "æœªçŸ¥æ ‡é¢˜",
                    "abstract": patent.get("patent_abstract") or "",
                    "applicants": [patent.get("assignee_organization")] if patent.get("assignee_organization") else [],
                    "inventors": [],
                    "application_date": patent.get("patent_date") or "",
                    "publication_date": patent.get("patent_date") or "",
                    "ipc_classes": [patent.get("ipc_class")] if patent.get("ipc_class") else [],
                    "cpc_classes": [patent.get("cpc_class")] if patent.get("cpc_class") else [],
                    "country": patent.get("assignee_country") or "US",
                    "status": "å·²å…¬å¼€",
                    "source": "patentsview_api",
                    "collected_at": datetime.now().isoformat(),
                    "patent_type": patent.get("patent_type") or "utility"
                }
                
                # æ·»åŠ å‘æ˜äººä¿¡æ¯
                first_name = patent.get("inventor_name_first", "")
                last_name = patent.get("inventor_name_last", "")
                if first_name or last_name:
                    inventor_name = f"{first_name} {last_name}".strip()
                    if inventor_name:
                        standard_patent["inventors"] = [inventor_name]
                
                # æ·»åŠ æ‘˜è¦æ–‡æœ¬
                if patent_id in summary_dict:
                    standard_patent["summary_text"] = summary_dict[patent_id]
                
                # æ·»åŠ æƒåˆ©è¦æ±‚
                if patent_id in claims_dict:
                    standard_patent["claims"] = claims_dict[patent_id]
                
                standard_patents.append(standard_patent)
            
            self.logger.info(f"Converted {len(standard_patents)} patents to standard format")
            return standard_patents
            
        except Exception as e:
            self.logger.error(f"Error converting PatentsView data: {str(e)}")
            return []
    
    async def _fetch_from_source(self, source: str, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """ä»å…¶ä»–æ•°æ®æºè·å–æ•°æ®ï¼ˆä¿ç•™åŸæœ‰æ¨¡æ‹Ÿå®ç°ä½œä¸ºåå¤‡ï¼‰."""
        # è¿™é‡Œä¿ç•™åŸæœ‰çš„æ¨¡æ‹Ÿå®ç°ä½œä¸ºå…¶ä»–æ•°æ®æºçš„åå¤‡
        
        self.logger.info(f"Fetching data from {source} with keywords: {keywords}")
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.5)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        mock_patents = []
        
        applicants = [
            "åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸", "è…¾è®¯ç§‘æŠ€", "é˜¿é‡Œå·´å·´", "ç™¾åº¦", "å­—èŠ‚è·³åŠ¨",
            "Apple Inc.", "Google LLC", "Microsoft Corporation", "Samsung Electronics"
        ]
        
        ipc_classes = ["G06F", "H04L", "G06N", "H04W", "G06Q", "H01L"]
        countries = ["CN", "US", "JP", "KR", "DE"]
        
        import random
        random.seed(hash(source + str(keywords)))  # ç¡®ä¿ç»“æœå¯é‡ç°
        
        for i in range(min(limit, 50)):  # æ¯ä¸ªæºæœ€å¤šè¿”å›50æ¡
            year = random.randint(2020, 2024)
            month = random.randint(1, 12)
            day = random.randint(1, 28)
            
            patent = {
                "patent_id": f"{source.upper()}{year}{i:04d}",
                "patent_number": f"{source.upper()}{year}{i:04d}",
                "title": f"å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„{random.choice(['æ–¹æ³•', 'ç³»ç»Ÿ', 'è£…ç½®'])} - {source}",
                "abstract": f"æœ¬å‘æ˜æ¶‰åŠ{keywords[0] if keywords else 'æŠ€æœ¯'}é¢†åŸŸï¼Œæ¥æºäº{source}æ•°æ®åº“...",
                "applicants": [random.choice(applicants)],
                "inventors": [f"å‘æ˜äºº{i+1}"],
                "application_date": f"{year}-{month:02d}-{day:02d}",
                "publication_date": f"{year}-{month+3 if month <= 9 else month-9:02d}-{day:02d}",
                "ipc_classes": [random.choice(ipc_classes)],
                "country": random.choice(countries),
                "status": random.choice(["å·²æˆæƒ", "å®¡æŸ¥ä¸­", "å·²å…¬å¼€"]),
                "source": source,
                "collected_at": datetime.now().isoformat()
            }
            mock_patents.append(patent)
        
        self.logger.info(f"Collected {len(mock_patents)} patents from {source}")
        return mock_patents
    
    async def _process_and_validate_data(self, collection_results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """å¤„ç†å’ŒéªŒè¯æ”¶é›†çš„æ•°æ®."""
        try:
            # åˆå¹¶æ‰€æœ‰æ•°æ®æºçš„ç»“æœ
            all_patents = []
            source_stats = {}
            
            for source, patents in collection_results.items():
                source_stats[source] = len(patents)
                for patent in patents:
                    patent["data_source"] = source
                    all_patents.append(patent)
            
            # æ•°æ®æ¸…æ´—
            cleaned_patents = []
            for patent in all_patents:
                if self.validate_patent_data(patent):
                    cleaned_patent = self.clean_patent_data(patent)
                    cleaned_patents.append(cleaned_patent)
            
            # å»é‡
            deduplicated_patents = self._deduplicate_patents(cleaned_patents)
            
            # è´¨é‡è¯„ä¼°
            quality_score = self._calculate_data_quality(deduplicated_patents)
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "total_collected": len(all_patents),
                "after_cleaning": len(cleaned_patents),
                "after_deduplication": len(deduplicated_patents),
                "source_breakdown": source_stats,
                "quality_score": quality_score,
                "duplicate_ratio": (len(cleaned_patents) - len(deduplicated_patents)) / max(len(cleaned_patents), 1)
            }
            
            return {
                "patents": deduplicated_patents,
                "statistics": stats,
                "quality_score": quality_score
            }
            
        except Exception as e:
            self.logger.error(f"Error processing collected data: {str(e)}")
            return {
                "patents": [],
                "statistics": {"error": str(e)},
                "quality_score": 0.0
            }
    
    def _deduplicate_patents(self, patents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ä¸“åˆ©æ•°æ®."""
        seen_signatures = set()
        deduplicated = []
        
        for patent in patents:
            # ç”Ÿæˆä¸“åˆ©ç­¾å
            signature = self._generate_patent_signature(patent)
            
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                deduplicated.append(patent)
            else:
                self.logger.debug(f"Duplicate patent found: {patent.get('title', 'Unknown')}")
        
        return deduplicated
    
    def _generate_patent_signature(self, patent: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸“åˆ©ç­¾åç”¨äºå»é‡."""
        # ä½¿ç”¨ä¸“åˆ©ID/ä¸“åˆ©å·å’Œæ ‡é¢˜çš„ç»„åˆä½œä¸ºç­¾å
        patent_id = patent.get("patent_id", "").strip().lower()
        patent_number = patent.get("patent_number", "").strip().lower()
        title = patent.get("title", "").strip().lower()
        
        # ä¼˜å…ˆä½¿ç”¨ä¸“åˆ©IDï¼Œå…¶æ¬¡æ˜¯ä¸“åˆ©å·
        identifier = patent_id or patent_number
        
        # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆç§»é™¤å¸¸è§çš„å˜åŒ–ï¼‰
        title = re.sub(r'[^\w\s]', '', title)  # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        title = re.sub(r'\s+', ' ', title)     # æ ‡å‡†åŒ–ç©ºæ ¼
        
        return f"{identifier}|{title}"
    
    def validate_patent_data(self, patent: Dict[str, Any]) -> bool:
        """éªŒè¯ä¸“åˆ©æ•°æ®çš„æœ‰æ•ˆæ€§."""
        try:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = self.quality_config["required_fields"]
            for field in required_fields:
                if not patent.get(field):
                    return False
            
            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦
            title = patent.get("title", "")
            if len(title) < self.quality_config["min_title_length"]:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error validating patent data: {str(e)}")
            return False
    
    def clean_patent_data(self, patent: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—ä¸“åˆ©æ•°æ®."""
        try:
            cleaned_patent = patent.copy()
            
            # æ¸…ç†æ ‡é¢˜
            if "title" in cleaned_patent:
                title = cleaned_patent["title"].strip()
                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
                title = re.sub(r'\s+', ' ', title)
                cleaned_patent["title"] = title
            
            # æ¸…ç†æ‘˜è¦
            if "abstract" in cleaned_patent:
                abstract = cleaned_patent["abstract"].strip()
                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
                abstract = re.sub(r'\s+', ' ', abstract)
                cleaned_patent["abstract"] = abstract
            
            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            for date_field in ["application_date", "publication_date"]:
                if date_field in cleaned_patent and cleaned_patent[date_field]:
                    try:
                        # å°è¯•è§£æå’Œæ ‡å‡†åŒ–æ—¥æœŸ
                        date_str = cleaned_patent[date_field]
                        if isinstance(date_str, str) and len(date_str) >= 10:
                            # ä¿æŒ YYYY-MM-DD æ ¼å¼
                            cleaned_patent[date_field] = date_str[:10]
                    except Exception:
                        pass
            
            # ç¡®ä¿åˆ—è¡¨å­—æ®µæ˜¯åˆ—è¡¨
            list_fields = ["applicants", "inventors", "ipc_classes", "cpc_classes"]
            for field in list_fields:
                if field in cleaned_patent:
                    if not isinstance(cleaned_patent[field], list):
                        if cleaned_patent[field]:
                            cleaned_patent[field] = [cleaned_patent[field]]
                        else:
                            cleaned_patent[field] = []
            
            return cleaned_patent
            
        except Exception as e:
            self.logger.error(f"Error cleaning patent data: {str(e)}")
            return patent
    
    def _calculate_data_quality(self, patents: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°."""
        if not patents:
            return 0.0
        
        quality_factors = {
            "completeness": 0,
            "validity": 0,
            "consistency": 0
        }
        
        total_patents = len(patents)
        
        # å®Œæ•´æ€§æ£€æŸ¥
        complete_patents = 0
        for patent in patents:
            required_fields = self.quality_config["required_fields"]
            if all(patent.get(field) for field in required_fields):
                complete_patents += 1
        
        quality_factors["completeness"] = complete_patents / total_patents
        
        # æœ‰æ•ˆæ€§æ£€æŸ¥
        valid_patents = 0
        for patent in patents:
            title_length = len(patent.get("title", ""))
            if title_length >= self.quality_config["min_title_length"]:
                valid_patents += 1
        
        quality_factors["validity"] = valid_patents / total_patents
        
        # ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆç®€å•å®ç°ï¼‰
        quality_factors["consistency"] = 0.8  # å‡è®¾ä¸€è‡´æ€§è¾ƒå¥½
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        overall_quality = (
            quality_factors["completeness"] * 0.4 +
            quality_factors["validity"] * 0.4 +
            quality_factors["consistency"] * 0.2
        )
        
        return round(overall_quality, 2)
    
    async def _generate_collection_response(self, collection_params: Dict[str, Any], processed_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æ”¶é›†å“åº”å†…å®¹."""
        try:
            patents = processed_data.get("patents", [])
            stats = processed_data.get("statistics", {})
            quality_score = processed_data.get("quality_score", 0.0)
            
            response_parts = []
            
            # æ·»åŠ æ”¶é›†æ¦‚è¿°
            keywords_str = "ã€".join(collection_params.get("keywords", ["ç›¸å…³æŠ€æœ¯"]))
            response_parts.append(f"## ä¸“åˆ©æ•°æ®æ”¶é›†æŠ¥å‘Š")
            response_parts.append(f"**æ”¶é›†ä¸»é¢˜**: {keywords_str}")
            response_parts.append(f"**æ”¶é›†ç±»å‹**: {collection_params.get('collection_type', 'comprehensive')}")
            response_parts.append("")
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            response_parts.append("### ğŸ“Š æ”¶é›†ç»Ÿè®¡")
            response_parts.append(f"- **æ€»æ”¶é›†æ•°é‡**: {stats.get('total_collected', 0)}ä»¶")
            response_parts.append(f"- **æ¸…æ´—åæ•°é‡**: {stats.get('after_cleaning', 0)}ä»¶")
            response_parts.append(f"- **å»é‡åæ•°é‡**: {stats.get('after_deduplication', 0)}ä»¶")
            response_parts.append(f"- **æ•°æ®è´¨é‡åˆ†æ•°**: {quality_score:.2f}/1.00")
            response_parts.append("")
            
            # æ·»åŠ æ•°æ®æºç»Ÿè®¡
            source_breakdown = stats.get("source_breakdown", {})
            if source_breakdown:
                response_parts.append("### ğŸ”— æ•°æ®æºåˆ†å¸ƒ")
                for source, count in source_breakdown.items():
                    response_parts.append(f"- **{source}**: {count}ä»¶")
                response_parts.append("")
            
            # æ·»åŠ æ ·æœ¬æ•°æ®
            if patents:
                response_parts.append("### ğŸ“‹ æ ·æœ¬æ•°æ®")
                for i, patent in enumerate(patents[:3]):  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
                    response_parts.append(f"**æ ·æœ¬ {i+1}:**")
                    response_parts.append(f"- æ ‡é¢˜: {patent.get('title', 'N/A')}")
                    response_parts.append(f"- ä¸“åˆ©å·: {patent.get('patent_number', 'N/A')}")
                    response_parts.append(f"- ä¸“åˆ©ID: {patent.get('patent_id', 'N/A')}")
                    response_parts.append(f"- ç”³è¯·äºº: {', '.join(patent.get('applicants', ['N/A']))}")
                    response_parts.append(f"- ç”³è¯·æ—¥æœŸ: {patent.get('application_date', 'N/A')}")
                    response_parts.append("")
            
            # æ·»åŠ è´¨é‡è¯„ä¼°
            duplicate_ratio = stats.get("duplicate_ratio", 0)
            response_parts.append("### ğŸ¯ è´¨é‡è¯„ä¼°")
            response_parts.append(f"- **é‡å¤ç‡**: {duplicate_ratio:.1%}")
            
            if quality_score >= 0.8:
                response_parts.append("- **è´¨é‡ç­‰çº§**: ä¼˜ç§€ âœ…")
            elif quality_score >= 0.6:
                response_parts.append("- **è´¨é‡ç­‰çº§**: è‰¯å¥½ âš ï¸")
            else:
                response_parts.append("- **è´¨é‡ç­‰çº§**: éœ€æ”¹è¿› âŒ")
            
            # æ·»åŠ å»ºè®®
            response_parts.append("\n### ğŸ’¡ å»ºè®®")
            if quality_score < 0.7:
                response_parts.append("- å»ºè®®è°ƒæ•´æœç´¢å…³é”®è¯ä»¥æé«˜æ•°æ®è´¨é‡")
            if duplicate_ratio > 0.2:
                response_parts.append("- æ£€æµ‹åˆ°è¾ƒé«˜é‡å¤ç‡ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®æºé€‰æ‹©")
            if len(patents) < collection_params.get("limit", 100) * 0.5:
                response_parts.append("- æ”¶é›†æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®æ‰©å¤§æœç´¢èŒƒå›´")
            
            response_parts.append("- æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯è¿›è¡Œåç»­åˆ†æå¤„ç†")
            
            # æ·»åŠ æ•°æ®è¯´æ˜
            response_parts.append("\n---")
            response_parts.append("*æ”¶é›†çš„ä¸“åˆ©æ•°æ®å·²ç»è¿‡æ¸…æ´—å’Œå»é‡å¤„ç†ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥çš„åˆ†æå’Œç ”ç©¶ã€‚*")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            self.logger.error(f"Error generating collection response: {str(e)}")
            return f"æ•°æ®æ”¶é›†æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _generate_collection_actions(self, collection_params: Dict[str, Any], processed_data: Dict[str, Any]) -> List[Action]:
        """ç”Ÿæˆæ•°æ®æ”¶é›†åç»­åŠ¨ä½œ."""
        actions = []
        
        try:
            patents = processed_data.get("patents", [])
            
            # åŸºç¡€åç»­åŠ¨ä½œ
            if patents:
                actions.append(Action(
                    action_type="export_collected_data",
                    parameters={"format": "json", "include_metadata": True},
                    description="å¯¼å‡ºæ”¶é›†çš„ä¸“åˆ©æ•°æ®"
                ))
                
                actions.append(Action(
                    action_type="start_analysis",
                    parameters={"data_count": len(patents), "analysis_type": "comprehensive"},
                    description="å¼€å§‹ä¸“åˆ©æ•°æ®åˆ†æ"
                ))
                
                actions.append(Action(
                    action_type="data_validation",
                    parameters={"validation_level": "detailed"},
                    description="æ‰§è¡Œè¯¦ç»†æ•°æ®éªŒè¯"
                ))
            
            # åŸºäºè´¨é‡çš„åŠ¨ä½œ
            quality_score = processed_data.get("quality_score", 0.0)
            if quality_score < 0.7:
                actions.append(Action(
                    action_type="improve_data_quality",
                    parameters={"target_quality": 0.8, "retry_collection": True},
                    description="æ”¹è¿›æ•°æ®è´¨é‡"
                ))
            
            # åŸºäºæ•°é‡çš„åŠ¨ä½œ
            if len(patents) < collection_params.get("limit", 100) * 0.5:
                actions.append(Action(
                    action_type="expand_collection",
                    parameters={"additional_sources": True, "broader_keywords": True},
                    description="æ‰©å¤§æ•°æ®æ”¶é›†èŒƒå›´"
                ))
            
            return actions
            
        except Exception as e:
            self.logger.error(f"Error generating collection actions: {str(e)}")
            return []
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº."""
        try:
            if self.patentsview_service:
                await self.patentsview_service.cleanup()
                self.patentsview_service = None
            
            # è°ƒç”¨çˆ¶ç±»æ¸…ç†æ–¹æ³•
            await super().cleanup()
            
        except Exception as e:
            self.logger.error(f"Error during cleanup: {str(e)}")
    
    async def _build_patentsview_query(self, keywords: List[str], date_range: Optional[Dict[str, int]]) -> Dict[str, Any]:
        """æ„å»º PatentsView API æœç´¢æŸ¥è¯¢."""
        query = {}
        
        # å…³é”®è¯æŸ¥è¯¢
        if keywords:
            keyword_conditions = []
            for keyword in keywords:
                keyword_conditions.append({
                    "_text_any": {
                        "patent_title": keyword,
                        "patent_abstract": keyword
                    }
                })
            
            if len(keyword_conditions) == 1:
                query.update(keyword_conditions[0])
            else:
                query["_or"] = keyword_conditions
        
        # æ—¥æœŸèŒƒå›´
        if date_range:
            date_condition = {}
            if date_range.get("start_year"):
                date_condition["_gte"] = f"{date_range['start_year']}-01-01"
            if date_range.get("end_year"):
                date_condition["_lte"] = f"{date_range['end_year']}-12-31"
            
            if date_condition:
                query["patent_date"] = date_condition
        
        return query
    
    async def _search_patents_direct(self, query: Dict[str, Any], max_results: int = 1000) -> Dict[str, Any]:
        """ç›´æ¥æœç´¢ä¸“åˆ©åŸºç¡€ä¿¡æ¯."""
        try:
            endpoint = "/patent/"
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                'q': json.dumps(query),
                'f': json.dumps([
                    "patent_id", "patent_number", "patent_title", 
                    "patent_abstract", "patent_date", "patent_type",
                    "assignee_organization", "assignee_country",
                    "inventor_name_first", "inventor_name_last",
                    "ipc_class", "cpc_class"
                ]),
                'o': json.dumps({
                    "size": min(max_results, 1000),
                    "pad_patent_id": False
                }),
                's': json.dumps([{"patent_date": "desc"}])
            }
            
            response_data = await self._make_patentsview_request(endpoint, params)
            
            return {
                "type": "patents",
                "data": response_data.get("patents", []),
                "total_count": response_data.get("total_patent_count", 0)
            }
            
        except Exception as e:
            self.logger.error(f"Error searching patents: {str(e)}")
            return {"type": "patents", "data": [], "error": str(e)}
    
    async def _search_patent_texts_direct(self, query: Dict[str, Any]) -> Dict[str, Any]:
        """ç›´æ¥æœç´¢ä¸“åˆ©æ–‡æœ¬ä¿¡æ¯."""
        try:
            # æœç´¢ä¸“åˆ©æ‘˜è¦
            summary_endpoint = "/g_brf_sum_text/"
            summary_params = {
                'q': json.dumps(query),
                'f': json.dumps(["patent_id", "summary_text"]),
                'o': json.dumps({"size": 100})
            }
            
            summary_data = await self._make_patentsview_request(summary_endpoint, summary_params)
            
            # æœç´¢æƒåˆ©è¦æ±‚
            claims_endpoint = "/g_claim/"
            claims_params = {
                'q': json.dumps(query),
                'f': json.dumps(["patent_id", "claim_sequence", "claim_text"]),
                'o': json.dumps({"size": 200})
            }
            
            claims_data = await self._make_patentsview_request(claims_endpoint, claims_params)
            
            return {
                "type": "patent_texts",
                "summaries": summary_data.get("g_brf_sum_texts", []),
                "claims": claims_data.get("g_claims", [])
            }
            
        except Exception as e:
            self.logger.error(f"Error searching patent texts: {str(e)}")
            return {"type": "patent_texts", "summaries": [], "claims": [], "error": str(e)}
    
    async def _make_patentsview_request(self, endpoint: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """å‘èµ· PatentsView API è¯·æ±‚."""
        url = f"{self.patentsview_config.base_url}{endpoint}"
        
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'PatentDataCollectionAgent/1.0'
        }
        
        # æ·»åŠ  API å¯†é’¥ï¼ˆå¦‚æœæœ‰ï¼‰
        if self.patentsview_config.api_key:
            headers['X-API-Key'] = self.patentsview_config.api_key
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.patentsview_config.max_retries):
            try:
                timeout = aiohttp.ClientTimeout(total=self.patentsview_config.timeout)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    
                    # ä½¿ç”¨ POST è¯·æ±‚å‘é€å¤æ‚æŸ¥è¯¢
                    async with session.post(url, json=params, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            return data
                        elif response.status == 429:
                            # é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•
                            wait_time = self.patentsview_config.rate_limit_delay * (2 ** attempt)
                            await asyncio.sleep(wait_time)
                            continue
                        else:
                            error_text = await response.text()
                            raise Exception(f"API request failed with status {response.status}: {error_text}")
            
            except asyncio.TimeoutError:
                if attempt == self.patentsview_config.max_retries - 1:
                    raise Exception("API request timeout after all retries")
                await asyncio.sleep(self.patentsview_config.rate_limit_delay)
            
            except Exception as e:
                if attempt == self.patentsview_config.max_retries - 1:
                    raise e
                await asyncio.sleep(self.patentsview_config.rate_limit_delay)
        
        raise Exception("API request failed after all retries")
    
    async def _integrate_patentsview_results(self, results: List[Any]) -> Dict[str, Any]:
        """æ•´åˆ PatentsView æœç´¢ç»“æœ."""
        integrated_data = {
            "patents": [],
            "patent_texts": {"summaries": [], "claims": []},
            "errors": []
        }
        
        for result in results:
            if isinstance(result, Exception):
                integrated_data["errors"].append(str(result))
                continue
            
            if isinstance(result, dict):
                result_type = result.get("type")
                
                if result_type == "patents":
                    integrated_data["patents"] = result.get("data", [])
                elif result_type == "patent_texts":
                    integrated_data["patent_texts"]["summaries"] = result.get("summaries", [])
                    integrated_data["patent_texts"]["claims"] = result.get("claims", [])
                
                # è®°å½•é”™è¯¯
                if "error" in result:
                    integrated_data["errors"].append(result["error"])
        
        return integrated_data