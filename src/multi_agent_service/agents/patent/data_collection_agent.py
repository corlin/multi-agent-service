"""Patent data collection agent implementation."""

import asyncio
import aiohttp
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import re

from .base import PatentBaseAgent
from ...models.base import UserRequest, AgentResponse, Action
from ...models.config import AgentConfig
from ...models.enums import AgentType
from ...services.model_client import BaseModelClient


logger = logging.getLogger(__name__)


class PatentDataCollectionAgent(PatentBaseAgent):
    """ä¸“åˆ©æ•°æ®æ”¶é›†Agentï¼Œè´Ÿè´£ä»å¤šä¸ªæ•°æ®æºæ”¶é›†ä¸“åˆ©ä¿¡æ¯."""
    
    def __init__(self, config: AgentConfig, model_client: BaseModelClient):
        """åˆå§‹åŒ–ä¸“åˆ©æ•°æ®æ”¶é›†Agent."""
        super().__init__(config, model_client)
        
        # æ•°æ®æ”¶é›†ç›¸å…³å…³é”®è¯
        self.collection_keywords = [
            "æ”¶é›†", "è·å–", "é‡‡é›†", "æŠ“å–", "ä¸‹è½½", "å¯¼å…¥", "åŒæ­¥",
            "collect", "gather", "fetch", "retrieve", "download", "import", "sync"
        ]
        
        # æ•°æ®æºé…ç½®
        self.data_sources_config = {
            'google_patents': {
                'base_url': 'https://patents.google.com/api',
                'rate_limit': 10,
                'timeout': 30,
                'enabled': True
            },
            'patent_public_api': {
                'base_url': 'https://api.patentsview.org/patents/query',
                'rate_limit': 5,
                'timeout': 30,
                'enabled': True
            },
            'cnipa_api': {
                'base_url': 'http://epub.cnipa.gov.cn/api',
                'rate_limit': 3,
                'timeout': 45,
                'enabled': True
            }
        }
        
        # æ•°æ®è´¨é‡é…ç½®
        self.quality_config = {
            "min_title_length": 5,
            "required_fields": ["title", "application_number"],
            "max_duplicates_ratio": 0.1
        }
    
    async def can_handle_request(self, request: UserRequest) -> float:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†æ•°æ®æ”¶é›†ç›¸å…³è¯·æ±‚."""
        # å…ˆè°ƒç”¨çˆ¶ç±»çš„ä¸“åˆ©ç›¸å…³åˆ¤æ–­
        base_confidence = await super().can_handle_request(request)
        
        content = request.content.lower()
        
        # æ£€æŸ¥æ•°æ®æ”¶é›†å…³é”®è¯
        collection_matches = sum(1 for keyword in self.collection_keywords if keyword in content)
        collection_score = min(collection_matches * 0.3, 0.6)
        
        # æ£€æŸ¥æ•°æ®æ”¶é›†ç‰¹å®šæ¨¡å¼
        collection_patterns = [
            r"(æ”¶é›†|è·å–|é‡‡é›†).*?(ä¸“åˆ©|æ•°æ®|ä¿¡æ¯)",
            r"(ä¸‹è½½|å¯¼å…¥|åŒæ­¥).*?(ä¸“åˆ©|æ–‡ä»¶|æ•°æ®)",
            r"(æŠ“å–|çˆ¬å–).*?(ä¸“åˆ©|ç½‘ç«™|æ•°æ®)",
            r"(collect|gather|fetch).*?(patent|data|information)",
            r"(download|import|sync).*?(patent|file|data)"
        ]
        
        pattern_score = 0
        for pattern in collection_patterns:
            if re.search(pattern, content):
                pattern_score += 0.25
        
        # ç»¼åˆè¯„åˆ†
        total_score = min(base_confidence + collection_score + pattern_score, 1.0)
        
        return max(total_score, 0.0)
    
    async def get_capabilities(self) -> List[str]:
        """è·å–æ•°æ®æ”¶é›†Agentçš„èƒ½åŠ›åˆ—è¡¨."""
        base_capabilities = await super().get_capabilities()
        collection_capabilities = [
            "å¤šæ•°æ®æºä¸“åˆ©æ”¶é›†",
            "Google Patents APIé›†æˆ",
            "ä¸“åˆ©å…¬å¼€APIé›†æˆ",
            "CNIPAæ•°æ®æºé›†æˆ",
            "æ•°æ®æ ‡å‡†åŒ–å’Œæ¸…æ´—",
            "æ•°æ®å»é‡å’Œè´¨é‡æ§åˆ¶",
            "æ‰¹é‡æ•°æ®å¤„ç†",
            "å¢é‡æ•°æ®åŒæ­¥"
        ]
        return base_capabilities + collection_capabilities
    
    async def estimate_processing_time(self, request: UserRequest) -> int:
        """ä¼°ç®—æ•°æ®æ”¶é›†å¤„ç†æ—¶é—´."""
        content = request.content.lower()
        
        # å°é‡æ•°æ®ï¼š20-30ç§’
        if any(word in content for word in ["å°‘é‡", "å‡ ä¸ª", "10", "20"]):
            return 25
        
        # ä¸­é‡æ•°æ®ï¼š45-60ç§’
        if any(word in content for word in ["ä¸­ç­‰", "100", "200"]):
            return 50
        
        # å¤§é‡æ•°æ®ï¼š90-120ç§’
        if any(word in content for word in ["å¤§é‡", "å…¨éƒ¨", "1000", "æ‰¹é‡"]):
            return 105
        
        # é»˜è®¤æ”¶é›†æ—¶é—´
        return 40
    
    async def _process_request_specific(self, request: UserRequest) -> AgentResponse:
        """å¤„ç†æ•°æ®æ”¶é›†ç›¸å…³çš„å…·ä½“è¯·æ±‚."""
        start_time = datetime.now()
        
        try:
            # è§£ææ”¶é›†è¯·æ±‚
            collection_params = self._parse_collection_request(request.content)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_collection_cache_key(collection_params)
            cached_result = await self.get_from_cache(cache_key)
            
            if cached_result:
                self.logger.info("Returning cached collection results")
                return AgentResponse(
                    agent_id=self.agent_id,
                    agent_type=self.agent_type,
                    response_content=cached_result["response_content"],
                    confidence=0.9,
                    metadata={
                        **cached_result.get("metadata", {}),
                        "from_cache": True
                    }
                )
            
            # æ‰§è¡Œæ•°æ®æ”¶é›†
            collection_results = await self._execute_data_collection(collection_params)
            
            # æ•°æ®å¤„ç†å’Œè´¨é‡æ§åˆ¶
            processed_data = await self._process_and_validate_data(collection_results)
            
            # ç”Ÿæˆå“åº”å†…å®¹
            response_content = await self._generate_collection_response(
                collection_params, processed_data
            )
            
            # ç”Ÿæˆåç»­åŠ¨ä½œ
            next_actions = self._generate_collection_actions(collection_params, processed_data)
            
            # ç¼“å­˜ç»“æœ
            result_data = {
                "response_content": response_content,
                "metadata": {
                    "collection_params": collection_params,
                    "total_collected": len(processed_data.get("patents", [])),
                    "processing_time": (datetime.now() - start_time).total_seconds(),
                    "sources_used": list(collection_results.keys()),
                    "data_quality_score": processed_data.get("quality_score", 0.0)
                }
            }
            await self.save_to_cache(cache_key, result_data)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("data_collection", duration, True)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=response_content,
                confidence=0.85,
                next_actions=next_actions,
                collaboration_needed=False,
                metadata=result_data["metadata"]
            )
            
        except Exception as e:
            self.logger.error(f"Error processing collection request: {str(e)}")
            
            # è®°å½•å¤±è´¥æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("data_collection", duration, False)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=f"æ•°æ®æ”¶é›†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
                confidence=0.0,
                collaboration_needed=True,
                metadata={
                    "error": str(e),
                    "processing_time": duration
                }
            )
    
    def _parse_collection_request(self, content: str) -> Dict[str, Any]:
        """è§£ææ•°æ®æ”¶é›†è¯·æ±‚å‚æ•°."""
        params = {
            "keywords": [],
            "sources": list(self.data_sources_config.keys()),
            "limit": 100,
            "date_range": None,
            "collection_type": "comprehensive",
            "quality_level": "standard"
        }
        
        content_lower = content.lower()
        
        # æå–å…³é”®è¯
        keywords = []
        
        # æŸ¥æ‰¾å¼•å·ä¸­çš„å…³é”®è¯
        quoted_keywords = re.findall(r'["""\'](.*?)["""\']', content)
        keywords.extend(quoted_keywords)
        
        # æŸ¥æ‰¾å¸¸è§çš„æŠ€æœ¯æœ¯è¯­
        tech_patterns = [
            r'(äººå·¥æ™ºèƒ½|AI|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ )',
            r'(åŒºå—é“¾|blockchain)',
            r'(ç‰©è”ç½‘|IoT)',
            r'(5G|é€šä¿¡æŠ€æœ¯)',
            r'(æ–°èƒ½æº|ç”µæ± æŠ€æœ¯)',
            r'(ç”Ÿç‰©æŠ€æœ¯|åŸºå› )',
            r'(èŠ¯ç‰‡|åŠå¯¼ä½“)',
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
        if not keywords:
            stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "ç­‰", "æ”¶é›†", "ä¸“åˆ©"}
            words = content.split()
            keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        params["keywords"] = keywords[:5]  # é™åˆ¶å…³é”®è¯æ•°é‡
        
        # åˆ¤æ–­æ”¶é›†ç±»å‹
        if any(word in content_lower for word in ["å¿«é€Ÿ", "ç®€å•", "åŸºç¡€"]):
            params["collection_type"] = "quick"
            params["limit"] = 50
        elif any(word in content_lower for word in ["å…¨é¢", "è¯¦ç»†", "å®Œæ•´"]):
            params["collection_type"] = "comprehensive"
            params["limit"] = 200
        elif any(word in content_lower for word in ["æ·±åº¦", "ä¸“ä¸š", "é«˜è´¨é‡"]):
            params["collection_type"] = "deep"
            params["quality_level"] = "high"
        
        # åˆ¤æ–­æ•°æ®æºåå¥½
        if "google" in content_lower:
            params["sources"] = ["google_patents"]
        elif "cnipa" in content_lower or "ä¸­å›½" in content_lower:
            params["sources"] = ["cnipa_api"]
        elif "å…¬å¼€" in content_lower or "public" in content_lower:
            params["sources"] = ["patent_public_api"]
        
        # æå–æ•°é‡é™åˆ¶
        limit_match = re.search(r'(\d+).*?(ä¸ª|æ¡|ä»¶)', content)
        if limit_match:
            params["limit"] = min(int(limit_match.group(1)), 500)  # æœ€å¤§é™åˆ¶500
        
        # æå–æ—¶é—´èŒƒå›´
        year_matches = re.findall(r'(\d{4})', content)
        if len(year_matches) >= 2:
            years = [int(y) for y in year_matches if 2000 <= int(y) <= 2024]
            if len(years) >= 2:
                params["date_range"] = {
                    "start_year": min(years),
                    "end_year": max(years)
                }
        
        return params
    
    def _generate_collection_cache_key(self, collection_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ”¶é›†ç¼“å­˜é”®."""
        key_parts = [
            "collection",
            "_".join(sorted(collection_params["keywords"])),
            "_".join(sorted(collection_params["sources"])),
            str(collection_params["limit"]),
            collection_params["collection_type"]
        ]
        
        if collection_params.get("date_range"):
            date_range = collection_params["date_range"]
            key_parts.append(f"{date_range['start_year']}-{date_range['end_year']}")
        
        return "_".join(key_parts).replace(" ", "_")
    
    async def _execute_data_collection(self, collection_params: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """æ‰§è¡Œæ•°æ®æ”¶é›†."""
        collection_tasks = []
        
        # ä¸ºæ¯ä¸ªæ•°æ®æºåˆ›å»ºæ”¶é›†ä»»åŠ¡
        for source in collection_params["sources"]:
            if source in self.data_sources_config and self.data_sources_config[source]["enabled"]:
                task = self._collect_from_source(source, collection_params)
                collection_tasks.append((source, task))
        
        # å¹¶è¡Œæ‰§è¡Œæ”¶é›†ä»»åŠ¡
        results = {}
        completed_tasks = await asyncio.gather(
            *[task for _, task in collection_tasks],
            return_exceptions=True
        )
        
        # å¤„ç†ç»“æœ
        for i, (source, _) in enumerate(collection_tasks):
            result = completed_tasks[i]
            if isinstance(result, Exception):
                self.logger.error(f"Collection failed for {source}: {str(result)}")
                results[source] = []
            else:
                results[source] = result or []
        
        return results
    
    async def _collect_from_source(self, source: str, collection_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»æŒ‡å®šæ•°æ®æºæ”¶é›†æ•°æ®."""
        try:
            keywords = collection_params["keywords"]
            limit = collection_params["limit"]
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶
            return await self.with_retry(
                self._fetch_from_source,
                max_retries=3,
                delay=1.0,
                source=source,
                keywords=keywords,
                limit=limit
            )
            
        except Exception as e:
            self.logger.error(f"Error collecting from {source}: {str(e)}")
            return []
    
    async def _fetch_from_source(self, source: str, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """ä»æ•°æ®æºè·å–æ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰."""
        # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…ä¸­ä¼šè°ƒç”¨çœŸå®çš„API
        
        self.logger.info(f"Fetching data from {source} with keywords: {keywords}")
        
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.5)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        mock_patents = []
        
        applicants = [
            "åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸", "è…¾è®¯ç§‘æŠ€", "é˜¿é‡Œå·´å·´", "ç™¾åº¦", "å­—èŠ‚è·³åŠ¨",
            "Apple Inc.", "Google LLC", "Microsoft Corporation", "Samsung Electronics"
        ]
        
        ipc_classes = ["G06F", "H04L", "G06N", "H04W", "G06Q", "H01L"]
        countries = ["CN", "US", "JP", "KR", "DE"]
        
        import random
        random.seed(hash(source + str(keywords)))  # ç¡®ä¿ç»“æœå¯é‡ç°
        
        for i in range(min(limit, 50)):  # æ¯ä¸ªæºæœ€å¤šè¿”å›50æ¡
            year = random.randint(2020, 2024)
            month = random.randint(1, 12)
            day = random.randint(1, 28)
            
            patent = {
                "application_number": f"{source.upper()}{year}{i:04d}",
                "title": f"å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„{random.choice(['æ–¹æ³•', 'ç³»ç»Ÿ', 'è£…ç½®'])} - {source}",
                "abstract": f"æœ¬å‘æ˜æ¶‰åŠ{keywords[0] if keywords else 'æŠ€æœ¯'}é¢†åŸŸï¼Œæ¥æºäº{source}æ•°æ®åº“...",
                "applicants": [random.choice(applicants)],
                "inventors": [f"å‘æ˜äºº{i+1}"],
                "application_date": f"{year}-{month:02d}-{day:02d}",
                "publication_date": f"{year}-{month+3 if month <= 9 else month-9:02d}-{day:02d}",
                "ipc_classes": [random.choice(ipc_classes)],
                "country": random.choice(countries),
                "status": random.choice(["å·²æˆæƒ", "å®¡æŸ¥ä¸­", "å·²å…¬å¼€"]),
                "source": source,
                "collected_at": datetime.now().isoformat()
            }
            mock_patents.append(patent)
        
        self.logger.info(f"Collected {len(mock_patents)} patents from {source}")
        return mock_patents
    
    async def _process_and_validate_data(self, collection_results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """å¤„ç†å’ŒéªŒè¯æ”¶é›†çš„æ•°æ®."""
        try:
            # åˆå¹¶æ‰€æœ‰æ•°æ®æºçš„ç»“æœ
            all_patents = []
            source_stats = {}
            
            for source, patents in collection_results.items():
                source_stats[source] = len(patents)
                for patent in patents:
                    patent["data_source"] = source
                    all_patents.append(patent)
            
            # æ•°æ®æ¸…æ´—
            cleaned_patents = []
            for patent in all_patents:
                if self.validate_patent_data(patent):
                    cleaned_patent = self.clean_patent_data(patent)
                    cleaned_patents.append(cleaned_patent)
            
            # å»é‡
            deduplicated_patents = self._deduplicate_patents(cleaned_patents)
            
            # è´¨é‡è¯„ä¼°
            quality_score = self._calculate_data_quality(deduplicated_patents)
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "total_collected": len(all_patents),
                "after_cleaning": len(cleaned_patents),
                "after_deduplication": len(deduplicated_patents),
                "source_breakdown": source_stats,
                "quality_score": quality_score,
                "duplicate_ratio": (len(cleaned_patents) - len(deduplicated_patents)) / max(len(cleaned_patents), 1)
            }
            
            return {
                "patents": deduplicated_patents,
                "statistics": stats,
                "quality_score": quality_score
            }
            
        except Exception as e:
            self.logger.error(f"Error processing collected data: {str(e)}")
            return {
                "patents": [],
                "statistics": {"error": str(e)},
                "quality_score": 0.0
            }
    
    def _deduplicate_patents(self, patents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ä¸“åˆ©æ•°æ®."""
        seen_signatures = set()
        deduplicated = []
        
        for patent in patents:
            # ç”Ÿæˆä¸“åˆ©ç­¾å
            signature = self._generate_patent_signature(patent)
            
            if signature not in seen_signatures:
                seen_signatures.add(signature)
                deduplicated.append(patent)
            else:
                self.logger.debug(f"Duplicate patent found: {patent.get('title', 'Unknown')}")
        
        return deduplicated
    
    def _generate_patent_signature(self, patent: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¸“åˆ©ç­¾åç”¨äºå»é‡."""
        # ä½¿ç”¨ç”³è¯·å·å’Œæ ‡é¢˜çš„ç»„åˆä½œä¸ºç­¾å
        app_number = patent.get("application_number", "").strip().lower()
        title = patent.get("title", "").strip().lower()
        
        # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆç§»é™¤å¸¸è§çš„å˜åŒ–ï¼‰
        title = re.sub(r'[^\w\s]', '', title)  # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        title = re.sub(r'\s+', ' ', title)     # æ ‡å‡†åŒ–ç©ºæ ¼
        
        return f"{app_number}|{title}"
    
    def _calculate_data_quality(self, patents: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°."""
        if not patents:
            return 0.0
        
        quality_factors = {
            "completeness": 0,
            "validity": 0,
            "consistency": 0
        }
        
        total_patents = len(patents)
        
        # å®Œæ•´æ€§æ£€æŸ¥
        complete_patents = 0
        for patent in patents:
            required_fields = self.quality_config["required_fields"]
            if all(patent.get(field) for field in required_fields):
                complete_patents += 1
        
        quality_factors["completeness"] = complete_patents / total_patents
        
        # æœ‰æ•ˆæ€§æ£€æŸ¥
        valid_patents = 0
        for patent in patents:
            title_length = len(patent.get("title", ""))
            if title_length >= self.quality_config["min_title_length"]:
                valid_patents += 1
        
        quality_factors["validity"] = valid_patents / total_patents
        
        # ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆç®€å•å®ç°ï¼‰
        quality_factors["consistency"] = 0.8  # å‡è®¾ä¸€è‡´æ€§è¾ƒå¥½
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        overall_quality = (
            quality_factors["completeness"] * 0.4 +
            quality_factors["validity"] * 0.4 +
            quality_factors["consistency"] * 0.2
        )
        
        return round(overall_quality, 2)
    
    async def _generate_collection_response(self, collection_params: Dict[str, Any], processed_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æ”¶é›†å“åº”å†…å®¹."""
        try:
            patents = processed_data.get("patents", [])
            stats = processed_data.get("statistics", {})
            quality_score = processed_data.get("quality_score", 0.0)
            
            response_parts = []
            
            # æ·»åŠ æ”¶é›†æ¦‚è¿°
            keywords_str = "ã€".join(collection_params.get("keywords", ["ç›¸å…³æŠ€æœ¯"]))
            response_parts.append(f"## ä¸“åˆ©æ•°æ®æ”¶é›†æŠ¥å‘Š")
            response_parts.append(f"**æ”¶é›†ä¸»é¢˜**: {keywords_str}")
            response_parts.append(f"**æ”¶é›†ç±»å‹**: {collection_params.get('collection_type', 'comprehensive')}")
            response_parts.append("")
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            response_parts.append("### ğŸ“Š æ”¶é›†ç»Ÿè®¡")
            response_parts.append(f"- **æ€»æ”¶é›†æ•°é‡**: {stats.get('total_collected', 0)}ä»¶")
            response_parts.append(f"- **æ¸…æ´—åæ•°é‡**: {stats.get('after_cleaning', 0)}ä»¶")
            response_parts.append(f"- **å»é‡åæ•°é‡**: {stats.get('after_deduplication', 0)}ä»¶")
            response_parts.append(f"- **æ•°æ®è´¨é‡åˆ†æ•°**: {quality_score:.2f}/1.00")
            response_parts.append("")
            
            # æ·»åŠ æ•°æ®æºç»Ÿè®¡
            source_breakdown = stats.get("source_breakdown", {})
            if source_breakdown:
                response_parts.append("### ğŸ”— æ•°æ®æºåˆ†å¸ƒ")
                for source, count in source_breakdown.items():
                    response_parts.append(f"- **{source}**: {count}ä»¶")
                response_parts.append("")
            
            # æ·»åŠ æ ·æœ¬æ•°æ®
            if patents:
                response_parts.append("### ğŸ“‹ æ ·æœ¬æ•°æ®")
                for i, patent in enumerate(patents[:3]):  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
                    response_parts.append(f"**æ ·æœ¬ {i+1}:**")
                    response_parts.append(f"- æ ‡é¢˜: {patent.get('title', 'N/A')}")
                    response_parts.append(f"- ç”³è¯·å·: {patent.get('application_number', 'N/A')}")
                    response_parts.append(f"- ç”³è¯·äºº: {', '.join(patent.get('applicants', ['N/A']))}")
                    response_parts.append(f"- ç”³è¯·æ—¥æœŸ: {patent.get('application_date', 'N/A')}")
                    response_parts.append("")
            
            # æ·»åŠ è´¨é‡è¯„ä¼°
            duplicate_ratio = stats.get("duplicate_ratio", 0)
            response_parts.append("### ğŸ¯ è´¨é‡è¯„ä¼°")
            response_parts.append(f"- **é‡å¤ç‡**: {duplicate_ratio:.1%}")
            
            if quality_score >= 0.8:
                response_parts.append("- **è´¨é‡ç­‰çº§**: ä¼˜ç§€ âœ…")
            elif quality_score >= 0.6:
                response_parts.append("- **è´¨é‡ç­‰çº§**: è‰¯å¥½ âš ï¸")
            else:
                response_parts.append("- **è´¨é‡ç­‰çº§**: éœ€æ”¹è¿› âŒ")
            
            # æ·»åŠ å»ºè®®
            response_parts.append("\n### ğŸ’¡ å»ºè®®")
            if quality_score < 0.7:
                response_parts.append("- å»ºè®®è°ƒæ•´æœç´¢å…³é”®è¯ä»¥æé«˜æ•°æ®è´¨é‡")
            if duplicate_ratio > 0.2:
                response_parts.append("- æ£€æµ‹åˆ°è¾ƒé«˜é‡å¤ç‡ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®æºé€‰æ‹©")
            if len(patents) < collection_params.get("limit", 100) * 0.5:
                response_parts.append("- æ”¶é›†æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®æ‰©å¤§æœç´¢èŒƒå›´")
            
            response_parts.append("- æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯è¿›è¡Œåç»­åˆ†æå¤„ç†")
            
            # æ·»åŠ æ•°æ®è¯´æ˜
            response_parts.append("\n---")
            response_parts.append("*æ”¶é›†çš„ä¸“åˆ©æ•°æ®å·²ç»è¿‡æ¸…æ´—å’Œå»é‡å¤„ç†ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥çš„åˆ†æå’Œç ”ç©¶ã€‚*")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            self.logger.error(f"Error generating collection response: {str(e)}")
            return f"æ•°æ®æ”¶é›†æŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _generate_collection_actions(self, collection_params: Dict[str, Any], processed_data: Dict[str, Any]) -> List[Action]:
        """ç”Ÿæˆæ•°æ®æ”¶é›†åç»­åŠ¨ä½œ."""
        actions = []
        
        try:
            patents = processed_data.get("patents", [])
            
            # åŸºç¡€åç»­åŠ¨ä½œ
            if patents:
                actions.append(Action(
                    action_type="export_collected_data",
                    parameters={"format": "json", "include_metadata": True},
                    description="å¯¼å‡ºæ”¶é›†çš„ä¸“åˆ©æ•°æ®"
                ))
                
                actions.append(Action(
                    action_type="start_analysis",
                    parameters={"data_count": len(patents), "analysis_type": "comprehensive"},
                    description="å¼€å§‹ä¸“åˆ©æ•°æ®åˆ†æ"
                ))
                
                actions.append(Action(
                    action_type="data_validation",
                    parameters={"validation_level": "detailed"},
                    description="æ‰§è¡Œè¯¦ç»†æ•°æ®éªŒè¯"
                ))
            
            # åŸºäºè´¨é‡çš„åŠ¨ä½œ
            quality_score = processed_data.get("quality_score", 0.0)
            if quality_score < 0.7:
                actions.append(Action(
                    action_type="improve_data_quality",
                    parameters={"target_quality": 0.8, "retry_collection": True},
                    description="æ”¹è¿›æ•°æ®è´¨é‡"
                ))
            
            # åŸºäºæ•°é‡çš„åŠ¨ä½œ
            if len(patents) < collection_params.get("limit", 100) * 0.5:
                actions.append(Action(
                    action_type="expand_collection",
                    parameters={"additional_sources": True, "broader_keywords": True},
                    description="æ‰©å¤§æ•°æ®æ”¶é›†èŒƒå›´"
                ))
            
            return actions
            
        except Exception as e:
            self.logger.error(f"Error generating collection actions: {str(e)}")
            return []