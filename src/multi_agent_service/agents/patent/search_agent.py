"""Patent search enhancement agent implementation."""

import asyncio
import aiohttp
import json
import logging
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
from urllib.parse import quote

from .base import PatentBaseAgent
from ...models.base import UserRequest, AgentResponse, Action
from ...models.config import AgentConfig
from ...models.enums import AgentType
from ...services.model_client import BaseModelClient


logger = logging.getLogger(__name__)


class PatentSearchAgent(PatentBaseAgent):
    """ä¸“åˆ©æœç´¢å¢å¼ºAgentï¼Œé›†æˆå¤šç§æœç´¢æºæä¾›å…¨é¢çš„ä¸“åˆ©å’ŒæŠ€æœ¯ä¿¡æ¯."""
    
    def __init__(self, config: AgentConfig, model_client: BaseModelClient):
        """åˆå§‹åŒ–ä¸“åˆ©æœç´¢Agent."""
        super().__init__(config, model_client)
        
        # æœç´¢ç›¸å…³å…³é”®è¯
        self.search_keywords = [
            "æœç´¢", "æ£€ç´¢", "æŸ¥æ‰¾", "æŸ¥è¯¢", "å¯»æ‰¾", "è·å–", "æ”¶é›†",
            "search", "find", "query", "retrieve", "collect", "gather"
        ]
        
        # æœç´¢å®¢æˆ·ç«¯é…ç½®
        self.search_clients = {
            "cnki": CNKIClient(),
            "bocha_ai": BochaAIClient(),
            "web_crawler": SmartCrawler()
        }
        
        # æœç´¢ç»“æœè´¨é‡è¯„ä¼°æƒé‡
        self.quality_weights = {
            "relevance": 0.4,      # ç›¸å…³æ€§
            "authority": 0.3,      # æƒå¨æ€§
            "freshness": 0.2,      # æ—¶æ•ˆæ€§
            "completeness": 0.1    # å®Œæ•´æ€§
        }
    
    async def can_handle_request(self, request: UserRequest) -> float:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†æœç´¢ç›¸å…³è¯·æ±‚."""
        # å…ˆè°ƒç”¨çˆ¶ç±»çš„ä¸“åˆ©ç›¸å…³åˆ¤æ–­
        base_confidence = await super().can_handle_request(request)
        
        content = request.content.lower()
        
        # æ£€æŸ¥æœç´¢å…³é”®è¯
        search_matches = sum(1 for keyword in self.search_keywords if keyword in content)
        search_score = min(search_matches * 0.3, 0.6)
        
        # æ£€æŸ¥æœç´¢ç‰¹å®šæ¨¡å¼
        search_patterns = [
            r"(æœç´¢|æ£€ç´¢|æŸ¥æ‰¾).*?(ä¸“åˆ©|æŠ€æœ¯|æ–‡çŒ®)",
            r"(æŸ¥è¯¢|è·å–).*?(ä¿¡æ¯|æ•°æ®|èµ„æ–™)",
            r"(æ”¶é›†|æ•´ç†).*?(ä¸“åˆ©|æŠ€æœ¯).*?(ä¿¡æ¯|æ•°æ®)",
            r"(search|find).*?(patent|technology|literature)",
            r"(query|retrieve).*?(information|data)",
            r"(collect|gather).*?(patent|technology).*?(information|data)"
        ]
        
        pattern_score = 0
        for pattern in search_patterns:
            if re.search(pattern, content):
                pattern_score += 0.25
        
        # ç»¼åˆè¯„åˆ†
        total_score = min(base_confidence + search_score + pattern_score, 1.0)
        
        return max(total_score, 0.0)
    
    async def get_capabilities(self) -> List[str]:
        """è·å–æœç´¢Agentçš„èƒ½åŠ›åˆ—è¡¨."""
        base_capabilities = await super().get_capabilities()
        search_capabilities = [
            "CNKIå­¦æœ¯æœç´¢",
            "åšæŸ¥AIæ™ºèƒ½æœç´¢",
            "æ™ºèƒ½ç½‘é¡µçˆ¬å–",
            "å¤šæºæ•°æ®æ•´åˆ",
            "æœç´¢ç»“æœè´¨é‡è¯„ä¼°",
            "æœç´¢ç»“æœä¼˜åŒ–æ’åº"
        ]
        return base_capabilities + search_capabilities
    
    async def estimate_processing_time(self, request: UserRequest) -> int:
        """ä¼°ç®—æœç´¢å¤„ç†æ—¶é—´."""
        content = request.content.lower()
        
        # ç®€å•æœç´¢ï¼š15-25ç§’
        if any(word in content for word in ["ç®€å•", "å¿«é€Ÿ", "åŸºç¡€"]):
            return 20
        
        # æ·±åº¦æœç´¢ï¼š30-45ç§’
        if any(word in content for word in ["æ·±åº¦", "è¯¦ç»†", "å…¨é¢"]):
            return 40
        
        # å¤šæºæœç´¢ï¼š45-60ç§’
        if any(word in content for word in ["å¤šä¸ª", "æ‰€æœ‰", "ç»¼åˆ"]):
            return 55
        
        # é»˜è®¤æœç´¢æ—¶é—´
        return 30
    
    async def _process_request_specific(self, request: UserRequest) -> AgentResponse:
        """å¤„ç†æœç´¢ç›¸å…³çš„å…·ä½“è¯·æ±‚."""
        start_time = datetime.now()
        
        try:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
            if request.context and request.context.get("mock_mode", False):
                self.logger.info("Running in mock mode, returning simulated search results")
                return await self._generate_mock_search_response(request)
            
            # è§£ææœç´¢è¯·æ±‚
            search_params = self._parse_search_request(request.content)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(search_params)
            cached_result = await self.get_from_cache(cache_key)
            
            if cached_result:
                self.logger.info("Returning cached search results")
                return AgentResponse(
                    agent_id=self.agent_id,
                    agent_type=self.agent_type,
                    response_content=cached_result["response_content"],
                    confidence=0.9,
                    metadata={
                        **cached_result.get("metadata", {}),
                        "from_cache": True
                    }
                )
            
            # æ‰§è¡Œå¤šæºå¹¶è¡Œæœç´¢
            search_results = await self._execute_parallel_search(search_params)
            
            # è´¨é‡è¯„ä¼°å’Œç»“æœä¼˜åŒ–
            optimized_results = await self._optimize_search_results(search_results)
            
            # ç”Ÿæˆå“åº”å†…å®¹
            response_content = await self._generate_search_response(
                search_params, optimized_results
            )
            
            # ç”Ÿæˆåç»­åŠ¨ä½œ
            next_actions = self._generate_search_actions(search_params, optimized_results)
            
            # ç¼“å­˜ç»“æœ
            result_data = {
                "response_content": response_content,
                "metadata": {
                    "search_params": search_params,
                    "results_count": len(optimized_results),
                    "processing_time": (datetime.now() - start_time).total_seconds(),
                    "sources_used": list(search_results.keys())
                }
            }
            await self.save_to_cache(cache_key, result_data)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("search", duration, True)
            
            # é›†æˆç°æœ‰ç›‘æ§ç³»ç»Ÿè®°å½•æœç´¢æŒ‡æ ‡
            await self._log_search_metrics(search_params, optimized_results, duration)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=response_content,
                confidence=0.85,
                next_actions=next_actions,
                collaboration_needed=False,
                metadata=result_data["metadata"]
            )
            
        except Exception as e:
            self.logger.error(f"Error processing search request: {str(e)}")
            
            # è®°å½•å¤±è´¥æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("search", duration, False)
            
            # è®°å½•æœç´¢å¤±è´¥æŒ‡æ ‡
            await self._log_search_failure(str(e), duration)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
                confidence=0.0,
                collaboration_needed=True,
                metadata={
                    "error": str(e),
                    "processing_time": duration
                }
            )
    
    def _parse_search_request(self, content: str) -> Dict[str, Any]:
        """è§£ææœç´¢è¯·æ±‚å‚æ•°."""
        params = {
            "keywords": [],
            "search_type": "general",
            "sources": ["cnki", "bocha_ai", "web_crawler"],
            "limit": 20,
            "language": "auto"
        }
        
        content_lower = content.lower()
        
        # æå–å…³é”®è¯
        # ç®€å•çš„å…³é”®è¯æå–é€»è¾‘
        keywords = []
        
        # æŸ¥æ‰¾å¼•å·ä¸­çš„å…³é”®è¯
        quoted_keywords = re.findall(r'["""\'](.*?)["""\']', content)
        keywords.extend(quoted_keywords)
        
        # æŸ¥æ‰¾å¸¸è§çš„æŠ€æœ¯æœ¯è¯­
        tech_patterns = [
            r'(äººå·¥æ™ºèƒ½|AI|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ )',
            r'(åŒºå—é“¾|blockchain)',
            r'(ç‰©è”ç½‘|IoT)',
            r'(5G|é€šä¿¡æŠ€æœ¯)',
            r'(æ–°èƒ½æº|ç”µæ± æŠ€æœ¯)',
            r'(ç”Ÿç‰©æŠ€æœ¯|åŸºå› )',
            r'(èŠ¯ç‰‡|åŠå¯¼ä½“)',
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œä½¿ç”¨ç®€å•åˆ†è¯
        if not keywords:
            # ç§»é™¤å¸¸è§åœç”¨è¯
            stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "ç­‰"}
            words = content.split()
            keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        params["keywords"] = keywords[:5]  # é™åˆ¶å…³é”®è¯æ•°é‡
        
        # åˆ¤æ–­æœç´¢ç±»å‹
        if any(word in content_lower for word in ["å­¦æœ¯", "è®ºæ–‡", "æ–‡çŒ®", "ç ”ç©¶"]):
            params["search_type"] = "academic"
        elif any(word in content_lower for word in ["æ–°é—»", "èµ„è®¯", "åŠ¨æ€", "æœ€æ–°"]):
            params["search_type"] = "news"
        elif any(word in content_lower for word in ["ä¸“åˆ©", "å‘æ˜", "ç”³è¯·"]):
            params["search_type"] = "patent"
        
        # åˆ¤æ–­æ•°æ®æºåå¥½
        if "cnki" in content_lower or "å­¦æœ¯" in content_lower:
            params["sources"] = ["cnki", "web_crawler"]
        elif "åšæŸ¥" in content_lower or "ai" in content_lower:
            params["sources"] = ["bocha_ai", "web_crawler"]
        
        # åˆ¤æ–­ç»“æœæ•°é‡
        limit_match = re.search(r'(\d+).*?(ä¸ª|æ¡|ç¯‡)', content)
        if limit_match:
            params["limit"] = min(int(limit_match.group(1)), 50)
        
        return params
    
    def _generate_cache_key(self, search_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®."""
        key_parts = [
            "search",
            "_".join(sorted(search_params["keywords"])),
            search_params["search_type"],
            "_".join(sorted(search_params["sources"])),
            str(search_params["limit"])
        ]
        return "_".join(key_parts).replace(" ", "_")
    
    async def _execute_parallel_search(self, search_params: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """æ‰§è¡Œå¹¶è¡Œæœç´¢ï¼ŒåŒ…å«æœåŠ¡é™çº§å’Œæ•…éšœè½¬ç§»æœºåˆ¶."""
        search_tasks = []
        
        # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€å¹¶è°ƒæ•´æœç´¢æº
        available_sources = await self._check_service_health(search_params["sources"])
        
        # ä¸ºæ¯ä¸ªå¯ç”¨çš„æœç´¢æºåˆ›å»ºä»»åŠ¡
        for source in available_sources:
            if source in self.search_clients:
                task = self._search_with_source_and_fallback(source, search_params)
                search_tasks.append((source, task))
        
        # å¦‚æœæ²¡æœ‰å¯ç”¨çš„æœç´¢æºï¼Œä½¿ç”¨é™çº§ç­–ç•¥
        if not search_tasks:
            self.logger.warning("No available search sources, using emergency fallback")
            return await self._emergency_fallback_search(search_params)
        
        # å¹¶è¡Œæ‰§è¡Œæœç´¢
        results = {}
        completed_tasks = await asyncio.gather(
            *[task for _, task in search_tasks],
            return_exceptions=True
        )
        
        # å¤„ç†ç»“æœå¹¶å®æ–½æ•…éšœè½¬ç§»
        failed_sources = []
        for i, (source, _) in enumerate(search_tasks):
            result = completed_tasks[i]
            if isinstance(result, Exception):
                self.logger.error(f"Search failed for {source}: {str(result)}")
                failed_sources.append(source)
                results[source] = []
            else:
                results[source] = result or []
        
        # å¦‚æœä¸»è¦æœç´¢æºå¤±è´¥ï¼Œå°è¯•æ•…éšœè½¬ç§»
        if failed_sources:
            await self._handle_search_failures(failed_sources, search_params, results)
        
        return results
    
    async def _check_service_health(self, requested_sources: List[str]) -> List[str]:
        """æ£€æŸ¥æœç´¢æœåŠ¡å¥åº·çŠ¶æ€."""
        available_sources = []
        
        for source in requested_sources:
            try:
                if source == "cnki":
                    # ç®€å•çš„å¥åº·æ£€æŸ¥
                    health_ok = await self._check_cnki_health()
                elif source == "bocha_ai":
                    health_ok = await self._check_bocha_health()
                elif source == "web_crawler":
                    health_ok = await self._check_crawler_health()
                else:
                    health_ok = True  # é»˜è®¤å¯ç”¨
                
                if health_ok:
                    available_sources.append(source)
                else:
                    self.logger.warning(f"Service {source} is not healthy, skipping")
                    
            except Exception as e:
                self.logger.error(f"Health check failed for {source}: {str(e)}")
        
        return available_sources
    
    async def _check_cnki_health(self) -> bool:
        """æ£€æŸ¥CNKIæœåŠ¡å¥åº·çŠ¶æ€."""
        try:
            # ç®€å•çš„è¿é€šæ€§æ£€æŸ¥
            client = self.search_clients["cnki"]
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å¥åº·æ£€æŸ¥é€»è¾‘
            return True
        except Exception:
            return False
    
    async def _check_bocha_health(self) -> bool:
        """æ£€æŸ¥åšæŸ¥AIæœåŠ¡å¥åº·çŠ¶æ€."""
        try:
            client = self.search_clients["bocha_ai"]
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å¥åº·æ£€æŸ¥é€»è¾‘
            return True
        except Exception:
            return False
    
    async def _check_crawler_health(self) -> bool:
        """æ£€æŸ¥çˆ¬è™«æœåŠ¡å¥åº·çŠ¶æ€."""
        try:
            client = self.search_clients["web_crawler"]
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å¥åº·æ£€æŸ¥é€»è¾‘
            return True
        except Exception:
            return False
    
    async def _search_with_source_and_fallback(self, source: str, search_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æŒ‡å®šæ•°æ®æºè¿›è¡Œæœç´¢ï¼ŒåŒ…å«é™çº§æœºåˆ¶."""
        try:
            client = self.search_clients[source]
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶
            result = await self.with_retry(
                client.search,
                max_retries=2,
                delay=1.0,
                keywords=search_params["keywords"],
                search_type=search_params["search_type"],
                limit=search_params["limit"]
            )
            
            # æ£€æŸ¥ç»“æœè´¨é‡
            if not result or len(result) == 0:
                self.logger.warning(f"No results from {source}, trying degraded search")
                return await self._degraded_search(source, search_params)
            
            return result
            
        except Exception as e:
            self.logger.error(f"Error searching with {source}: {str(e)}")
            # å°è¯•é™çº§æœç´¢
            return await self._degraded_search(source, search_params)
    
    async def _degraded_search(self, source: str, search_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é™çº§æœç´¢ç­–ç•¥."""
        try:
            # ç®€åŒ–æœç´¢å‚æ•°
            simplified_params = {
                "keywords": search_params["keywords"][:2],  # å‡å°‘å…³é”®è¯
                "search_type": "general",  # ä½¿ç”¨é€šç”¨æœç´¢
                "limit": min(search_params["limit"], 10)  # å‡å°‘ç»“æœæ•°é‡
            }
            
            client = self.search_clients[source]
            result = await client.search(**simplified_params)
            
            # æ ‡è®°ä¸ºé™çº§ç»“æœ
            for item in result:
                item["is_degraded"] = True
                item["degradation_reason"] = "service_degradation"
            
            return result
            
        except Exception as e:
            self.logger.error(f"Degraded search also failed for {source}: {str(e)}")
            return []
    
    async def _handle_search_failures(self, failed_sources: List[str], search_params: Dict[str, Any], results: Dict[str, List[Dict[str, Any]]]):
        """å¤„ç†æœç´¢å¤±è´¥ï¼Œå®æ–½æ•…éšœè½¬ç§»."""
        # æ•…éšœè½¬ç§»ç­–ç•¥
        failover_mapping = {
            "cnki": ["bocha_ai", "web_crawler"],
            "bocha_ai": ["cnki", "web_crawler"],
            "web_crawler": ["bocha_ai", "cnki"]
        }
        
        for failed_source in failed_sources:
            failover_sources = failover_mapping.get(failed_source, [])
            
            for failover_source in failover_sources:
                if failover_source not in failed_sources and failover_source in self.search_clients:
                    try:
                        self.logger.info(f"Attempting failover from {failed_source} to {failover_source}")
                        
                        # æ‰§è¡Œæ•…éšœè½¬ç§»æœç´¢
                        failover_results = await self._search_with_source_and_fallback(
                            failover_source, search_params
                        )
                        
                        if failover_results:
                            # æ ‡è®°ä¸ºæ•…éšœè½¬ç§»ç»“æœ
                            for item in failover_results:
                                item["is_failover"] = True
                                item["original_source"] = failed_source
                                item["failover_source"] = failover_source
                            
                            # å°†æ•…éšœè½¬ç§»ç»“æœæ·»åŠ åˆ°åŸå§‹æºçš„ç»“æœä¸­
                            results[failed_source] = failover_results[:5]  # é™åˆ¶æ•…éšœè½¬ç§»ç»“æœæ•°é‡
                            
                            self.logger.info(f"Failover successful: {len(failover_results)} results from {failover_source}")
                            break
                            
                    except Exception as e:
                        self.logger.error(f"Failover to {failover_source} failed: {str(e)}")
                        continue
    
    async def _emergency_fallback_search(self, search_params: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """ç´§æ€¥é™çº§æœç´¢ï¼ˆå½“æ‰€æœ‰æœåŠ¡éƒ½ä¸å¯ç”¨æ—¶ï¼‰."""
        self.logger.warning("All search services unavailable, using emergency fallback")
        
        keywords = search_params.get("keywords", ["æŠ€æœ¯"])
        limit = search_params.get("limit", 10)
        
        # ç”ŸæˆåŸºç¡€çš„é™çº§ç»“æœ
        emergency_results = []
        for i in range(min(limit, 5)):
            result = {
                "title": f"[ç´§æ€¥é™çº§] å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„åŸºç¡€ä¿¡æ¯ {i+1}",
                "url": f"https://emergency-fallback.local/{i+1}",
                "content": f"ç”±äºæ‰€æœ‰æœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¿™æ˜¯å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„åŸºç¡€ä¿¡æ¯ã€‚å»ºè®®ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
                "source": "Emergency Fallback",
                "search_type": "emergency",
                "relevance_score": 0.1,
                "is_emergency_fallback": True,
                "generated_at": datetime.now().isoformat()
            }
            emergency_results.append(result)
        
        return {"emergency": emergency_results}
    
    async def _search_with_source(self, source: str, search_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨æŒ‡å®šæ•°æ®æºè¿›è¡Œæœç´¢."""
        try:
            client = self.search_clients[source]
            
            # ä½¿ç”¨é‡è¯•æœºåˆ¶
            return await self.with_retry(
                client.search,
                max_retries=2,
                delay=1.0,
                keywords=search_params["keywords"],
                search_type=search_params["search_type"],
                limit=search_params["limit"]
            )
            
        except Exception as e:
            self.logger.error(f"Error searching with {source}: {str(e)}")
            return []
    
    async def _optimize_search_results(self, search_results: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–å’Œæ’åºæœç´¢ç»“æœï¼ŒåŒ…å«é«˜çº§è´¨é‡è¯„ä¼°ç®—æ³•."""
        all_results = []
        
        # åˆå¹¶æ‰€æœ‰ç»“æœå¹¶è®¡ç®—åˆå§‹è´¨é‡åˆ†æ•°
        for source, results in search_results.items():
            for result in results:
                result["source"] = source
                result["initial_quality_score"] = self._calculate_quality_score(result)
                all_results.append(result)
        
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œç›´æ¥è¿”å›
        if not all_results:
            return []
        
        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ç›¸ä¼¼æ€§å’Œå†…å®¹ç›¸ä¼¼æ€§ï¼‰
        deduplicated_results = await self._advanced_deduplicate_results(all_results)
        
        # è®¡ç®—é«˜çº§è´¨é‡åˆ†æ•°ï¼ˆè€ƒè™‘å»é‡åçš„ä¸Šä¸‹æ–‡ï¼‰
        enhanced_results = await self._calculate_enhanced_quality_scores(deduplicated_results)
        
        # å¤šç»´åº¦æ’åºä¼˜åŒ–
        optimized_results = await self._multi_dimensional_sort(enhanced_results)
        
        # ç»“æœå¤šæ ·æ€§ä¼˜åŒ–
        diversified_results = await self._optimize_result_diversity(optimized_results)
        
        return diversified_results[:20]  # è¿”å›å‰20ä¸ªç»“æœ
    
    async def _advanced_deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é«˜çº§å»é‡ç®—æ³•ï¼ŒåŸºäºæ ‡é¢˜å’Œå†…å®¹ç›¸ä¼¼æ€§."""
        if not results:
            return []
        
        deduplicated = []
        seen_signatures = set()
        
        for result in results:
            # ç”Ÿæˆå†…å®¹ç­¾å
            signature = self._generate_content_signature(result)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼å†…å®¹
            is_duplicate = False
            for seen_sig in seen_signatures:
                if self._calculate_signature_similarity(signature, seen_sig) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                seen_signatures.add(signature)
                deduplicated.append(result)
            else:
                # å¦‚æœæ˜¯é‡å¤å†…å®¹ï¼Œä½†è´¨é‡æ›´é«˜ï¼Œåˆ™æ›¿æ¢
                existing_index = self._find_similar_result_index(deduplicated, result)
                if existing_index >= 0:
                    existing_quality = deduplicated[existing_index].get("initial_quality_score", 0)
                    current_quality = result.get("initial_quality_score", 0)
                    
                    if current_quality > existing_quality:
                        deduplicated[existing_index] = result
        
        return deduplicated
    
    def _generate_content_signature(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆå†…å®¹ç­¾åç”¨äºå»é‡."""
        title = result.get("title", "").lower().strip()
        content = result.get("content", "").lower().strip()
        
        # æå–å…³é”®ç‰¹å¾
        title_words = set(title.split()[:10])  # æ ‡é¢˜å‰10ä¸ªè¯
        content_words = set(content.split()[:50])  # å†…å®¹å‰50ä¸ªè¯
        
        # ç”Ÿæˆç­¾å
        signature_parts = [
            "title:" + "_".join(sorted(title_words)),
            "content:" + "_".join(sorted(list(content_words)[:20]))  # é™åˆ¶é•¿åº¦
        ]
        
        return "|".join(signature_parts)
    
    def _calculate_signature_similarity(self, sig1: str, sig2: str) -> float:
        """è®¡ç®—ç­¾åç›¸ä¼¼æ€§."""
        try:
            parts1 = sig1.split("|")
            parts2 = sig2.split("|")
            
            if len(parts1) != len(parts2):
                return 0.0
            
            similarities = []
            for p1, p2 in zip(parts1, parts2):
                # ç®€å•çš„Jaccardç›¸ä¼¼æ€§
                words1 = set(p1.split("_"))
                words2 = set(p2.split("_"))
                
                intersection = len(words1 & words2)
                union = len(words1 | words2)
                
                if union == 0:
                    similarities.append(0.0)
                else:
                    similarities.append(intersection / union)
            
            return sum(similarities) / len(similarities)
            
        except Exception:
            return 0.0
    
    def _find_similar_result_index(self, results: List[Dict[str, Any]], target: Dict[str, Any]) -> int:
        """æŸ¥æ‰¾ç›¸ä¼¼ç»“æœçš„ç´¢å¼•."""
        target_sig = self._generate_content_signature(target)
        
        for i, result in enumerate(results):
            result_sig = self._generate_content_signature(result)
            if self._calculate_signature_similarity(target_sig, result_sig) > 0.8:
                return i
        
        return -1
    
    async def _calculate_enhanced_quality_scores(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è®¡ç®—å¢å¼ºçš„è´¨é‡åˆ†æ•°."""
        for result in results:
            # åŸºç¡€è´¨é‡åˆ†æ•°
            base_score = result.get("initial_quality_score", 0.5)
            
            # å†…å®¹è´¨é‡è¯„ä¼°
            content_quality = self._assess_content_quality(result)
            
            # æ¥æºæƒå¨æ€§è¯„ä¼°
            source_authority = self._assess_source_authority(result)
            
            # æ—¶æ•ˆæ€§è¯„ä¼°
            freshness = self._assess_freshness(result)
            
            # ç›¸å…³æ€§è¯„ä¼°ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰
            relevance = self._assess_relevance(result)
            
            # å®Œæ•´æ€§è¯„ä¼°
            completeness = self._assess_completeness(result)
            
            # ç»¼åˆè´¨é‡åˆ†æ•°
            enhanced_score = (
                base_score * 0.2 +
                content_quality * 0.25 +
                source_authority * 0.2 +
                freshness * 0.15 +
                relevance * 0.15 +
                completeness * 0.05
            )
            
            result["enhanced_quality_score"] = min(enhanced_score, 1.0)
            result["quality_breakdown"] = {
                "base_score": base_score,
                "content_quality": content_quality,
                "source_authority": source_authority,
                "freshness": freshness,
                "relevance": relevance,
                "completeness": completeness
            }
        
        return results
    
    def _assess_content_quality(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°å†…å®¹è´¨é‡."""
        content = result.get("content", "")
        title = result.get("title", "")
        
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # å†…å®¹é•¿åº¦è¯„ä¼°
        if len(content) > 200:
            score += 0.2
        elif len(content) > 100:
            score += 0.1
        
        # æ ‡é¢˜è´¨é‡è¯„ä¼°
        if len(title) > 10 and len(title) < 100:
            score += 0.1
        
        # å†…å®¹ç»“æ„è¯„ä¼°ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        if "ã€‚" in content or "." in content:  # æœ‰å¥å·ï¼Œè¯´æ˜æœ‰å®Œæ•´å¥å­
            score += 0.1
        
        # ä¸“ä¸šæœ¯è¯­å¯†åº¦ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        tech_terms = ["æŠ€æœ¯", "æ–¹æ³•", "ç³»ç»Ÿ", "ç®—æ³•", "æ¨¡å‹", "åˆ†æ", "ç ”ç©¶"]
        term_count = sum(1 for term in tech_terms if term in content)
        if term_count >= 2:
            score += 0.1
        
        return min(score, 1.0)
    
    def _assess_source_authority(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°æ¥æºæƒå¨æ€§."""
        source = result.get("source", "").lower()
        
        # æ¥æºæƒå¨æ€§æ˜ å°„
        authority_scores = {
            "cnki": 0.9,
            "bocha ai": 0.7,
            "web_crawler": 0.5,
            "emergency": 0.1
        }
        
        base_authority = 0.5
        for source_key, score in authority_scores.items():
            if source_key in source:
                base_authority = score
                break
        
        # è€ƒè™‘æ˜¯å¦ä¸ºé™çº§æˆ–æ•…éšœè½¬ç§»ç»“æœ
        if result.get("is_degraded"):
            base_authority *= 0.8
        
        if result.get("is_failover"):
            base_authority *= 0.9
        
        if result.get("is_emergency_fallback"):
            base_authority = 0.1
        
        return base_authority
    
    def _assess_freshness(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°æ—¶æ•ˆæ€§."""
        try:
            pub_date_str = result.get("publication_date") or result.get("generated_at", "")
            if not pub_date_str:
                return 0.5  # é»˜è®¤ä¸­ç­‰æ—¶æ•ˆæ€§
            
            # ç®€å•çš„æ—¶æ•ˆæ€§è¯„ä¼°
            from datetime import datetime
            try:
                if "2024" in pub_date_str:
                    return 0.9
                elif "2023" in pub_date_str:
                    return 0.7
                elif "2022" in pub_date_str:
                    return 0.5
                else:
                    return 0.3
            except:
                return 0.5
                
        except Exception:
            return 0.5
    
    def _assess_relevance(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°ç›¸å…³æ€§."""
        # è¿™é‡Œåº”è¯¥åŸºäºæœç´¢å…³é”®è¯è®¡ç®—ç›¸å…³æ€§
        # ç®€åŒ–å®ç°
        relevance_score = result.get("relevance_score", 0.5)
        
        # å¦‚æœæœ‰ç°æœ‰çš„ç›¸å…³æ€§åˆ†æ•°ï¼Œä½¿ç”¨å®ƒ
        if "relevance_score" in result:
            return result["relevance_score"]
        
        return 0.5  # é»˜è®¤ç›¸å…³æ€§
    
    def _assess_completeness(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°å®Œæ•´æ€§."""
        required_fields = ["title", "content", "url"]
        optional_fields = ["summary", "publication_date", "source"]
        
        required_score = sum(1 for field in required_fields if result.get(field)) / len(required_fields)
        optional_score = sum(1 for field in optional_fields if result.get(field)) / len(optional_fields)
        
        return (required_score * 0.7 + optional_score * 0.3)
    
    async def _multi_dimensional_sort(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤šç»´åº¦æ’åºä¼˜åŒ–."""
        # ä¸»è¦æŒ‰å¢å¼ºè´¨é‡åˆ†æ•°æ’åº
        primary_sorted = sorted(
            results,
            key=lambda x: x.get("enhanced_quality_score", 0),
            reverse=True
        )
        
        # åœ¨ç›¸ä¼¼è´¨é‡åˆ†æ•°çš„ç»“æœä¸­ï¼ŒæŒ‰æ—¶æ•ˆæ€§è¿›è¡ŒäºŒæ¬¡æ’åº
        final_sorted = []
        current_group = []
        current_score = None
        
        for result in primary_sorted:
            score = result.get("enhanced_quality_score", 0)
            
            if current_score is None or abs(score - current_score) < 0.05:  # è´¨é‡åˆ†æ•°ç›¸è¿‘
                current_group.append(result)
                current_score = score
            else:
                # å¯¹å½“å‰ç»„æŒ‰æ—¶æ•ˆæ€§æ’åº
                if current_group:
                    current_group.sort(
                        key=lambda x: x.get("quality_breakdown", {}).get("freshness", 0),
                        reverse=True
                    )
                    final_sorted.extend(current_group)
                
                current_group = [result]
                current_score = score
        
        # å¤„ç†æœ€åä¸€ç»„
        if current_group:
            current_group.sort(
                key=lambda x: x.get("quality_breakdown", {}).get("freshness", 0),
                reverse=True
            )
            final_sorted.extend(current_group)
        
        return final_sorted
    
    async def _optimize_result_diversity(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–ç»“æœå¤šæ ·æ€§ï¼Œé¿å…è¿‡åº¦ç›¸ä¼¼çš„ç»“æœ."""
        if len(results) <= 5:
            return results  # ç»“æœå¤ªå°‘ï¼Œä¸éœ€è¦å¤šæ ·æ€§ä¼˜åŒ–
        
        diversified = []
        remaining = results.copy()
        
        # é€‰æ‹©è´¨é‡æœ€é«˜çš„ç»“æœä½œä¸ºç¬¬ä¸€ä¸ª
        if remaining:
            best_result = remaining.pop(0)
            diversified.append(best_result)
        
        # é€ä¸ªé€‰æ‹©ä¸å·²é€‰ç»“æœå·®å¼‚è¾ƒå¤§çš„ç»“æœ
        while remaining and len(diversified) < 20:
            best_candidate = None
            best_diversity_score = -1
            
            for candidate in remaining:
                # è®¡ç®—ä¸å·²é€‰ç»“æœçš„å¤šæ ·æ€§åˆ†æ•°
                diversity_score = self._calculate_diversity_score(candidate, diversified)
                
                # ç»¼åˆè€ƒè™‘è´¨é‡å’Œå¤šæ ·æ€§
                combined_score = (
                    candidate.get("enhanced_quality_score", 0) * 0.7 +
                    diversity_score * 0.3
                )
                
                if combined_score > best_diversity_score:
                    best_diversity_score = combined_score
                    best_candidate = candidate
            
            if best_candidate:
                diversified.append(best_candidate)
                remaining.remove(best_candidate)
            else:
                break
        
        return diversified
    
    def _calculate_diversity_score(self, candidate: Dict[str, Any], selected: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å€™é€‰ç»“æœä¸å·²é€‰ç»“æœçš„å¤šæ ·æ€§åˆ†æ•°."""
        if not selected:
            return 1.0
        
        candidate_sig = self._generate_content_signature(candidate)
        
        similarities = []
        for selected_result in selected:
            selected_sig = self._generate_content_signature(selected_result)
            similarity = self._calculate_signature_similarity(candidate_sig, selected_sig)
            similarities.append(similarity)
        
        # å¤šæ ·æ€§åˆ†æ•° = 1 - æœ€å¤§ç›¸ä¼¼æ€§
        max_similarity = max(similarities) if similarities else 0
        diversity_score = 1.0 - max_similarity
        
        return max(diversity_score, 0.0)
    
    def _calculate_quality_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—ç»“æœè´¨é‡åˆ†æ•°."""
        score = 0.0
        
        # ç›¸å…³æ€§è¯„åˆ†ï¼ˆåŸºäºæ ‡é¢˜å’Œå†…å®¹å…³é”®è¯åŒ¹é…ï¼‰
        relevance_score = self._calculate_relevance_score(result)
        score += relevance_score * self.quality_weights["relevance"]
        
        # æƒå¨æ€§è¯„åˆ†ï¼ˆåŸºäºæ¥æºï¼‰
        authority_score = self._calculate_authority_score(result)
        score += authority_score * self.quality_weights["authority"]
        
        # æ—¶æ•ˆæ€§è¯„åˆ†ï¼ˆåŸºäºå‘å¸ƒæ—¶é—´ï¼‰
        freshness_score = self._calculate_freshness_score(result)
        score += freshness_score * self.quality_weights["freshness"]
        
        # å®Œæ•´æ€§è¯„åˆ†ï¼ˆåŸºäºä¿¡æ¯å®Œæ•´åº¦ï¼‰
        completeness_score = self._calculate_completeness_score(result)
        score += completeness_score * self.quality_weights["completeness"]
        
        return min(score, 1.0)
    
    def _calculate_relevance_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°."""
        # ç®€åŒ–çš„ç›¸å…³æ€§è®¡ç®—
        title = result.get("title", "").lower()
        content = result.get("content", "").lower()
        
        # è¿™é‡Œåº”è¯¥æœ‰æ›´å¤æ‚çš„ç›¸å…³æ€§ç®—æ³•
        # æš‚æ—¶ä½¿ç”¨ç®€å•çš„å…³é”®è¯åŒ¹é…
        return 0.7  # é»˜è®¤ç›¸å…³æ€§
    
    def _calculate_authority_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—æƒå¨æ€§åˆ†æ•°."""
        source = result.get("source", "")
        
        authority_scores = {
            "cnki": 0.9,      # å­¦æœ¯æƒå¨æ€§é«˜
            "bocha_ai": 0.7,  # AIæœç´¢ä¸­ç­‰æƒå¨æ€§
            "web_crawler": 0.5  # ç½‘é¡µçˆ¬å–æƒå¨æ€§è¾ƒä½
        }
        
        return authority_scores.get(source, 0.5)
    
    def _calculate_freshness_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°."""
        # ç®€åŒ–çš„æ—¶æ•ˆæ€§è®¡ç®—
        # å®é™…åº”è¯¥åŸºäºå‘å¸ƒæ—¶é—´è®¡ç®—
        return 0.6  # é»˜è®¤æ—¶æ•ˆæ€§
    
    def _calculate_completeness_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—å®Œæ•´æ€§åˆ†æ•°."""
        required_fields = ["title", "content", "url"]
        present_fields = sum(1 for field in required_fields if result.get(field))
        
        return present_fields / len(required_fields)
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é™¤é‡å¤ç»“æœ."""
        seen_titles = set()
        deduplicated = []
        
        for result in results:
            title = result.get("title", "").lower().strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                deduplicated.append(result)
        
        return deduplicated
    
    async def _generate_search_response(self, search_params: Dict[str, Any], results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæœç´¢å“åº”å†…å®¹."""
        keywords = ", ".join(search_params["keywords"])
        results_count = len(results)
        
        response = f"æ ¹æ®æ‚¨çš„æœç´¢éœ€æ±‚ã€Œ{keywords}ã€ï¼Œæˆ‘ä¸ºæ‚¨æ‰¾åˆ°äº† {results_count} æ¡ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        
        # æ˜¾ç¤ºå‰5ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        for i, result in enumerate(results[:5], 1):
            title = result.get("title", "æ— æ ‡é¢˜")
            source = result.get("source", "æœªçŸ¥æ¥æº")
            url = result.get("url", "")
            quality_score = result.get("quality_score", 0)
            
            response += f"**{i}. {title}**\n"
            response += f"   æ¥æºï¼š{source} | è´¨é‡è¯„åˆ†ï¼š{quality_score:.2f}\n"
            
            if url:
                response += f"   é“¾æ¥ï¼š{url}\n"
            
            # æ·»åŠ æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            summary = result.get("summary", result.get("content", ""))
            if summary:
                summary = summary[:200] + "..." if len(summary) > 200 else summary
                response += f"   æ‘˜è¦ï¼š{summary}\n"
            
            response += "\n"
        
        # æ·»åŠ æœç´¢ç»Ÿè®¡ä¿¡æ¯
        if results_count > 5:
            response += f"è¿˜æœ‰ {results_count - 5} æ¡ç›¸å…³ç»“æœã€‚å¦‚éœ€æŸ¥çœ‹æ›´å¤šä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨æ„Ÿå…´è¶£çš„å…·ä½“æ–¹å‘ã€‚\n\n"
        
        # æ·»åŠ æœç´¢å»ºè®®
        response += "ğŸ’¡ **æœç´¢å»ºè®®**ï¼š\n"
        response += "- å¦‚éœ€æ›´ç²¾ç¡®çš„ç»“æœï¼Œè¯·æä¾›æ›´å…·ä½“çš„å…³é”®è¯\n"
        response += "- å¦‚éœ€å­¦æœ¯æ–‡çŒ®ï¼Œæˆ‘å¯ä»¥é‡ç‚¹æœç´¢CNKIæ•°æ®åº“\n"
        response += "- å¦‚éœ€æœ€æ–°èµ„è®¯ï¼Œæˆ‘å¯ä»¥åŠ å¼ºç½‘ç»œæœç´¢\n"
        
        return response
    
    async def _generate_mock_search_response(self, request: UserRequest) -> AgentResponse:
        """ç”Ÿæˆæ¨¡æ‹Ÿæœç´¢å“åº”ï¼Œç”¨äºæµ‹è¯•æ¨¡å¼."""
        keywords = request.context.get("keywords", ["æµ‹è¯•"])
        
        # æ¨¡æ‹Ÿæœç´¢ç»“æœ
        mock_results = [
            {
                "title": f"å…³äº{keywords[0]}çš„ä¸“åˆ©æŠ€æœ¯ç ”ç©¶",
                "abstract": f"æœ¬å‘æ˜æ¶‰åŠ{keywords[0]}é¢†åŸŸçš„æŠ€æœ¯åˆ›æ–°ï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆã€‚",
                "applicant": "æµ‹è¯•å…¬å¸",
                "publication_date": "2024-01-01",
                "patent_number": "CN123456789A",
                "ipc_class": "G06F",
                "relevance_score": 0.95
            },
            {
                "title": f"{keywords[0]}ç³»ç»Ÿçš„ä¼˜åŒ–æ–¹æ³•",
                "abstract": f"é’ˆå¯¹ç°æœ‰{keywords[0]}ç³»ç»Ÿçš„ä¸è¶³ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
                "applicant": "åˆ›æ–°ç§‘æŠ€æœ‰é™å…¬å¸",
                "publication_date": "2024-02-01",
                "patent_number": "CN987654321A",
                "ipc_class": "H04L",
                "relevance_score": 0.88
            }
        ]
        
        response_content = f"""
ğŸ” **ä¸“åˆ©æœç´¢ç»“æœ** (æ¨¡æ‹Ÿæ¨¡å¼)

**æœç´¢å…³é”®è¯**: {', '.join(keywords)}
**ç»“æœæ•°é‡**: {len(mock_results)}

**æœç´¢ç»“æœ**:
"""
        
        for i, result in enumerate(mock_results, 1):
            response_content += f"""
{i}. **{result['title']}**
   - ç”³è¯·äºº: {result['applicant']}
   - å…¬å¼€æ—¥æœŸ: {result['publication_date']}
   - ä¸“åˆ©å·: {result['patent_number']}
   - IPCåˆ†ç±»: {result['ipc_class']}
   - ç›¸å…³åº¦: {result['relevance_score']:.2f}
   - æ‘˜è¦: {result['abstract']}
"""
        
        response_content += "\nâœ… æœç´¢å®Œæˆ (æ¨¡æ‹Ÿæ•°æ®)"
        
        return AgentResponse(
            agent_id=self.agent_id,
            agent_type=self.agent_type,
            response_content=response_content,
            confidence=0.85,
            collaboration_needed=False,
            metadata={
                "mock_mode": True,
                "results_count": len(mock_results),
                "keywords": keywords,
                "processing_time": 0.1
            }
        )
    
    def _generate_search_actions(self, search_params: Dict[str, Any], results: List[Dict[str, Any]]) -> List[Action]:
        """ç”Ÿæˆæœç´¢ç›¸å…³çš„åç»­åŠ¨ä½œ."""
        actions = []
        
        # æ·±åº¦æœç´¢åŠ¨ä½œ
        actions.append(Action(
            action_type="deep_search",
            parameters={
                "keywords": search_params["keywords"],
                "focus_area": "academic"
            },
            description="è¿›è¡Œæ›´æ·±åº¦çš„å­¦æœ¯æœç´¢"
        ))
        
        # ç»“æœå¯¼å‡ºåŠ¨ä½œ
        if results:
            actions.append(Action(
                action_type="export_results",
                parameters={
                    "format": "excel",
                    "results_count": len(results)
                },
                description="å¯¼å‡ºæœç´¢ç»“æœåˆ°Excel"
            ))
        
        # ç›¸å…³æœç´¢å»ºè®®
        actions.append(Action(
            action_type="related_search",
            parameters={
                "base_keywords": search_params["keywords"]
            },
            description="æœç´¢ç›¸å…³ä¸»é¢˜"
        ))
        
        return actions
    
    # ç›‘æ§å’ŒæŒ‡æ ‡è®°å½•æ–¹æ³•
    async def _log_search_metrics(self, search_params: Dict[str, Any], results: List[Dict[str, Any]], duration: float):
        """è®°å½•æœç´¢æŒ‡æ ‡åˆ°ç›‘æ§ç³»ç»Ÿ."""
        try:
            metrics = {
                "search_duration": duration,
                "results_count": len(results),
                "keywords_count": len(search_params.get("keywords", [])),
                "sources_used": len(search_params.get("sources", [])),
                "search_type": search_params.get("search_type", "general"),
                "cache_hit": any(r.get("from_cache", False) for r in results),
                "quality_scores": [r.get("quality_score", 0) for r in results],
                "average_quality": sum(r.get("quality_score", 0) for r in results) / max(len(results), 1)
            }
            
            # è¿™é‡Œåº”è¯¥é›†æˆåˆ°ç°æœ‰çš„MonitoringSystem
            # ä¾‹å¦‚: await self.monitoring_system.record_metrics("patent_search", metrics)
            
            self.logger.info(f"Search metrics recorded: {metrics}")
            
        except Exception as e:
            self.logger.error(f"Failed to log search metrics: {str(e)}")
    
    async def _log_search_failure(self, error_message: str, duration: float):
        """è®°å½•æœç´¢å¤±è´¥æŒ‡æ ‡."""
        try:
            failure_metrics = {
                "failure_duration": duration,
                "error_message": error_message,
                "failure_timestamp": datetime.now().isoformat(),
                "agent_id": self.agent_id
            }
            
            # è¿™é‡Œåº”è¯¥é›†æˆåˆ°ç°æœ‰çš„MonitoringSystem
            # ä¾‹å¦‚: await self.monitoring_system.record_failure("patent_search", failure_metrics)
            
            self.logger.error(f"Search failure recorded: {failure_metrics}")
            
        except Exception as e:
            self.logger.error(f"Failed to log search failure: {str(e)}")
    
    # å¥åº·æ£€æŸ¥å¢å¼º
    async def _health_check_specific(self) -> bool:
        """ä¸“åˆ©æœç´¢Agentç‰¹å®šçš„å¥åº·æ£€æŸ¥."""
        try:
            # è°ƒç”¨çˆ¶ç±»å¥åº·æ£€æŸ¥
            base_health = await super()._health_check_specific()
            if not base_health:
                return False
            
            # æ£€æŸ¥æœç´¢å®¢æˆ·ç«¯å¥åº·çŠ¶æ€
            for client_name, client in self.search_clients.items():
                try:
                    if hasattr(client, 'close'):  # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æœ‰ä¼šè¯
                        # ç®€å•çš„å¥åº·æ£€æŸ¥
                        test_keywords = ["test"]
                        test_results = await client.search(test_keywords, "general", 1)
                        
                        if test_results is None:
                            self.logger.error(f"Search client {client_name} health check failed")
                            return False
                            
                except Exception as e:
                    self.logger.error(f"Health check failed for {client_name}: {str(e)}")
                    # ä¸å› ä¸ºå•ä¸ªå®¢æˆ·ç«¯å¤±è´¥è€Œæ•´ä½“å¤±è´¥
                    continue
            
            return True
            
        except Exception as e:
            self.logger.error(f"Patent search agent health check failed: {str(e)}")
            return False
    
    # æ¸…ç†èµ„æº
    async def _cleanup_specific(self) -> None:
        """æ¸…ç†æœç´¢Agentç‰¹å®šçš„èµ„æº."""
        try:
            # å…³é—­æ‰€æœ‰æœç´¢å®¢æˆ·ç«¯çš„ä¼šè¯
            for client_name, client in self.search_clients.items():
                try:
                    if hasattr(client, 'close'):
                        await client.close()
                        self.logger.info(f"Closed {client_name} client session")
                except Exception as e:
                    self.logger.error(f"Failed to close {client_name} client: {str(e)}")
            
            # è°ƒç”¨çˆ¶ç±»æ¸…ç†
            await super()._cleanup_specific()
            
        except Exception as e:
            self.logger.error(f"Patent search agent cleanup failed: {str(e)}")


class CNKIClient:
    """CNKIå­¦æœ¯æœç´¢å®¢æˆ·ç«¯ï¼Œé›†æˆCNKI AIæœç´¢API."""
    
    def __init__(self):
        self.base_url = "https://kns.cnki.net"
        self.api_url = "https://api.cnki.net"  # å‡è®¾çš„APIç«¯ç‚¹
        self.timeout = 30
        self.rate_limit = 5  # æ¯ç§’æœ€å¤š5ä¸ªè¯·æ±‚
        self.last_request_time = 0
        self.session = None
        
        # æœç´¢é…ç½®
        self.search_config = {
            "academic": {
                "databases": ["CJFD", "CDFD", "CMFD"],  # æœŸåˆŠã€åšå£«ã€ç¡•å£«è®ºæ–‡åº“
                "fields": ["TI", "AB", "KY"],  # æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯
                "sort": "RELEVANCE"
            },
            "patent": {
                "databases": ["SCPD"],  # ä¸“åˆ©æ•°æ®åº“
                "fields": ["TI", "AB", "CL"],  # æ ‡é¢˜ã€æ‘˜è¦ã€åˆ†ç±»å·
                "sort": "PUBLISH_DATE"
            },
            "general": {
                "databases": ["CJFD", "SCPD"],
                "fields": ["TI", "AB"],
                "sort": "RELEVANCE"
            }
        }
    
    async def search(self, keywords: List[str], search_type: str = "general", limit: int = 20) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒCNKIæœç´¢."""
        try:
            # é€Ÿç‡é™åˆ¶
            await self._rate_limit_check()
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            query = self._build_search_query(keywords, search_type)
            
            # æ‰§è¡Œæœç´¢
            results = await self._execute_search(query, search_type, limit)
            
            # å¤„ç†å’Œæ ‡å‡†åŒ–ç»“æœ
            processed_results = self._process_search_results(results, search_type)
            
            logger.info(f"CNKI search completed: {len(processed_results)} results for keywords: {keywords}")
            return processed_results
            
        except Exception as e:
            logger.error(f"CNKI search failed: {str(e)}")
            # è¿”å›é™çº§ç»“æœ
            return await self._get_fallback_results(keywords, search_type, limit)
    
    async def _rate_limit_check(self):
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶."""
        import time
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < (1.0 / self.rate_limit):
            sleep_time = (1.0 / self.rate_limit) - time_since_last
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def _build_search_query(self, keywords: List[str], search_type: str) -> Dict[str, Any]:
        """æ„å»ºCNKIæœç´¢æŸ¥è¯¢."""
        config = self.search_config.get(search_type, self.search_config["general"])
        
        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
        query_parts = []
        for keyword in keywords[:3]:  # é™åˆ¶å…³é”®è¯æ•°é‡
            # å¯¹æ¯ä¸ªå­—æ®µæ„å»ºæŸ¥è¯¢
            field_queries = []
            for field in config["fields"]:
                field_queries.append(f"{field}='{keyword}'")
            query_parts.append("(" + " OR ".join(field_queries) + ")")
        
        query_string = " AND ".join(query_parts)
        
        return {
            "query": query_string,
            "databases": config["databases"],
            "sort": config["sort"],
            "fields": config["fields"]
        }
    
    async def _execute_search(self, query: Dict[str, Any], search_type: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡Œå®é™…çš„æœç´¢è¯·æ±‚."""
        try:
            # åˆå§‹åŒ–ä¼šè¯
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout),
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                        "Accept": "application/json, text/plain, */*",
                        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                        "Content-Type": "application/json"
                    }
                )
            
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            api_params = {
                "query": query["query"],
                "database": ",".join(query["databases"]),
                "sort": query["sort"],
                "size": min(limit, 50),
                "format": "json",
                "fields": ",".join(query["fields"])
            }
            
            # å°è¯•çœŸå®APIè°ƒç”¨ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            try:
                real_results = await self._real_cnki_api_call(api_params, search_type)
                if real_results:
                    return real_results
            except Exception as e:
                logger.warning(f"Real CNKI API call failed, using mock data: {str(e)}")
            
            # ä½¿ç”¨å¢å¼ºçš„æ¨¡æ‹ŸAPIè°ƒç”¨
            mock_results = await self._enhanced_mock_cnki_api_call(api_params, search_type)
            return mock_results
            
        except aiohttp.ClientError as e:
            logger.error(f"CNKI API request failed: {str(e)}")
            raise
        except asyncio.TimeoutError:
            logger.error("CNKI API request timeout")
            raise
    
    async def _real_cnki_api_call(self, params: Dict[str, Any], search_type: str) -> Optional[List[Dict[str, Any]]]:
        """å°è¯•çœŸå®çš„CNKI APIè°ƒç”¨."""
        try:
            # è¿™é‡Œå¯ä»¥é…ç½®çœŸå®çš„CNKI APIç«¯ç‚¹
            # éœ€è¦APIå¯†é’¥å’Œæ­£ç¡®çš„ç«¯ç‚¹URL
            api_endpoint = f"{self.api_url}/search"
            
            # æ·»åŠ è®¤è¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            headers = {}
            api_key = self._get_api_key()
            if api_key:
                headers["Authorization"] = f"Bearer {api_key}"
            
            async with self.session.post(api_endpoint, json=params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_real_api_response(data, search_type)
                elif response.status == 401:
                    logger.error("CNKI API authentication failed")
                    return None
                elif response.status == 429:
                    logger.error("CNKI API rate limit exceeded")
                    return None
                else:
                    logger.error(f"CNKI API returned status {response.status}")
                    return None
                    
        except Exception as e:
            logger.debug(f"Real CNKI API call failed: {str(e)}")
            return None
    
    def _get_api_key(self) -> Optional[str]:
        """è·å–CNKI APIå¯†é’¥."""
        import os
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è·å–APIå¯†é’¥
        return os.getenv("CNKI_API_KEY")
    
    def _parse_real_api_response(self, data: Dict[str, Any], search_type: str) -> List[Dict[str, Any]]:
        """è§£æçœŸå®APIå“åº”."""
        results = []
        
        # æ ¹æ®CNKI APIçš„å®é™…å“åº”æ ¼å¼è§£ææ•°æ®
        items = data.get("items", []) or data.get("results", [])
        
        for item in items:
            result = {
                "title": item.get("title", ""),
                "authors": item.get("authors", []),
                "abstract": item.get("abstract", ""),
                "keywords": item.get("keywords", []),
                "publication_date": item.get("publication_date", ""),
                "source": item.get("source", ""),
                "doi": item.get("doi", ""),
                "url": item.get("url", ""),
                "database": item.get("database", ""),
                "relevance_score": item.get("score", 0.5)
            }
            results.append(result)
        
        return results
    
    async def _enhanced_mock_cnki_api_call(self, params: Dict[str, Any], search_type: str) -> List[Dict[str, Any]]:
        """æ¨¡æ‹ŸCNKI APIè°ƒç”¨ï¼ˆå®é™…å®ç°æ—¶æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨ï¼‰."""
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.5)
        
        results = []
        result_count = min(params.get("size", 10), 15)
        
        for i in range(result_count):
            if search_type == "academic":
                result = {
                    "title": f"åŸºäºæ·±åº¦å­¦ä¹ çš„æŠ€æœ¯ç ”ç©¶ä¸åº”ç”¨ {i+1}",
                    "authors": ["å¼ æ•™æˆ", "æç ”ç©¶å‘˜", "ç‹åšå£«"],
                    "journal": "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦æŠ¥",
                    "volume": "45",
                    "issue": "3",
                    "pages": f"{100+i*10}-{110+i*10}",
                    "publication_date": f"2024-0{(i%12)+1:02d}-01",
                    "abstract": "æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§...",
                    "keywords": ["æ·±åº¦å­¦ä¹ ", "æŠ€æœ¯åˆ›æ–°", "ç®—æ³•ä¼˜åŒ–"],
                    "doi": f"10.1000/journal.2024.{i+1:04d}",
                    "citation_count": 15 + i * 3,
                    "database": "CJFD"
                }
            elif search_type == "patent":
                result = {
                    "title": f"ä¸€ç§åŸºäºäººå·¥æ™ºèƒ½çš„æŠ€æœ¯æ–¹æ³• {i+1}",
                    "patent_number": f"CN{202400000000 + i}A",
                    "applicant": "æŸç§‘æŠ€æœ‰é™å…¬å¸",
                    "inventor": ["å‘æ˜äººç”²", "å‘æ˜äººä¹™"],
                    "application_date": f"2024-0{(i%12)+1:02d}-01",
                    "publication_date": f"2024-0{(i%12)+2:02d}-01",
                    "abstract": "æœ¬å‘æ˜æ¶‰åŠäººå·¥æ™ºèƒ½æŠ€æœ¯é¢†åŸŸï¼Œæä¾›äº†ä¸€ç§æ–°çš„æŠ€æœ¯è§£å†³æ–¹æ¡ˆ...",
                    "ipc_class": ["G06N3/08", "G06F17/30"],
                    "legal_status": "å®¡æŸ¥ä¸­",
                    "database": "SCPD"
                }
            else:
                result = {
                    "title": f"æŠ€æœ¯å‘å±•ç°çŠ¶ä¸è¶‹åŠ¿åˆ†æ {i+1}",
                    "authors": ["ä¸“å®¶A", "å­¦è€…B"],
                    "source": "æŠ€æœ¯å‘å±•æŠ¥å‘Š",
                    "publication_date": f"2024-0{(i%12)+1:02d}-01",
                    "abstract": "æœ¬æ–‡åˆ†æäº†å½“å‰æŠ€æœ¯å‘å±•çš„ç°çŠ¶å’Œæœªæ¥è¶‹åŠ¿...",
                    "keywords": ["æŠ€æœ¯å‘å±•", "è¶‹åŠ¿åˆ†æ", "åˆ›æ–°"],
                    "database": "CJFD"
                }
            
            result["cnki_url"] = f"https://kns.cnki.net/kcms/detail/{result.get('database', 'CJFD')}.{i+1:06d}.html"
            result["relevance_score"] = 0.9 - (i * 0.05)  # é€’å‡çš„ç›¸å…³æ€§åˆ†æ•°
            results.append(result)
        
        return results
    
    def _process_search_results(self, results: List[Dict[str, Any]], search_type: str) -> List[Dict[str, Any]]:
        """å¤„ç†å’Œæ ‡å‡†åŒ–æœç´¢ç»“æœ."""
        processed_results = []
        
        for result in results:
            processed_result = {
                "title": result.get("title", ""),
                "url": result.get("cnki_url", ""),
                "content": result.get("abstract", ""),
                "source": "CNKI",
                "search_type": search_type,
                "relevance_score": result.get("relevance_score", 0.5),
                "publication_date": result.get("publication_date", ""),
                "database": result.get("database", ""),
                "raw_data": result  # ä¿ç•™åŸå§‹æ•°æ®
            }
            
            # æ ¹æ®æœç´¢ç±»å‹æ·»åŠ ç‰¹å®šå­—æ®µ
            if search_type == "academic":
                processed_result.update({
                    "authors": result.get("authors", []),
                    "journal": result.get("journal", ""),
                    "keywords": result.get("keywords", []),
                    "citation_count": result.get("citation_count", 0)
                })
            elif search_type == "patent":
                processed_result.update({
                    "patent_number": result.get("patent_number", ""),
                    "applicant": result.get("applicant", ""),
                    "inventor": result.get("inventor", []),
                    "ipc_class": result.get("ipc_class", [])
                })
            
            processed_results.append(processed_result)
        
        return processed_results
    
    async def _get_fallback_results(self, keywords: List[str], search_type: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–é™çº§ç»“æœï¼ˆå½“APIè°ƒç”¨å¤±è´¥æ—¶ï¼‰."""
        logger.info("Using CNKI fallback results")
        
        fallback_results = []
        for i in range(min(limit, 5)):  # é™çº§æ—¶è¿”å›è¾ƒå°‘ç»“æœ
            result = {
                "title": f"[é™çº§ç»“æœ] å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„ç ”ç©¶ {i+1}",
                "url": f"https://kns.cnki.net/fallback/{i+1}",
                "content": f"ç”±äºç½‘ç»œé—®é¢˜ï¼Œè¿™æ˜¯å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„é™çº§æœç´¢ç»“æœ...",
                "source": "CNKI",
                "search_type": search_type,
                "relevance_score": 0.3,
                "is_fallback": True,
                "publication_date": "2024-01-01"
            }
            fallback_results.append(result)
        
        return fallback_results
    
    async def close(self):
        """å…³é—­HTTPä¼šè¯."""
        if self.session:
            await self.session.close()
            self.session = None


class BochaAIClient:
    """åšæŸ¥AIæœç´¢å®¢æˆ·ç«¯ï¼ŒåŸºäºå®˜æ–¹APIæ–‡æ¡£å®ç°ï¼Œä¼˜åŒ–ç‰ˆæœ¬."""
    
    def __init__(self):
        # APIç«¯ç‚¹é…ç½®
        self.base_url = "https://api.bochaai.com"
        self.web_search_url = f"{self.base_url}/v1/web-search"
        self.ai_search_url = f"{self.base_url}/v1/ai-search"
        self.agent_search_url = f"{self.base_url}/v1/agent-search"
        self.rerank_url = f"{self.base_url}/v1/rerank"
        
        # æ€§èƒ½é…ç½®
        self.timeout = 30
        self.rate_limit = 10  # æ¯ç§’æœ€å¤š10ä¸ªè¯·æ±‚
        self.last_request_time = 0
        self.session = None
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger(f"{__name__}.BochaAIClient")
        
        # ç›´æ¥ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.api_key = self._get_api_key()
        
        # APIé…ç½®ä¼˜åŒ–
        self.api_config = {
            "web_search": {
                "enabled": True,
                "max_results": 50,
                "timeout": 25,
                "summary": True,
                "freshness": "noLimit",
                "include_images": True,
                "retry_count": 2
            },
            "ai_search": {
                "enabled": True,
                "max_results": 20,
                "timeout": 30,
                "answer": True,
                "stream": False,
                "include_sources": True,
                "retry_count": 2
            },
            "agent_search": {
                "enabled": True,
                "available_agents": {
                    "academic": "bocha-scholar-agent",    # å­¦æœ¯æœç´¢
                    "patent": "bocha-scholar-agent",      # ä¸“åˆ©æœç´¢ä¹Ÿç”¨å­¦æœ¯Agent
                    "company": "bocha-company-agent",     # ä¼ä¸šæœç´¢
                    "document": "bocha-wenku-agent",      # æ–‡åº“æœç´¢
                    "general": "bocha-scholar-agent"      # é€šç”¨æœç´¢
                },
                "timeout": 35,
                "retry_count": 1
            },
            "rerank": {
                "enabled": True,
                "model": "gte-rerank",
                "top_n": 15,
                "timeout": 20,
                "return_documents": False
            }
        }
        
        # æœç´¢è´¨é‡è¯„ä¼°é…ç½®ä¼˜åŒ–
        self.quality_config = {
            "min_content_length": 30,
            "max_content_length": 10000,
            "relevance_threshold": 0.4,
            "freshness_weight": 0.25,
            "authority_weight": 0.35,
            "completeness_weight": 0.15,
            "diversity_threshold": 0.7
        }
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cache_hits": 0,
            "api_errors": {},
            "average_response_time": 0.0
        }
    
    def _get_api_key(self) -> str:
        """ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥ï¼Œæ”¯æŒå¤šç§è·å–æ–¹å¼å’ŒéªŒè¯."""
        import os
        from pathlib import Path
        
        # ä¼˜å…ˆçº§é¡ºåºè·å–APIå¯†é’¥
        api_key_sources = [
            "BOCHA_AI_API_KEY",           # ä¸»è¦ç¯å¢ƒå˜é‡
            "BOCHAAI_API_KEY",            # å¤‡ç”¨ç¯å¢ƒå˜é‡1
            "BOCHA_API_KEY",              # å¤‡ç”¨ç¯å¢ƒå˜é‡2
            "BOCHA_AI_TOKEN",             # å¤‡ç”¨ç¯å¢ƒå˜é‡3
        ]
        
        # 1. ä»ç¯å¢ƒå˜é‡è·å–
        for env_var in api_key_sources:
            api_key = os.getenv(env_var)
            if api_key and api_key.strip():
                api_key = api_key.strip()
                if self._validate_api_key_format(api_key):
                    self.logger.info(f"âœ… APIå¯†é’¥å·²ä»ç¯å¢ƒå˜é‡ {env_var} æˆåŠŸè·å–")
                    return api_key
                else:
                    self.logger.warning(f"âš ï¸ ç¯å¢ƒå˜é‡ {env_var} ä¸­çš„APIå¯†é’¥æ ¼å¼æ— æ•ˆ")
        
        # 2. å°è¯•ä».envæ–‡ä»¶ç›´æ¥è¯»å–ï¼ˆä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
        try:
            env_file_paths = [
                Path(".env"),
                Path("../.env"),
                Path("../../.env")
            ]
            
            for env_path in env_file_paths:
                if env_path.exists():
                    api_key = self._read_api_key_from_file(env_path)
                    if api_key:
                        self.logger.info(f"âœ… APIå¯†é’¥å·²ä»æ–‡ä»¶ {env_path} è·å–")
                        return api_key
                        
        except Exception as e:
            self.logger.debug(f"ä».envæ–‡ä»¶è¯»å–APIå¯†é’¥å¤±è´¥: {str(e)}")
        
        # 3. ä½¿ç”¨é»˜è®¤å¯†é’¥ï¼ˆæœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼‰
        default_key = "skhello"
        self.logger.warning("ğŸ”‘ æœªæ‰¾åˆ°æœ‰æ•ˆçš„åšæŸ¥AI APIå¯†é’¥ï¼Œä½¿ç”¨é»˜è®¤å¯†é’¥")
        self.logger.info("ğŸ’¡ å»ºè®®è®¾ç½®ç¯å¢ƒå˜é‡: BOCHA_AI_API_KEY=your_api_key")
        
        return default_key
    
    def _validate_api_key_format(self, api_key: str) -> bool:
        """éªŒè¯APIå¯†é’¥æ ¼å¼æ˜¯å¦æ­£ç¡®."""
        if not api_key:
            return False
        
        # åšæŸ¥AIå¯†é’¥æ ¼å¼éªŒè¯
        # é€šå¸¸ä»¥ sk- å¼€å¤´ï¼Œé•¿åº¦åœ¨30-50å­—ç¬¦ä¹‹é—´
        if not api_key.startswith("sk-"):
            return False
        
        if len(api_key) < 20 or len(api_key) > 100:
            return False
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆå­—ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ï¼‰
        import re
        if not re.match(r'^sk-[a-zA-Z0-9\-_]+$', api_key):
            return False
        
        return True
    
    def _read_api_key_from_file(self, file_path):
        """ä».envæ–‡ä»¶ä¸­è¯»å–APIå¯†é’¥."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('BOCHA_AI_API_KEY='):
                        api_key = line.split('=', 1)[1].strip()
                        # ç§»é™¤å¯èƒ½çš„å¼•å·
                        api_key = api_key.strip('"\'')
                        if self._validate_api_key_format(api_key):
                            return api_key
            return None
        except Exception as e:
            self.logger.debug(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
            return None
    
    def get_api_key_info(self) -> Dict[str, Any]:
        """è·å–APIå¯†é’¥ç›¸å…³ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’ŒçŠ¶æ€æ£€æŸ¥ï¼‰."""
        import os
        
        info = {
            "api_key_configured": bool(self.api_key),
            "api_key_valid_format": self._validate_api_key_format(self.api_key) if self.api_key else False,
            "api_key_length": len(self.api_key) if self.api_key else 0,
            "api_key_prefix": self.api_key[:10] + "..." if self.api_key and len(self.api_key) > 10 else self.api_key,
            "environment_variables": {},
            "is_default_key": self.api_key == "sk-57e5481fb6954679ad0b58043fb9f963"
        }
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡çŠ¶æ€
        env_vars = ["BOCHA_AI_API_KEY", "BOCHAAI_API_KEY", "BOCHA_API_KEY", "BOCHA_AI_TOKEN"]
        for env_var in env_vars:
            value = os.getenv(env_var)
            info["environment_variables"][env_var] = {
                "exists": bool(value),
                "length": len(value) if value else 0,
                "valid_format": self._validate_api_key_format(value) if value else False
            }
        
        return info
    
    def get_api_status(self) -> Dict[str, Any]:
        """è·å–APIçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…å«è¯¦ç»†çš„å¯†é’¥å’Œé…ç½®ä¿¡æ¯."""
        api_key_info = self.get_api_key_info()
        
        return {
            "api_key_info": api_key_info,
            "api_key_configured": api_key_info["api_key_configured"],
            "api_key_valid": api_key_info["api_key_valid_format"],
            "api_key_source": "environment_variable" if not api_key_info["is_default_key"] else "default",
            "base_url": self.base_url,
            "endpoints": {
                "web_search": self.web_search_url,
                "ai_search": self.ai_search_url,
                "agent_search": self.agent_search_url,
                "rerank": self.rerank_url
            },
            "config": self.api_config,
            "stats": self.stats,
            "health": {
                "rate_limit": f"{self.rate_limit} requests/second",
                "timeout": f"{self.timeout} seconds",
                "session_active": self.session is not None
            }
        }
    
    async def search(self, keywords: List[str], search_type: str = "general", limit: int = 20) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒåšæŸ¥AIæœç´¢ï¼ŒåŸºäºå®˜æ–¹APIï¼Œä¼˜åŒ–ç‰ˆæœ¬."""
        start_time = asyncio.get_event_loop().time()
        
        try:
            # æ›´æ–°ç»Ÿè®¡
            self.stats["total_requests"] += 1
            
            # éªŒè¯APIå¯†é’¥
            if not self._validate_api_key():
                self.logger.error("APIå¯†é’¥æ— æ•ˆï¼Œä½¿ç”¨é™çº§æœç´¢")
                return await self._get_fallback_results(keywords, search_type, limit)
            
            # é€Ÿç‡é™åˆ¶æ£€æŸ¥
            await self._rate_limit_check()
            
            # æ„å»ºæœç´¢æŸ¥è¯¢
            query = self._build_optimized_query(keywords, search_type)
            
            # æ™ºèƒ½æœç´¢ç­–ç•¥é€‰æ‹©
            search_strategy = self._select_search_strategy(search_type, limit)
            
            # æ‰§è¡Œå¹¶è¡Œæœç´¢
            search_results = await self._execute_smart_parallel_search(query, search_strategy)
            
            # ç»“æœå¤„ç†å’Œä¼˜åŒ–
            optimized_results = await self._process_and_optimize_results(
                query, search_results, limit
            )
            
            # æ›´æ–°æˆåŠŸç»Ÿè®¡
            self.stats["successful_requests"] += 1
            duration = asyncio.get_event_loop().time() - start_time
            self._update_response_time(duration)
            
            self.logger.info(
                f"åšæŸ¥AIæœç´¢å®Œæˆ: {len(optimized_results)} ä¸ªç»“æœ, "
                f"æŸ¥è¯¢: '{query}', è€—æ—¶: {duration:.2f}s"
            )
            
            return optimized_results
            
        except Exception as e:
            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            self.stats["failed_requests"] += 1
            error_type = type(e).__name__
            self.stats["api_errors"][error_type] = self.stats["api_errors"].get(error_type, 0) + 1
            
            self.logger.error(f"åšæŸ¥AIæœç´¢å¤±è´¥: {str(e)}")
            
            # è¿”å›é™çº§ç»“æœ
            return await self._get_fallback_results(keywords, search_type, limit)
    
    def _validate_api_key(self) -> bool:
        """éªŒè¯APIå¯†é’¥æ˜¯å¦å¯ç”¨ï¼ˆä½¿ç”¨æ›´ä¸¥æ ¼çš„éªŒè¯ï¼‰."""
        return self._validate_api_key_format(self.api_key)
    
    def _build_optimized_query(self, keywords: List[str], search_type: str) -> str:
        """æ„å»ºä¼˜åŒ–çš„æœç´¢æŸ¥è¯¢."""
        base_query = " ".join(keywords)
        
        # æ ¹æ®æœç´¢ç±»å‹æ·»åŠ ä¼˜åŒ–å…³é”®è¯
        type_enhancements = {
            "patent": " ä¸“åˆ© æŠ€æœ¯ å‘æ˜ ç”³è¯·",
            "academic": " ç ”ç©¶ è®ºæ–‡ å­¦æœ¯ ç§‘ç ”",
            "company": " ä¼ä¸š å…¬å¸ å•†ä¸š",
            "news": " æ–°é—» èµ„è®¯ åŠ¨æ€ æœ€æ–°",
            "document": " æ–‡æ¡£ èµ„æ–™ æŠ¥å‘Š"
        }
        
        enhancement = type_enhancements.get(search_type, "")
        return f"{base_query}{enhancement}".strip()
    
    def _select_search_strategy(self, search_type: str, limit: int) -> Dict[str, Any]:
        """æ™ºèƒ½é€‰æ‹©æœç´¢ç­–ç•¥."""
        strategies = {
            "academic": {
                "use_agent": True,
                "use_ai": True,
                "use_web": False,
                "agent_weight": 0.6,
                "ai_weight": 0.4
            },
            "patent": {
                "use_agent": True,
                "use_ai": True,
                "use_web": True,
                "agent_weight": 0.5,
                "ai_weight": 0.3,
                "web_weight": 0.2
            },
            "company": {
                "use_agent": True,
                "use_ai": False,
                "use_web": True,
                "agent_weight": 0.7,
                "web_weight": 0.3
            },
            "general": {
                "use_agent": False,
                "use_ai": True,
                "use_web": True,
                "ai_weight": 0.6,
                "web_weight": 0.4
            }
        }
        
        strategy = strategies.get(search_type, strategies["general"])
        strategy["limit"] = limit
        strategy["search_type"] = search_type
        
        return strategy
    
    async def _execute_smart_parallel_search(self, query: str, strategy: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
        """æ‰§è¡Œæ™ºèƒ½å¹¶è¡Œæœç´¢."""
        tasks = []
        task_names = []
        
        limit = strategy["limit"]
        
        # æ ¹æ®ç­–ç•¥åˆ›å»ºæœç´¢ä»»åŠ¡
        if strategy.get("use_web", False):
            web_limit = int(limit * strategy.get("web_weight", 0.3))
            tasks.append(self._web_search_with_retry(query, strategy["search_type"], web_limit))
            task_names.append("web")
        
        if strategy.get("use_ai", False):
            ai_limit = int(limit * strategy.get("ai_weight", 0.4))
            tasks.append(self._ai_search_with_retry(query, strategy["search_type"], ai_limit))
            task_names.append("ai")
        
        if strategy.get("use_agent", False):
            agent_limit = int(limit * strategy.get("agent_weight", 0.5))
            tasks.append(self._agent_search_with_retry(query, strategy["search_type"], agent_limit))
            task_names.append("agent")
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        if not tasks:
            return {}
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        search_results = {}
        for i, (task_name, result) in enumerate(zip(task_names, results)):
            if isinstance(result, Exception):
                self.logger.warning(f"{task_name} æœç´¢å¤±è´¥: {str(result)}")
                search_results[task_name] = []
            else:
                search_results[task_name] = result or []
        
        return search_results
    
    async def _process_and_optimize_results(self, query: str, search_results: Dict[str, List[Dict[str, Any]]], limit: int) -> List[Dict[str, Any]]:
        """å¤„ç†å’Œä¼˜åŒ–æœç´¢ç»“æœ."""
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = []
        for source, results in search_results.items():
            for result in results:
                result["search_source"] = source
                all_results.append(result)
        
        if not all_results:
            return []
        
        # å»é‡
        deduplicated_results = await self._simple_deduplicate_results(all_results)
        
        # è¯­ä¹‰é‡æ’åºï¼ˆå¦‚æœç»“æœè¶³å¤Ÿå¤šä¸”å¯ç”¨ï¼‰
        if len(deduplicated_results) > 3 and self.api_config["rerank"]["enabled"]:
            try:
                reranked_results = await self._rerank_results(query, deduplicated_results)
                if reranked_results:
                    deduplicated_results = reranked_results
            except Exception as e:
                self.logger.warning(f"è¯­ä¹‰é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’åº: {str(e)}")
        
        # è´¨é‡è¯„ä¼°å’Œæœ€ç»ˆæ’åº
        final_results = await self._optimize_results(deduplicated_results, [])
        
        # ç®€å•çš„å¤šæ ·æ€§ä¼˜åŒ–ï¼ˆå–å‰Nä¸ªä¸åŒæ¥æºçš„ç»“æœï¼‰
        if len(final_results) > 5:
            final_results = self._simple_diversity_optimization(final_results)
        
        return final_results[:limit]
    
    def _update_response_time(self, duration: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´."""
        if self.stats["successful_requests"] == 1:
            self.stats["average_response_time"] = duration
        else:
            # æŒ‡æ•°ç§»åŠ¨å¹³å‡
            alpha = 0.1
            self.stats["average_response_time"] = (
                (1 - alpha) * self.stats["average_response_time"] + 
                alpha * duration
            )
    
    async def _rate_limit_check(self):
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶ï¼Œä¼˜åŒ–ç‰ˆæœ¬."""
        import time
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        min_interval = 1.0 / self.rate_limit
        if time_since_last < min_interval:
            sleep_time = min_interval - time_since_last
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    async def _web_search_with_retry(self, query: str, search_type: str, limit: int) -> List[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„Webæœç´¢."""
        retry_count = self.api_config["web_search"]["retry_count"]
        
        for attempt in range(retry_count + 1):
            try:
                return await self._web_search(query, search_type, limit)
            except Exception as e:
                if attempt == retry_count:
                    self.logger.error(f"Webæœç´¢é‡è¯•{retry_count}æ¬¡åä»å¤±è´¥: {str(e)}")
                    return await self._get_mock_web_results(query, limit)
                else:
                    self.logger.warning(f"Webæœç´¢ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œé‡è¯•ä¸­: {str(e)}")
                    await asyncio.sleep(1 * (attempt + 1))  # æŒ‡æ•°é€€é¿
        
        return []
    
    async def _ai_search_with_retry(self, query: str, search_type: str, limit: int) -> List[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„AIæœç´¢."""
        retry_count = self.api_config["ai_search"]["retry_count"]
        
        for attempt in range(retry_count + 1):
            try:
                return await self._ai_search(query, search_type, limit)
            except Exception as e:
                if attempt == retry_count:
                    self.logger.error(f"AIæœç´¢é‡è¯•{retry_count}æ¬¡åä»å¤±è´¥: {str(e)}")
                    return await self._get_mock_ai_results(query, limit)
                else:
                    self.logger.warning(f"AIæœç´¢ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œé‡è¯•ä¸­: {str(e)}")
                    await asyncio.sleep(2 * (attempt + 1))  # æŒ‡æ•°é€€é¿
        
        return []
    
    async def _agent_search_with_retry(self, query: str, search_type: str, limit: int) -> List[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„Agentæœç´¢."""
        retry_count = self.api_config["agent_search"]["retry_count"]
        
        for attempt in range(retry_count + 1):
            try:
                return await self._agent_search(query, search_type, limit)
            except Exception as e:
                if attempt == retry_count:
                    self.logger.warning(f"Agentæœç´¢é‡è¯•{retry_count}æ¬¡åä»å¤±è´¥: {str(e)}")
                    return []  # Agentæœç´¢å¤±è´¥ä¸æä¾›é™çº§ç»“æœ
                else:
                    self.logger.warning(f"Agentæœç´¢ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œé‡è¯•ä¸­: {str(e)}")
                    await asyncio.sleep(1.5 * (attempt + 1))  # æŒ‡æ•°é€€é¿
        
        return []
    
    async def _web_search(self, query: str, search_type: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒWebæœç´¢ï¼ŒåŸºäºåšæŸ¥AI Web Search API."""
        try:
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                )
            
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            params = {
                "query": query,
                "freshness": self.api_config["web_search"]["freshness"],
                "summary": self.api_config["web_search"]["summary"],
                "count": min(limit, self.api_config["web_search"]["max_results"])
            }
            
            # æ ¹æ®æœç´¢ç±»å‹è°ƒæ•´å‚æ•°
            if search_type == "patent":
                params["query"] += " ä¸“åˆ© æŠ€æœ¯ å‘æ˜"
            elif search_type == "academic":
                params["query"] += " ç ”ç©¶ è®ºæ–‡ å­¦æœ¯"
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            async with self.session.post(
                self.web_search_url, 
                json=params, 
                headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_web_search_response(data)
                elif response.status == 401:
                    logger.error("Bocha AI authentication failed - check API key")
                    return await self._get_mock_web_results(query, limit)
                elif response.status == 429:
                    logger.warning("Bocha AI rate limit exceeded")
                    await asyncio.sleep(2)
                    return await self._get_mock_web_results(query, limit)
                else:
                    logger.error(f"Bocha AI web search returned status {response.status}")
                    return await self._get_mock_web_results(query, limit)
                    
        except Exception as e:
            logger.error(f"Web search failed: {str(e)}")
            return await self._get_mock_web_results(query, limit)
    
    async def _real_web_search_api(self, query: Dict[str, Any], limit: int) -> Optional[List[Dict[str, Any]]]:
        """å°è¯•çœŸå®çš„åšæŸ¥AI Webæœç´¢APIè°ƒç”¨."""
        try:
            # å‡†å¤‡APIè¯·æ±‚å‚æ•°
            api_params = {
                "query": query["query"],
                "limit": limit,
                "content_types": query.get("content_types", ["news", "article"]),
                "regions": query.get("regions", ["global"]),
                "language": "zh-CN",
                "freshness": "month"
            }
            
            # æ·»åŠ è®¤è¯ä¿¡æ¯
            headers = {"Content-Type": "application/json"}
            api_key = self._get_bocha_api_key()
            if api_key:
                headers["Authorization"] = f"Bearer {api_key}"
                headers["X-API-Key"] = api_key
            
            async with self.session.post(self.web_search_url, json=api_params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_web_search_response(data)
                elif response.status == 401:
                    logger.error("Bocha AI authentication failed")
                    return None
                elif response.status == 429:
                    logger.warning("Bocha AI rate limit exceeded, waiting...")
                    await asyncio.sleep(2)
                    return None
                else:
                    logger.error(f"Bocha AI web search returned status {response.status}")
                    return None
                    
        except Exception as e:
            logger.debug(f"Real Bocha AI web search failed: {str(e)}")
            return None
    
    def _get_bocha_api_key(self) -> Optional[str]:
        """è·å–åšæŸ¥AI APIå¯†é’¥."""
        import os
        return os.getenv("BOCHA_AI_API_KEY")
    
    def _parse_web_search_response(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æWebæœç´¢APIå“åº”ï¼ŒåŸºäºåšæŸ¥AI APIæ ¼å¼."""
        results = []
        
        # æ ¹æ®åšæŸ¥AI APIæ–‡æ¡£è§£æå“åº”
        if data.get("code") == 200:
            search_data = data.get("data", {})
            web_pages = search_data.get("webPages", {})
            web_results = web_pages.get("value", [])
            
            for item in web_results:
                result = {
                    "title": item.get("name", ""),
                    "url": item.get("url", ""),
                    "content": item.get("snippet", ""),
                    "summary": item.get("summary", ""),
                    "publish_date": item.get("datePublished", "") or item.get("dateLastCrawled", ""),
                    "source_domain": item.get("siteName", ""),
                    "site_icon": item.get("siteIcon", ""),
                    "content_type": "webpage",
                    "language": item.get("language", "zh-CN"),
                    "relevance_score": 0.7,  # é»˜è®¤ç›¸å…³æ€§
                    "authority_score": 0.6,
                    "freshness_score": self._calculate_freshness_score(item.get("datePublished", "")),
                    "source": "bocha_web_search"
                }
                results.append(result)
            
            # å¤„ç†å›¾ç‰‡ç»“æœ
            images = search_data.get("images", {})
            if images and images.get("value"):
                for img in images.get("value", [])[:3]:  # æœ€å¤š3å¼ å›¾ç‰‡
                    result = {
                        "title": f"å›¾ç‰‡: {img.get('name', 'ç›¸å…³å›¾ç‰‡')}",
                        "url": img.get("hostPageUrl", ""),
                        "content": f"å›¾ç‰‡å†…å®¹: {img.get('name', '')}",
                        "summary": "",
                        "image_url": img.get("contentUrl", ""),
                        "thumbnail_url": img.get("thumbnailUrl", ""),
                        "content_type": "image",
                        "relevance_score": 0.5,
                        "authority_score": 0.4,
                        "freshness_score": 0.5,
                        "source": "bocha_image_search"
                    }
                    results.append(result)
        
        return results
    
    async def _ai_search(self, query: str, search_type: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒAIæ™ºèƒ½æœç´¢ï¼ŒåŸºäºåšæŸ¥AI AI Search API."""
        try:
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                )
            
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            params = {
                "query": query,
                "freshness": "noLimit",
                "count": min(limit, self.api_config["ai_search"]["max_results"]),
                "answer": self.api_config["ai_search"]["answer"],
                "stream": self.api_config["ai_search"]["stream"]
            }
            
            # æ ¹æ®æœç´¢ç±»å‹è°ƒæ•´æŸ¥è¯¢
            if search_type == "patent":
                params["query"] += " ä¸“åˆ©æŠ€æœ¯åˆ†æ"
            elif search_type == "academic":
                params["query"] += " å­¦æœ¯ç ”ç©¶åˆ†æ"
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            async with self.session.post(
                self.ai_search_url, 
                json=params, 
                headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_ai_search_response(data)
                elif response.status == 401:
                    logger.error("Bocha AI authentication failed - check API key")
                    return await self._get_mock_ai_results(query, limit)
                elif response.status == 429:
                    logger.warning("Bocha AI rate limit exceeded")
                    await asyncio.sleep(3)
                    return await self._get_mock_ai_results(query, limit)
                else:
                    logger.error(f"Bocha AI AI search returned status {response.status}")
                    return await self._get_mock_ai_results(query, limit)
                    
        except Exception as e:
            logger.error(f"AI search failed: {str(e)}")
            return await self._get_mock_ai_results(query, limit)
    
    async def _real_ai_search_api(self, query: Dict[str, Any], limit: int) -> Optional[List[Dict[str, Any]]]:
        """å°è¯•çœŸå®çš„åšæŸ¥AIæ™ºèƒ½æœç´¢APIè°ƒç”¨."""
        try:
            # å‡†å¤‡APIè¯·æ±‚å‚æ•°
            api_params = {
                "prompt": query["prompt"],
                "keywords": query["keywords"],
                "analysis_depth": query.get("analysis_depth", "medium"),
                "include_reasoning": query.get("include_reasoning", True),
                "max_results": limit,
                "language": "zh-CN",
                "format": "structured"
            }
            
            # æ·»åŠ è®¤è¯ä¿¡æ¯
            headers = {"Content-Type": "application/json"}
            api_key = self._get_bocha_api_key()
            if api_key:
                headers["Authorization"] = f"Bearer {api_key}"
                headers["X-API-Key"] = api_key
            
            async with self.session.post(self.ai_search_url, json=api_params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_ai_search_response(data)
                elif response.status == 401:
                    logger.error("Bocha AI authentication failed")
                    return None
                elif response.status == 429:
                    logger.warning("Bocha AI rate limit exceeded, waiting...")
                    await asyncio.sleep(3)
                    return None
                else:
                    logger.error(f"Bocha AI AI search returned status {response.status}")
                    return None
                    
        except Exception as e:
            logger.debug(f"Real Bocha AI AI search failed: {str(e)}")
            return None
    
    def _parse_ai_search_response(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æAIæœç´¢APIå“åº”ï¼ŒåŸºäºåšæŸ¥AI APIæ ¼å¼."""
        results = []
        
        # æ ¹æ®åšæŸ¥AI APIæ–‡æ¡£è§£æå“åº”
        if data.get("code") == 200:
            messages = data.get("messages", [])
            
            for message in messages:
                if message.get("role") == "assistant":
                    msg_type = message.get("type", "")
                    content_type = message.get("content_type", "")
                    content = message.get("content", "")
                    
                    if msg_type == "source":
                        # å¤„ç†å‚è€ƒæº
                        if content_type == "webpage":
                            try:
                                import json
                                webpage_data = json.loads(content) if isinstance(content, str) else content
                                if isinstance(webpage_data, dict):
                                    result = {
                                        "title": webpage_data.get("name", ""),
                                        "url": webpage_data.get("url", ""),
                                        "content": webpage_data.get("snippet", ""),
                                        "summary": webpage_data.get("summary", ""),
                                        "source_domain": webpage_data.get("siteName", ""),
                                        "content_type": "ai_webpage",
                                        "relevance_score": 0.8,
                                        "authority_score": 0.7,
                                        "freshness_score": 0.6,
                                        "source": "bocha_ai_search"
                                    }
                                    results.append(result)
                            except:
                                pass
                        
                        elif content_type in ["baike_pro", "medical_common", "weather_china"]:
                            # å¤„ç†æ¨¡æ€å¡
                            try:
                                import json
                                modal_data = json.loads(content) if isinstance(content, str) else content
                                if isinstance(modal_data, list) and modal_data:
                                    modal_item = modal_data[0]
                                    result = {
                                        "title": modal_item.get("name", f"{content_type}ä¿¡æ¯"),
                                        "url": modal_item.get("url", ""),
                                        "content": str(modal_item.get("modelCard", {}))[:500],
                                        "summary": f"æ¥è‡ª{content_type}çš„ä¸“ä¸šä¿¡æ¯",
                                        "content_type": f"modal_card_{content_type}",
                                        "relevance_score": 0.9,
                                        "authority_score": 0.8,
                                        "freshness_score": 0.7,
                                        "source": "bocha_modal_card"
                                    }
                                    results.append(result)
                            except:
                                pass
                    
                    elif msg_type == "answer" and content_type == "text":
                        # å¤„ç†AIç”Ÿæˆçš„ç­”æ¡ˆ
                        result = {
                            "title": "AIæ™ºèƒ½åˆ†æ",
                            "content": content,
                            "summary": content[:200] + "..." if len(content) > 200 else content,
                            "content_type": "ai_answer",
                            "relevance_score": 0.95,
                            "authority_score": 0.6,
                            "freshness_score": 1.0,
                            "source": "bocha_ai_answer"
                        }
                        results.append(result)
                    
                    elif msg_type == "follow_up":
                        # å¤„ç†è¿½é—®é—®é¢˜
                        result = {
                            "title": "ç›¸å…³é—®é¢˜",
                            "content": f"æ‚¨å¯èƒ½è¿˜æƒ³äº†è§£: {content}",
                            "summary": content,
                            "content_type": "follow_up",
                            "relevance_score": 0.6,
                            "authority_score": 0.4,
                            "freshness_score": 1.0,
                            "source": "bocha_follow_up"
                        }
                        results.append(result)
        
        return results
    
    async def _agent_search(self, query: str, search_type: str, limit: int) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒAgentæœç´¢ï¼ŒåŸºäºåšæŸ¥AI Agent Search API."""
        try:
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                )
            
            # æ ¹æ®æœç´¢ç±»å‹é€‰æ‹©åˆé€‚çš„Agent
            agent_id = self._select_agent_by_type(search_type)
            if not agent_id:
                return []
            
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            params = {
                "agentId": agent_id,
                "query": query,
                "searchType": "neural",  # ä½¿ç”¨è‡ªç„¶è¯­è¨€æœç´¢
                "answer": False,  # æš‚æ—¶ä¸è¿”å›AIç­”æ¡ˆ
                "stream": False
            }
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            async with self.session.post(
                self.agent_search_url, 
                json=params, 
                headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_agent_search_response(data, agent_id)
                elif response.status == 401:
                    logger.error("Bocha AI authentication failed for agent search")
                    return []
                elif response.status == 429:
                    logger.warning("Bocha AI rate limit exceeded for agent search")
                    await asyncio.sleep(2)
                    return []
                else:
                    logger.error(f"Bocha AI agent search returned status {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"Agent search failed: {str(e)}")
            return []
    
    def _select_agent_by_type(self, search_type: str) -> Optional[str]:
        """æ ¹æ®æœç´¢ç±»å‹é€‰æ‹©åˆé€‚çš„Agentï¼Œä¼˜åŒ–ç‰ˆæœ¬."""
        available_agents = self.api_config["agent_search"]["available_agents"]
        
        # ç›´æ¥ä»é…ç½®ä¸­è·å–Agentæ˜ å°„
        selected_agent = available_agents.get(search_type)
        
        if selected_agent:
            self.logger.debug(f"ä¸ºæœç´¢ç±»å‹ '{search_type}' é€‰æ‹©Agent: {selected_agent}")
            return selected_agent
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤Agent
        default_agent = available_agents.get("general", "bocha-scholar-agent")
        self.logger.debug(f"æœç´¢ç±»å‹ '{search_type}' æœªæ‰¾åˆ°ä¸“ç”¨Agentï¼Œä½¿ç”¨é»˜è®¤: {default_agent}")
        
        return default_agent
    
    def _parse_agent_search_response(self, data: Dict[str, Any], agent_id: str) -> List[Dict[str, Any]]:
        """è§£æAgentæœç´¢APIå“åº”."""
        results = []
        
        if data.get("code") == 200:
            messages = data.get("messages", [])
            
            for message in messages:
                if message.get("role") == "assistant" and message.get("type") == "source":
                    content_type = message.get("content_type", "")
                    content = message.get("content", "")
                    
                    try:
                        import json
                        if isinstance(content, str):
                            content_data = json.loads(content)
                        else:
                            content_data = content
                        
                        if isinstance(content_data, list):
                            for item in content_data:
                                result = self._parse_agent_item(item, content_type, agent_id)
                                if result:
                                    results.append(result)
                        elif isinstance(content_data, dict):
                            result = self._parse_agent_item(content_data, content_type, agent_id)
                            if result:
                                results.append(result)
                                
                    except Exception as e:
                        logger.debug(f"Failed to parse agent content: {str(e)}")
        
        return results
    
    def _parse_agent_item(self, item: Dict[str, Any], content_type: str, agent_id: str) -> Optional[Dict[str, Any]]:
        """è§£æAgentæœç´¢ç»“æœé¡¹."""
        try:
            if content_type == "restaurant" or content_type == "hotel" or content_type == "attraction":
                # åœ°ç†ä½ç½®ç›¸å…³ç»“æœï¼Œå¯¹ä¸“åˆ©æœç´¢ä¸å¤ªç›¸å…³
                return None
            
            # é€šç”¨è§£æ
            result = {
                "title": item.get("name", "") or item.get("title", ""),
                "content": item.get("metadata", {}).get("tag", "") or str(item.get("metadata", {}))[:300],
                "url": item.get("url", ""),
                "summary": f"æ¥è‡ª{agent_id}çš„ä¸“ä¸šæœç´¢ç»“æœ",
                "content_type": f"agent_{content_type}",
                "relevance_score": 0.85,
                "authority_score": 0.9,  # Agentæœç´¢æƒå¨æ€§è¾ƒé«˜
                "freshness_score": 0.7,
                "source": f"bocha_agent_{agent_id}",
                "agent_metadata": item.get("metadata", {})
            }
            
            return result
            
        except Exception as e:
            logger.debug(f"Failed to parse agent item: {str(e)}")
            return None
    
    async def _rerank_results(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨åšæŸ¥AIè¯­ä¹‰é‡æ’åºAPIä¼˜åŒ–æœç´¢ç»“æœ."""
        try:
            if not results or len(results) <= 1:
                return results
            
            if not self.session:
                self.session = aiohttp.ClientSession(
                    timeout=aiohttp.ClientTimeout(total=self.timeout)
                )
            
            # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
            documents = []
            for result in results:
                doc_text = f"{result.get('title', '')} {result.get('content', '')} {result.get('summary', '')}"
                documents.append(doc_text.strip())
            
            # æ„å»ºé‡æ’åºè¯·æ±‚
            params = {
                "model": self.api_config["rerank"]["model"],
                "query": query,
                "documents": documents,
                "top_n": min(len(documents), self.api_config["rerank"]["top_n"]),
                "return_documents": False  # æˆ‘ä»¬å·²ç»æœ‰åŸå§‹æ–‡æ¡£
            }
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            async with self.session.post(
                self.rerank_url, 
                json=params, 
                headers=headers
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._apply_rerank_results(results, data)
                else:
                    logger.warning(f"Rerank API failed with status {response.status}, using original order")
                    return results
                    
        except Exception as e:
            logger.error(f"Rerank failed: {str(e)}")
            return results
    
    def _apply_rerank_results(self, original_results: List[Dict[str, Any]], rerank_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åº”ç”¨é‡æ’åºç»“æœ."""
        try:
            if rerank_data.get("code") != 200:
                return original_results
            
            rerank_results = rerank_data.get("data", {}).get("results", [])
            if not rerank_results:
                return original_results
            
            # æŒ‰é‡æ’åºç»“æœé‡æ–°æ’åˆ—
            reordered_results = []
            for rerank_item in rerank_results:
                index = rerank_item.get("index", -1)
                relevance_score = rerank_item.get("relevance_score", 0.5)
                
                if 0 <= index < len(original_results):
                    result = original_results[index].copy()
                    result["rerank_score"] = relevance_score
                    result["relevance_score"] = max(result.get("relevance_score", 0.5), relevance_score)
                    reordered_results.append(result)
            
            # æ·»åŠ æœªè¢«é‡æ’åºçš„ç»“æœ
            reranked_indices = {item.get("index", -1) for item in rerank_results}
            for i, result in enumerate(original_results):
                if i not in reranked_indices:
                    result_copy = result.copy()
                    result_copy["rerank_score"] = 0.3  # è¾ƒä½çš„é‡æ’åºåˆ†æ•°
                    reordered_results.append(result_copy)
            
            return reordered_results
            
        except Exception as e:
            logger.error(f"Failed to apply rerank results: {str(e)}")
            return original_results
    
    def _calculate_freshness_score(self, date_str: str) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°."""
        try:
            if not date_str:
                return 0.5
            
            from datetime import datetime
            current_year = datetime.now().year
            
            if "2024" in date_str or str(current_year) in date_str:
                return 0.9
            elif "2023" in date_str:
                return 0.7
            elif "2022" in date_str:
                return 0.5
            else:
                return 0.3
                
        except Exception:
            return 0.5
    
    def _build_ai_query(self, keywords: List[str], search_type: str) -> Dict[str, Any]:
        """æ„å»ºAIæœç´¢æŸ¥è¯¢."""
        # æ„å»ºæ›´æ™ºèƒ½çš„æŸ¥è¯¢æç¤º
        if search_type == "patent":
            prompt = f"åˆ†æå…³äº{' '.join(keywords)}çš„ä¸“åˆ©æŠ€æœ¯å‘å±•ç°çŠ¶ã€è¶‹åŠ¿å’Œç«äº‰æ ¼å±€"
        elif search_type == "academic":
            prompt = f"æ€»ç»“{' '.join(keywords)}é¢†åŸŸçš„æœ€æ–°å­¦æœ¯ç ”ç©¶è¿›å±•å’Œç†è®ºå‘å±•"
        elif search_type == "news":
            prompt = f"åˆ†æ{' '.join(keywords)}ç›¸å…³çš„æœ€æ–°è¡Œä¸šåŠ¨æ€å’Œå¸‚åœºè¶‹åŠ¿"
        else:
            prompt = f"å…¨é¢åˆ†æ{' '.join(keywords)}çš„å‘å±•ç°çŠ¶ã€æŠ€æœ¯ç‰¹ç‚¹å’Œåº”ç”¨å‰æ™¯"
        
        return {
            "prompt": prompt,
            "keywords": keywords,
            "analysis_depth": self.api_config["ai_search"]["analysis_depth"],
            "include_reasoning": self.api_config["ai_search"]["include_reasoning"],
            "max_results": self.api_config["ai_search"]["max_results"]
        }
    
    async def _enhanced_mock_web_search_api(self, query: Dict[str, Any], limit: int) -> List[Dict[str, Any]]:
        """æ¨¡æ‹ŸWebæœç´¢APIè°ƒç”¨."""
        # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        await asyncio.sleep(0.3)
        
        results = []
        result_count = min(limit, 12)
        
        for i in range(result_count):
            result = {
                "title": f"Webæœç´¢ï¼š{query['query']} - ç›¸å…³èµ„è®¯ {i+1}",
                "url": f"https://example.com/news/{i+1}",
                "content": f"è¿™æ˜¯å…³äº{query['query']}çš„æœ€æ–°ç½‘ç»œèµ„è®¯å†…å®¹ï¼ŒåŒ…å«äº†è¯¦ç»†çš„åˆ†æå’Œè§‚ç‚¹...",
                "summary": f"å…³äº{query['query']}çš„ç½‘ç»œèµ„è®¯æ‘˜è¦ï¼Œæ¶µç›–äº†ä¸»è¦è§‚ç‚¹å’Œæ•°æ®...",
                "publish_date": f"2024-{(i%12)+1:02d}-{(i%28)+1:02d}",
                "source_domain": f"tech-news-{i%5+1}.com",
                "content_type": query["content_types"][i % len(query["content_types"])],
                "region": query["regions"][i % len(query["regions"])],
                "relevance_score": 0.85 - (i * 0.03),
                "authority_score": 0.7 + (i % 3) * 0.1,
                "freshness_score": 0.9 - (i * 0.05)
            }
            results.append(result)
        
        return results
    
    async def _enhanced_mock_ai_search_api(self, query: Dict[str, Any], limit: int) -> List[Dict[str, Any]]:
        """æ¨¡æ‹ŸAIæœç´¢APIè°ƒç”¨."""
        # æ¨¡æ‹ŸAIå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.8)
        
        results = []
        result_count = min(limit, 8)
        
        for i in range(result_count):
            result = {
                "title": f"AIåˆ†æï¼š{' '.join(query['keywords'])} - æ™ºèƒ½æ´å¯Ÿ {i+1}",
                "content": f"åŸºäºAIåˆ†æçš„{' '.join(query['keywords'])}æ·±åº¦æ´å¯ŸæŠ¥å‘Šï¼ŒåŒ…å«æŠ€æœ¯å‘å±•è¶‹åŠ¿ã€å¸‚åœºæœºä¼šå’Œé£é™©è¯„ä¼°...",
                "analysis_type": "ai_generated",
                "confidence": 0.88 - (i * 0.02),
                "reasoning": f"AIæ¨ç†è¿‡ç¨‹ï¼šé€šè¿‡åˆ†æå¤§é‡ç›¸å…³æ•°æ®ï¼Œå‘ç°{' '.join(query['keywords'])}åœ¨ä»¥ä¸‹æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰...",
                "key_insights": [
                    f"{query['keywords'][0] if query['keywords'] else 'æŠ€æœ¯'}å‘å±•è¶‹åŠ¿å‘å¥½",
                    f"å¸‚åœºéœ€æ±‚æŒç»­å¢é•¿",
                    f"æŠ€æœ¯åˆ›æ–°æ´»è·ƒåº¦é«˜"
                ],
                "data_sources": ["å­¦æœ¯è®ºæ–‡", "ä¸“åˆ©æ•°æ®", "å¸‚åœºæŠ¥å‘Š", "æ–°é—»èµ„è®¯"],
                "analysis_depth": query["analysis_depth"],
                "generated_at": datetime.now().isoformat(),
                "relevance_score": 0.9 - (i * 0.02),
                "quality_score": 0.85 - (i * 0.01)
            }
            results.append(result)
        
        return results
    
    def _process_web_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†Webæœç´¢ç»“æœ."""
        processed_results = []
        
        for result in results:
            # è´¨é‡è¿‡æ»¤
            if not self._meets_quality_standards(result):
                continue
            
            processed_result = {
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "content": result.get("content", ""),
                "summary": result.get("summary", ""),
                "source": "Bocha AI Web Search",
                "search_type": "web",
                "relevance_score": result.get("relevance_score", 0.5),
                "authority_score": result.get("authority_score", 0.5),
                "freshness_score": result.get("freshness_score", 0.5),
                "publication_date": result.get("publish_date", ""),
                "source_domain": result.get("source_domain", ""),
                "content_type": result.get("content_type", ""),
                "raw_data": result
            }
            
            processed_results.append(processed_result)
        
        return processed_results
    
    def _process_ai_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†AIæœç´¢ç»“æœ."""
        processed_results = []
        
        for result in results:
            processed_result = {
                "title": result.get("title", ""),
                "url": f"https://bocha.ai/analysis/{hash(result.get('title', ''))}", # ç”Ÿæˆåˆ†æé“¾æ¥
                "content": result.get("content", ""),
                "source": "Bocha AI Analysis",
                "search_type": "ai_analysis",
                "relevance_score": result.get("relevance_score", 0.5),
                "confidence": result.get("confidence", 0.5),
                "quality_score": result.get("quality_score", 0.5),
                "analysis_type": result.get("analysis_type", ""),
                "key_insights": result.get("key_insights", []),
                "reasoning": result.get("reasoning", ""),
                "data_sources": result.get("data_sources", []),
                "generated_at": result.get("generated_at", ""),
                "raw_data": result
            }
            
            processed_results.append(processed_result)
        
        return processed_results
    
    def _meets_quality_standards(self, result: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦ç¬¦åˆè´¨é‡æ ‡å‡†."""
        # åŸºç¡€å†…å®¹æ£€æŸ¥
        content = result.get("content", "")
        title = result.get("title", "")
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(content) < self.quality_config["min_content_length"]:
            return False
        
        if len(content) > self.quality_config["max_content_length"]:
            return False
        
        # æ£€æŸ¥æ ‡é¢˜è´¨é‡
        if len(title) < 5 or len(title) > 200:
            return False
        
        # æ£€æŸ¥ç›¸å…³æ€§åˆ†æ•°
        relevance = result.get("relevance_score", 0)
        if relevance < self.quality_config["relevance_threshold"]:
            return False
        
        # é«˜çº§è´¨é‡æ£€æŸ¥
        if not self._advanced_quality_check(result):
            return False
        
        return True
    
    def _advanced_quality_check(self, result: Dict[str, Any]) -> bool:
        """é«˜çº§è´¨é‡æ£€æŸ¥."""
        content = result.get("content", "").lower()
        title = result.get("title", "").lower()
        
        # æ£€æŸ¥åƒåœ¾å†…å®¹æ¨¡å¼
        spam_patterns = [
            r"ç‚¹å‡».*?äº†è§£æ›´å¤š",
            r"ç«‹å³.*?è´­ä¹°",
            r"å…è´¹.*?ä¸‹è½½",
            r"å¹¿å‘Š.*?æ¨å¹¿",
            r"è”ç³».*?å®¢æœ"
        ]
        
        for pattern in spam_patterns:
            if re.search(pattern, content) or re.search(pattern, title):
                return False
        
        # æ£€æŸ¥å†…å®¹è´¨é‡æŒ‡æ ‡
        # 1. å¥å­å®Œæ•´æ€§
        sentence_count = len([s for s in content.split('ã€‚') if len(s.strip()) > 10])
        if sentence_count < 2:
            return False
        
        # 2. ä¸“ä¸šæœ¯è¯­å¯†åº¦
        tech_terms = ["æŠ€æœ¯", "æ–¹æ³•", "ç³»ç»Ÿ", "ç®—æ³•", "æ¨¡å‹", "åˆ†æ", "ç ”ç©¶", "å‘æ˜", "ä¸“åˆ©", "åˆ›æ–°"]
        term_count = sum(1 for term in tech_terms if term in content)
        if term_count < 1:  # è‡³å°‘åŒ…å«ä¸€ä¸ªä¸“ä¸šæœ¯è¯­
            return False
        
        # 3. å†…å®¹å¤šæ ·æ€§ï¼ˆé¿å…é‡å¤å†…å®¹ï¼‰
        words = content.split()
        if len(set(words)) / max(len(words), 1) < 0.3:  # è¯æ±‡å¤šæ ·æ€§ä½äº30%
            return False
        
        return True
    
    async def _optimize_results(self, results: List[Dict[str, Any]], keywords: List[str]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–æœç´¢ç»“æœï¼ŒåŒ…å«é«˜çº§è¿‡æ»¤å’Œæ’åºç®—æ³•."""
        if not results:
            return []
        
        # ç¬¬ä¸€é˜¶æ®µï¼šè´¨é‡è¿‡æ»¤
        filtered_results = []
        for result in results:
            if self._meets_quality_standards(result):
                # è®¡ç®—å¢å¼ºçš„è´¨é‡åˆ†æ•°
                result["comprehensive_quality"] = self._calculate_comprehensive_quality(result, keywords)
                result["semantic_relevance"] = self._calculate_semantic_relevance(result, keywords)
                result["authority_score"] = self._calculate_authority_score(result)
                result["freshness_score"] = self._calculate_freshness_score(result)
                filtered_results.append(result)
        
        # ç¬¬äºŒé˜¶æ®µï¼šå»é‡å¤„ç†
        deduplicated_results = await self._advanced_deduplication(filtered_results)
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šå¤šç»´åº¦æ’åº
        ranked_results = await self._multi_dimensional_ranking(deduplicated_results, keywords)
        
        # ç¬¬å››é˜¶æ®µï¼šå¤šæ ·æ€§ä¼˜åŒ–
        diversified_results = await self._diversity_optimization(ranked_results)
        
        return diversified_results
    
    def _calculate_semantic_relevance(self, result: Dict[str, Any], keywords: List[str]) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§."""
        content = (result.get("content", "") + " " + result.get("title", "")).lower()
        
        if not keywords:
            return 0.5
        
        # ç›´æ¥å…³é”®è¯åŒ¹é…
        direct_matches = sum(1 for kw in keywords if kw.lower() in content)
        direct_score = direct_matches / len(keywords)
        
        # è¯­ä¹‰ç›¸å…³è¯åŒ¹é…
        semantic_keywords = self._expand_keywords_semantically(keywords)
        semantic_matches = sum(1 for kw in semantic_keywords if kw.lower() in content)
        semantic_score = semantic_matches / max(len(semantic_keywords), 1)
        
        # ä½ç½®æƒé‡ï¼ˆæ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜ï¼‰
        title = result.get("title", "").lower()
        title_matches = sum(1 for kw in keywords if kw.lower() in title)
        title_score = title_matches / len(keywords)
        
        # ç»¼åˆç›¸å…³æ€§åˆ†æ•°
        relevance = (
            direct_score * 0.5 +
            semantic_score * 0.3 +
            title_score * 0.2
        )
        
        return min(relevance, 1.0)
    
    def _expand_keywords_semantically(self, keywords: List[str]) -> List[str]:
        """è¯­ä¹‰æ‰©å±•å…³é”®è¯."""
        expanded = []
        
        # ç®€å•çš„è¯­ä¹‰æ‰©å±•æ˜ å°„
        semantic_map = {
            "äººå·¥æ™ºèƒ½": ["AI", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "æ™ºèƒ½ç®—æ³•"],
            "åŒºå—é“¾": ["blockchain", "åˆ†å¸ƒå¼è´¦æœ¬", "åŠ å¯†è´§å¸", "æ™ºèƒ½åˆçº¦"],
            "ç‰©è”ç½‘": ["IoT", "ä¼ æ„Ÿå™¨", "æ™ºèƒ½è®¾å¤‡", "è¿æ¥æŠ€æœ¯"],
            "5G": ["ç¬¬äº”ä»£ç§»åŠ¨é€šä¿¡", "æ— çº¿é€šä¿¡", "ç§»åŠ¨ç½‘ç»œ"],
            "æ–°èƒ½æº": ["æ¸…æ´èƒ½æº", "å¯å†ç”Ÿèƒ½æº", "å¤ªé˜³èƒ½", "é£èƒ½", "ç”µæ± "],
            "ç”Ÿç‰©æŠ€æœ¯": ["åŸºå› ", "è›‹ç™½è´¨", "ç»†èƒ", "åˆ†å­ç”Ÿç‰©å­¦"],
            "èŠ¯ç‰‡": ["åŠå¯¼ä½“", "é›†æˆç”µè·¯", "å¤„ç†å™¨", "å¾®å¤„ç†å™¨"]
        }
        
        for keyword in keywords:
            expanded.append(keyword)
            if keyword in semantic_map:
                expanded.extend(semantic_map[keyword])
        
        return list(set(expanded))  # å»é‡
    
    def _calculate_authority_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—æƒå¨æ€§åˆ†æ•°."""
        # åŸºäºæ¥æºåŸŸåçš„æƒå¨æ€§
        source_domain = result.get("source_domain", "").lower()
        
        authority_domains = {
            # å­¦æœ¯æœºæ„
            "edu.cn": 0.9, "ac.cn": 0.9, "edu": 0.8,
            # æ”¿åºœæœºæ„
            "gov.cn": 0.95, "gov": 0.9,
            # çŸ¥ååª’ä½“
            "xinhua": 0.8, "people": 0.8, "cctv": 0.8,
            # ä¸“ä¸šç½‘ç«™
            "ieee": 0.9, "acm": 0.9, "nature": 0.95, "science": 0.95,
            # ä¸“åˆ©ç½‘ç«™
            "patents.google": 0.85, "wipo": 0.9, "cnipa": 0.9
        }
        
        base_authority = 0.5
        for domain_key, score in authority_domains.items():
            if domain_key in source_domain:
                base_authority = score
                break
        
        # è€ƒè™‘å…¶ä»–æƒå¨æ€§æŒ‡æ ‡
        if result.get("citation_count", 0) > 10:
            base_authority += 0.1
        
        if result.get("peer_reviewed", False):
            base_authority += 0.1
        
        return min(base_authority, 1.0)
    
    def _calculate_freshness_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—æ—¶æ•ˆæ€§åˆ†æ•°."""
        try:
            pub_date_str = result.get("publish_date", "") or result.get("generated_at", "")
            if not pub_date_str:
                return 0.5
            
            from datetime import datetime, timedelta
            
            # å°è¯•è§£ææ—¥æœŸ
            try:
                if "2024" in pub_date_str:
                    pub_year = 2024
                elif "2023" in pub_date_str:
                    pub_year = 2023
                elif "2022" in pub_date_str:
                    pub_year = 2022
                else:
                    return 0.3
                
                current_year = datetime.now().year
                year_diff = current_year - pub_year
                
                # æ—¶æ•ˆæ€§è¯„åˆ†
                if year_diff == 0:
                    return 1.0
                elif year_diff == 1:
                    return 0.8
                elif year_diff == 2:
                    return 0.6
                else:
                    return max(0.3, 1.0 - (year_diff * 0.2))
                    
            except Exception:
                return 0.5
                
        except Exception:
            return 0.5
    
    async def _advanced_deduplication(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é«˜çº§å»é‡ç®—æ³•."""
        if len(results) <= 1:
            return results
        
        deduplicated = []
        processed_signatures = set()
        
        for result in results:
            # ç”Ÿæˆå†…å®¹æŒ‡çº¹
            signature = self._generate_content_fingerprint(result)
            
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            is_duplicate = False
            for existing_sig in processed_signatures:
                if self._calculate_similarity(signature, existing_sig) > 0.85:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                processed_signatures.add(signature)
                deduplicated.append(result)
            else:
                # å¦‚æœæ˜¯é‡å¤ä½†è´¨é‡æ›´é«˜ï¼Œåˆ™æ›¿æ¢
                existing_index = self._find_similar_result_index(deduplicated, result)
                if existing_index >= 0:
                    existing_quality = deduplicated[existing_index].get("comprehensive_quality", 0)
                    current_quality = result.get("comprehensive_quality", 0)
                    
                    if current_quality > existing_quality:
                        deduplicated[existing_index] = result
        
        return deduplicated
    
    def _generate_content_fingerprint(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆå†…å®¹æŒ‡çº¹."""
        title = result.get("title", "").lower().strip()
        content = result.get("content", "").lower().strip()
        
        # æå–å…³é”®ç‰¹å¾è¯
        import re
        title_words = set(re.findall(r'\w+', title)[:15])
        content_words = set(re.findall(r'\w+', content)[:50])
        
        # ç”ŸæˆæŒ‡çº¹
        fingerprint = "|".join([
            "t:" + "_".join(sorted(title_words)),
            "c:" + "_".join(sorted(list(content_words)[:25]))
        ])
        
        return fingerprint
    
    def _calculate_similarity(self, sig1: str, sig2: str) -> float:
        """è®¡ç®—æŒ‡çº¹ç›¸ä¼¼æ€§."""
        try:
            parts1 = sig1.split("|")
            parts2 = sig2.split("|")
            
            if len(parts1) != len(parts2):
                return 0.0
            
            similarities = []
            for p1, p2 in zip(parts1, parts2):
                words1 = set(p1.split("_"))
                words2 = set(p2.split("_"))
                
                intersection = len(words1 & words2)
                union = len(words1 | words2)
                
                if union == 0:
                    similarities.append(0.0)
                else:
                    similarities.append(intersection / union)
            
            return sum(similarities) / len(similarities)
            
        except Exception:
            return 0.0
    
    def _find_similar_result_index(self, results: List[Dict[str, Any]], target: Dict[str, Any]) -> int:
        """æŸ¥æ‰¾ç›¸ä¼¼ç»“æœçš„ç´¢å¼•."""
        target_sig = self._generate_content_fingerprint(target)
        
        for i, result in enumerate(results):
            result_sig = self._generate_content_fingerprint(result)
            if self._calculate_similarity(target_sig, result_sig) > 0.85:
                return i
        
        return -1
    
    async def _multi_dimensional_ranking(self, results: List[Dict[str, Any]], keywords: List[str]) -> List[Dict[str, Any]]:
        """å¤šç»´åº¦æ’åºç®—æ³•."""
        # è®¡ç®—ç»¼åˆæ’åºåˆ†æ•°
        for result in results:
            comprehensive_quality = result.get("comprehensive_quality", 0.5)
            semantic_relevance = result.get("semantic_relevance", 0.5)
            authority_score = result.get("authority_score", 0.5)
            freshness_score = result.get("freshness_score", 0.5)
            
            # åŠ æƒè®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = (
                comprehensive_quality * 0.3 +
                semantic_relevance * 0.35 +
                authority_score * 0.2 +
                freshness_score * 0.15
            )
            
            result["final_ranking_score"] = final_score
        
        # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
        ranked_results = sorted(
            results,
            key=lambda x: x.get("final_ranking_score", 0),
            reverse=True
        )
        
        return ranked_results
    
    async def _diversity_optimization(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤šæ ·æ€§ä¼˜åŒ–ï¼Œç¡®ä¿ç»“æœçš„å¤šæ ·æ€§."""
        if len(results) <= 3:
            return results
        
        diversified = []
        remaining = results.copy()
        
        # é€‰æ‹©æœ€é«˜è´¨é‡çš„ç»“æœä½œä¸ºç¬¬ä¸€ä¸ª
        if remaining:
            best_result = remaining.pop(0)
            diversified.append(best_result)
        
        # é€ä¸ªé€‰æ‹©ä¸å·²é€‰ç»“æœå·®å¼‚è¾ƒå¤§çš„ç»“æœ
        while remaining and len(diversified) < 15:
            best_candidate = None
            best_diversity_score = -1
            
            for candidate in remaining:
                # è®¡ç®—å¤šæ ·æ€§åˆ†æ•°
                diversity_score = self._calculate_diversity_score(candidate, diversified)
                
                # ç»¼åˆè€ƒè™‘è´¨é‡å’Œå¤šæ ·æ€§
                quality_score = candidate.get("final_ranking_score", 0)
                combined_score = quality_score * 0.7 + diversity_score * 0.3
                
                if combined_score > best_diversity_score:
                    best_diversity_score = combined_score
                    best_candidate = candidate
            
            if best_candidate:
                diversified.append(best_candidate)
                remaining.remove(best_candidate)
            else:
                break
        
        return diversified
    
    def _calculate_diversity_score(self, candidate: Dict[str, Any], selected: List[Dict[str, Any]]) -> float:
        """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°."""
        if not selected:
            return 1.0
        
        candidate_sig = self._generate_content_fingerprint(candidate)
        
        similarities = []
        for selected_result in selected:
            selected_sig = self._generate_content_fingerprint(selected_result)
            similarity = self._calculate_similarity(candidate_sig, selected_sig)
            similarities.append(similarity)
        
        # å¤šæ ·æ€§åˆ†æ•° = 1 - æœ€å¤§ç›¸ä¼¼æ€§
        max_similarity = max(similarities) if similarities else 0
        diversity_score = 1.0 - max_similarity
        
        return max(diversity_score, 0.0)
    
    def _calculate_comprehensive_quality(self, result: Dict[str, Any], keywords: List[str]) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°."""
        relevance = result.get("relevance_score", 0.5)
        authority = result.get("authority_score", 0.5)
        freshness = result.get("freshness_score", 0.5)
        confidence = result.get("confidence", 0.5)
        
        # åŠ æƒè®¡ç®—
        weights = {
            "relevance": 0.4,
            "authority": 0.25,
            "freshness": 0.2,
            "confidence": 0.15
        }
        
        comprehensive_score = (
            relevance * weights["relevance"] +
            authority * weights["authority"] +
            freshness * weights["freshness"] +
            confidence * weights["confidence"]
        )
        
        return min(comprehensive_score, 1.0)
    
    async def _get_fallback_results(self, keywords: List[str], search_type: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–é™çº§ç»“æœ."""
        logger.info("Using Bocha AI fallback results")
        
        fallback_results = []
        for i in range(min(limit, 3)):  # é™çº§æ—¶è¿”å›æ›´å°‘ç»“æœ
            result = {
                "title": f"[é™çº§ç»“æœ] {keywords[0] if keywords else 'æŠ€æœ¯'}ç›¸å…³åˆ†æ {i+1}",
                "url": f"https://bocha.ai/fallback/{i+1}",
                "content": f"ç”±äºç½‘ç»œé—®é¢˜ï¼Œè¿™æ˜¯å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„é™çº§åˆ†æç»“æœ...",
                "source": "Bocha AI",
                "search_type": search_type,
                "relevance_score": 0.2,
                "is_fallback": True,
                "generated_at": datetime.now().isoformat()
            }
            fallback_results.append(result)
        
        return fallback_results
    
    async def close(self):
        """å…³é—­HTTPä¼šè¯."""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£."""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£."""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def _simple_deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç®€å•å»é‡ç®—æ³•ï¼ŒåŸºäºæ ‡é¢˜ç›¸ä¼¼æ€§."""
        if not results:
            return []
        
        deduplicated = []
        seen_titles = set()
        
        for result in results:
            title = result.get("title", "").lower().strip()
            
            # ç”Ÿæˆæ ‡é¢˜çš„ç®€åŒ–ç‰ˆæœ¬ç”¨äºæ¯”è¾ƒ
            title_words = set(title.split()[:5])  # å–å‰5ä¸ªè¯
            title_signature = "_".join(sorted(title_words))
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼æ ‡é¢˜
            is_duplicate = False
            for seen_sig in seen_titles:
                # ç®€å•çš„ç›¸ä¼¼æ€§æ£€æŸ¥
                seen_words = set(seen_sig.split("_"))
                common_words = title_words & seen_words
                if len(common_words) >= min(3, len(title_words) * 0.7):
                    is_duplicate = True
                    break
            
            if not is_duplicate and title_signature:
                seen_titles.add(title_signature)
                deduplicated.append(result)
        
        return deduplicated
    
    def _simple_diversity_optimization(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç®€å•çš„å¤šæ ·æ€§ä¼˜åŒ–."""
        if len(results) <= 5:
            return results
        
        # æŒ‰æ¥æºåˆ†ç»„
        source_groups = {}
        for result in results:
            source = result.get('search_source', result.get('source', 'unknown'))
            if source not in source_groups:
                source_groups[source] = []
            source_groups[source].append(result)
        
        # ä»æ¯ä¸ªæ¥æºé€‰æ‹©æœ€å¥½çš„ç»“æœ
        diversified = []
        max_per_source = max(1, len(results) // len(source_groups))
        
        for source, source_results in source_groups.items():
            # æŒ‰è´¨é‡åˆ†æ•°æ’åº
            sorted_source_results = sorted(
                source_results,
                key=lambda x: x.get('quality_score', 0),
                reverse=True
            )
            diversified.extend(sorted_source_results[:max_per_source])
        
        # æŒ‰æ€»ä½“è´¨é‡åˆ†æ•°æ’åº
        return sorted(diversified, key=lambda x: x.get('quality_score', 0), reverse=True)


class SmartCrawler:
    """æ™ºèƒ½ç½‘é¡µçˆ¬è™«ï¼Œæ”¯æŒå¤šç§åçˆ¬è™«ç­–ç•¥å’Œåˆè§„æ€§æ£€æŸ¥."""
    
    def __init__(self):
        self.timeout = 30
        self.session = None
        self.rate_limiters = {}  # æ¯ä¸ªåŸŸåçš„é€Ÿç‡é™åˆ¶å™¨
        
        # å¤šç§User-Agentè½®æ¢
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0"
        ]
        
        # ç›®æ ‡ç½‘ç«™é…ç½®
        self.target_sites = {
            "patents.google.com": {
                "enabled": True,
                "rate_limit": 2,  # æ¯ç§’æœ€å¤š2ä¸ªè¯·æ±‚
                "delay_range": (1, 3),  # éšæœºå»¶è¿Ÿ1-3ç§’
                "selectors": {
                    "title": "h1, .patent-title, .invention-title",
                    "abstract": ".abstract, .patent-abstract, .description",
                    "content": ".patent-text, .description, .claims"
                },
                "search_patterns": [
                    "/?q={keywords}",
                    "/?q={keywords}&oq={keywords}"
                ],
                "robots_txt": "https://patents.google.com/robots.txt"
            },
            "www.wipo.int": {
                "enabled": True,
                "rate_limit": 1,  # æ¯ç§’æœ€å¤š1ä¸ªè¯·æ±‚
                "delay_range": (2, 5),  # éšæœºå»¶è¿Ÿ2-5ç§’
                "selectors": {
                    "title": "h1, .page-title, .article-title",
                    "content": ".content, .article-content, .main-content",
                    "abstract": ".summary, .abstract, .excerpt"
                },
                "search_patterns": [
                    "/search?q={keywords}",
                    "/publications?search={keywords}"
                ],
                "robots_txt": "https://www.wipo.int/robots.txt"
            },
            "scholar.google.com": {
                "enabled": True,
                "rate_limit": 1,  # è°·æ­Œå­¦æœ¯éœ€è¦æ›´ä¸¥æ ¼çš„é™åˆ¶
                "delay_range": (3, 8),
                "selectors": {
                    "title": "h3 a, .gs_rt a",
                    "abstract": ".gs_rs",
                    "content": ".gs_a"
                },
                "search_patterns": [
                    "/scholar?q={keywords}+patent",
                    "/scholar?q={keywords}+technology"
                ],
                "robots_txt": "https://scholar.google.com/robots.txt"
            }
        }
        
        # åçˆ¬è™«ç­–ç•¥é…ç½®
        self.anti_detection = {
            "rotate_user_agent": True,
            "random_delays": True,
            "respect_robots_txt": True,
            "use_proxies": False,  # å¯ä»¥é…ç½®ä»£ç†æ± 
            "session_rotation": True,
            "request_headers_variation": True
        }
        
        # åˆè§„æ€§æ£€æŸ¥é…ç½®
        self.compliance_config = {
            "check_robots_txt": True,
            "respect_crawl_delay": True,
            "max_requests_per_domain": 50,  # æ¯ä¸ªåŸŸåæœ€å¤šè¯·æ±‚æ•°
            "blacklisted_domains": [],
            "require_ssl": True
        }
        
        # å†…å®¹æå–é…ç½®
        self.extraction_config = {
            "min_content_length": 50,
            "max_content_length": 10000,
            "extract_images": False,
            "extract_links": True,
            "clean_html": True,
            "extract_metadata": True
        }
        
        # å¤±è´¥é‡è¯•é…ç½®
        self.retry_config = {
            "max_retries": 3,
            "retry_delays": [1, 2, 4],  # æŒ‡æ•°é€€é¿
            "retry_on_status": [429, 500, 502, 503, 504],
            "retry_on_timeout": True
        }
    
    async def search(self, keywords: List[str], search_type: str = "general", limit: int = 20) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ™ºèƒ½ç½‘é¡µçˆ¬å–æœç´¢."""
        try:
            # åˆå§‹åŒ–ä¼šè¯
            await self._ensure_session()
            
            # æ£€æŸ¥åˆè§„æ€§
            if not await self._check_compliance():
                logger.warning("Compliance check failed, using limited crawling")
                return await self._limited_crawl(keywords, limit)
            
            # é€‰æ‹©ç›®æ ‡ç½‘ç«™
            target_sites = await self._select_target_sites(search_type)
            
            # å¹¶è¡Œçˆ¬å–å¤šä¸ªç½‘ç«™
            crawl_tasks = []
            for site_domain in target_sites:
                task = self._crawl_site_with_keywords(site_domain, keywords, limit // len(target_sites))
                crawl_tasks.append((site_domain, task))
            
            # æ‰§è¡Œçˆ¬å–ä»»åŠ¡
            all_results = []
            completed_tasks = await asyncio.gather(
                *[task for _, task in crawl_tasks],
                return_exceptions=True
            )
            
            # å¤„ç†ç»“æœ
            for i, (site_domain, _) in enumerate(crawl_tasks):
                result = completed_tasks[i]
                if isinstance(result, Exception):
                    logger.error(f"Crawling failed for {site_domain}: {str(result)}")
                else:
                    all_results.extend(result or [])
            
            # åå¤„ç†å’Œè´¨é‡æ§åˆ¶
            processed_results = await self._post_process_results(all_results, keywords)
            
            logger.info(f"Web crawling completed: {len(processed_results)} results")
            return processed_results[:limit]
            
        except Exception as e:
            logger.error(f"Web crawling search failed: {str(e)}")
            return await self._fallback_crawl(keywords, limit)
    
    async def _ensure_session(self):
        """ç¡®ä¿HTTPä¼šè¯å­˜åœ¨."""
        if not self.session:
            connector = aiohttp.TCPConnector(
                limit=10,
                limit_per_host=2,
                ttl_dns_cache=300,
                use_dns_cache=True,
                ssl=False  # å…è®¸HTTPè¿æ¥ç”¨äºæµ‹è¯•
            )
            
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            self.session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers=self._get_base_headers()
            )
    
    def _get_base_headers(self) -> Dict[str, str]:
        """è·å–åŸºç¡€è¯·æ±‚å¤´."""
        import random
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
    
    async def _check_compliance(self) -> bool:
        """æ£€æŸ¥çˆ¬å–åˆè§„æ€§."""
        try:
            # æ£€æŸ¥robots.txtï¼ˆç®€åŒ–å®ç°ï¼‰
            if self.compliance_config["check_robots_txt"]:
                # è¿™é‡Œåº”è¯¥å®é™…æ£€æŸ¥robots.txt
                # ç®€åŒ–ä¸ºæ€»æ˜¯è¿”å›True
                pass
            
            return True
            
        except Exception as e:
            logger.error(f"Compliance check failed: {str(e)}")
            return False
    
    async def _select_target_sites(self, search_type: str) -> List[str]:
        """æ ¹æ®æœç´¢ç±»å‹é€‰æ‹©ç›®æ ‡ç½‘ç«™."""
        available_sites = []
        
        for domain, config in self.target_sites.items():
            if config.get("enabled", False):
                # æ ¹æ®æœç´¢ç±»å‹è¿‡æ»¤ç½‘ç«™
                if search_type == "patent" and "patent" in domain:
                    available_sites.append(domain)
                elif search_type == "academic" and "scholar" in domain:
                    available_sites.append(domain)
                elif search_type == "general":
                    available_sites.append(domain)
        
        # å¦‚æœæ²¡æœ‰ç‰¹å®šç½‘ç«™ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨ç½‘ç«™
        if not available_sites:
            available_sites = [domain for domain, config in self.target_sites.items() 
                             if config.get("enabled", False)]
        
        return available_sites[:3]  # é™åˆ¶åŒæ—¶çˆ¬å–çš„ç½‘ç«™æ•°é‡
    
    async def _crawl_site_with_keywords(self, site_domain: str, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """çˆ¬å–æŒ‡å®šç½‘ç«™çš„å…³é”®è¯ç›¸å…³å†…å®¹."""
        try:
            site_config = self.target_sites.get(site_domain, {})
            
            # é€Ÿç‡é™åˆ¶
            await self._apply_rate_limit(site_domain)
            
            # æ„å»ºæœç´¢URL
            search_urls = self._build_search_urls(site_domain, keywords)
            
            results = []
            for search_url in search_urls[:2]:  # é™åˆ¶æ¯ä¸ªç½‘ç«™çš„æœç´¢URLæ•°é‡
                try:
                    # æ‰§è¡Œå•ä¸ªURLçˆ¬å–
                    url_results = await self._crawl_single_url(search_url, site_config, keywords)
                    results.extend(url_results)
                    
                    if len(results) >= limit:
                        break
                        
                except Exception as e:
                    logger.error(f"Failed to crawl {search_url}: {str(e)}")
                    continue
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"Site crawling failed for {site_domain}: {str(e)}")
            return []
    
    async def _apply_rate_limit(self, domain: str):
        """åº”ç”¨é€Ÿç‡é™åˆ¶."""
        import time
        
        site_config = self.target_sites.get(domain, {})
        rate_limit = site_config.get("rate_limit", 1)
        
        # æ£€æŸ¥ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
        if domain not in self.rate_limiters:
            self.rate_limiters[domain] = {"last_request": 0, "request_count": 0}
        
        limiter = self.rate_limiters[domain]
        current_time = time.time()
        
        # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
        time_since_last = current_time - limiter["last_request"]
        min_interval = 1.0 / rate_limit
        
        if time_since_last < min_interval:
            sleep_time = min_interval - time_since_last
            
            # æ·»åŠ éšæœºå»¶è¿Ÿä»¥é¿å…æ£€æµ‹
            if self.anti_detection["random_delays"]:
                delay_range = site_config.get("delay_range", (1, 2))
                import random
                additional_delay = random.uniform(*delay_range)
                sleep_time += additional_delay
            
            await asyncio.sleep(sleep_time)
        
        limiter["last_request"] = time.time()
        limiter["request_count"] += 1
    
    def _build_search_urls(self, site_domain: str, keywords: List[str]) -> List[str]:
        """æ„å»ºæœç´¢URL."""
        site_config = self.target_sites.get(site_domain, {})
        search_patterns = site_config.get("search_patterns", [])
        
        urls = []
        keywords_str = "+".join(keywords[:3])  # é™åˆ¶å…³é”®è¯æ•°é‡
        
        for pattern in search_patterns:
            try:
                # ä½¿ç”¨quoteç¡®ä¿URLç¼–ç æ­£ç¡®
                encoded_keywords = quote(keywords_str)
                url = f"https://{site_domain}" + pattern.format(keywords=encoded_keywords)
                urls.append(url)
            except Exception as e:
                logger.error(f"Failed to build URL from pattern {pattern}: {str(e)}")
        
        return urls
    
    async def _crawl_single_url(self, url: str, site_config: Dict[str, Any], keywords: List[str]) -> List[Dict[str, Any]]:
        """çˆ¬å–å•ä¸ªURL."""
        try:
            # å‡†å¤‡è¯·æ±‚å¤´
            headers = self._get_request_headers()
            
            # æ‰§è¡Œè¯·æ±‚ï¼ˆå¸¦é‡è¯•ï¼‰
            html_content = await self._fetch_with_retry(url, headers)
            
            if not html_content:
                return []
            
            # è§£æHTMLå†…å®¹
            parsed_results = await self._parse_html_content(html_content, url, site_config, keywords)
            
            return parsed_results
            
        except Exception as e:
            logger.error(f"Failed to crawl URL {url}: {str(e)}")
            return []
    
    def _get_request_headers(self) -> Dict[str, str]:
        """è·å–è¯·æ±‚å¤´ï¼ˆåŒ…å«åæ£€æµ‹ç­–ç•¥ï¼‰."""
        headers = self._get_base_headers()
        
        # è½®æ¢User-Agent
        if self.anti_detection["rotate_user_agent"]:
            import random
            headers["User-Agent"] = random.choice(self.user_agents)
        
        # å˜åŒ–è¯·æ±‚å¤´
        if self.anti_detection["request_headers_variation"]:
            import random
            
            # éšæœºæ·»åŠ ä¸€äº›å¸¸è§çš„è¯·æ±‚å¤´
            optional_headers = {
                "DNT": "1",
                "Cache-Control": "no-cache",
                "Pragma": "no-cache",
            }
            
            for key, value in optional_headers.items():
                if random.random() > 0.5:  # 50%æ¦‚ç‡æ·»åŠ 
                    headers[key] = value
        
        return headers
    
    async def _fetch_with_retry(self, url: str, headers: Dict[str, str]) -> Optional[str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„ç½‘é¡µè·å–."""
        for attempt in range(self.retry_config["max_retries"]):
            try:
                async with self.session.get(url, headers=headers) as response:
                    # æ£€æŸ¥çŠ¶æ€ç 
                    if response.status == 200:
                        content = await response.text()
                        return content
                    elif response.status in self.retry_config["retry_on_status"]:
                        if attempt < self.retry_config["max_retries"] - 1:
                            delay = self.retry_config["retry_delays"][min(attempt, len(self.retry_config["retry_delays"]) - 1)]
                            logger.warning(f"HTTP {response.status} for {url}, retrying in {delay}s")
                            await asyncio.sleep(delay)
                            continue
                    else:
                        logger.error(f"HTTP {response.status} for {url}")
                        return None
                        
            except asyncio.TimeoutError:
                if self.retry_config["retry_on_timeout"] and attempt < self.retry_config["max_retries"] - 1:
                    delay = self.retry_config["retry_delays"][min(attempt, len(self.retry_config["retry_delays"]) - 1)]
                    logger.warning(f"Timeout for {url}, retrying in {delay}s")
                    await asyncio.sleep(delay)
                    continue
                else:
                    logger.error(f"Timeout for {url}")
                    return None
            except Exception as e:
                logger.error(f"Request failed for {url}: {str(e)}")
                if attempt < self.retry_config["max_retries"] - 1:
                    delay = self.retry_config["retry_delays"][min(attempt, len(self.retry_config["retry_delays"]) - 1)]
                    await asyncio.sleep(delay)
                    continue
                return None
        
        return None
    
    async def _parse_html_content(self, html_content: str, url: str, site_config: Dict[str, Any], keywords: List[str]) -> List[Dict[str, Any]]:
        """è§£æHTMLå†…å®¹æå–æœ‰ç”¨ä¿¡æ¯."""
        try:
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨BeautifulSoupè¿›è¡ŒçœŸå®çš„HTMLè§£æ
            # ç”±äºæ²¡æœ‰å®‰è£…BeautifulSoupï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿè§£æè¿‡ç¨‹
            
            results = []
            
            # æ¨¡æ‹Ÿä»HTMLä¸­æå–çš„å†…å®¹
            # å®é™…å®ç°åº”è¯¥ä½¿ç”¨BeautifulSoupçš„CSSé€‰æ‹©å™¨
            selectors = site_config.get("selectors", {})
            
            # æ¨¡æ‹Ÿæå–å¤šä¸ªç»“æœ
            for i in range(3):  # æ¯ä¸ªé¡µé¢æ¨¡æ‹Ÿæå–3ä¸ªç»“æœ
                result = {
                    "title": f"ä»{url}æå–çš„æ ‡é¢˜ {i+1}: {keywords[0] if keywords else 'æŠ€æœ¯'}ç›¸å…³å†…å®¹",
                    "url": f"{url}#result_{i+1}",
                    "content": f"è¿™æ˜¯ä»ç½‘é¡µ{url}æå–çš„å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„è¯¦ç»†å†…å®¹...",
                    "summary": f"ç½‘é¡µå†…å®¹æ‘˜è¦ï¼š{keywords[0] if keywords else 'æŠ€æœ¯'}ç›¸å…³ä¿¡æ¯...",
                    "crawl_date": datetime.now().isoformat(),
                    "source_url": url,
                    "extraction_method": "html_parsing",
                    "selectors_used": selectors,
                    "content_length": len(html_content),
                    "keywords_found": [kw for kw in keywords if kw.lower() in html_content.lower()]
                }
                
                # è´¨é‡æ£€æŸ¥
                if self._validate_extracted_content(result):
                    results.append(result)
            
            return results
            
        except Exception as e:
            logger.error(f"HTML parsing failed for {url}: {str(e)}")
            return []
    
    def _validate_extracted_content(self, result: Dict[str, Any]) -> bool:
        """éªŒè¯æå–çš„å†…å®¹è´¨é‡."""
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        content = result.get("content", "")
        if len(content) < self.extraction_config["min_content_length"]:
            return False
        
        if len(content) > self.extraction_config["max_content_length"]:
            return False
        
        # æ£€æŸ¥æ ‡é¢˜
        title = result.get("title", "")
        if len(title) < 5:
            return False
        
        return True
    
    async def _post_process_results(self, results: List[Dict[str, Any]], keywords: List[str]) -> List[Dict[str, Any]]:
        """åå¤„ç†çˆ¬å–ç»“æœ."""
        processed_results = []
        
        for result in results:
            # æ¸…ç†å†…å®¹
            if self.extraction_config["clean_html"]:
                result["content"] = self._clean_html_content(result.get("content", ""))
            
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            result["relevance_score"] = self._calculate_content_relevance(result, keywords)
            
            # æ·»åŠ å…ƒæ•°æ®
            if self.extraction_config["extract_metadata"]:
                result["metadata"] = {
                    "crawl_timestamp": datetime.now().isoformat(),
                    "crawler_version": "1.0",
                    "processing_flags": {
                        "html_cleaned": self.extraction_config["clean_html"],
                        "content_validated": True
                    }
                }
            
            processed_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        processed_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        return processed_results
    
    def _clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹."""
        # ç®€å•çš„HTMLæ¸…ç†ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„æ¸…ç†é€»è¾‘ï¼‰
        import re
        
        # ç§»é™¤HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:]', '', content)
        
        return content.strip()
    
    def _calculate_content_relevance(self, result: Dict[str, Any], keywords: List[str]) -> float:
        """è®¡ç®—å†…å®¹ç›¸å…³æ€§."""
        content = (result.get("content", "") + " " + result.get("title", "")).lower()
        
        if not keywords:
            return 0.5
        
        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        matches = 0
        for keyword in keywords:
            if keyword.lower() in content:
                matches += 1
        
        relevance = matches / len(keywords)
        return min(relevance, 1.0)
    
    async def _limited_crawl(self, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """å—é™çˆ¬å–ï¼ˆåˆè§„æ€§æ£€æŸ¥å¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰."""
        logger.info("Using limited crawling mode")
        
        results = []
        for i in range(min(limit, 3)):
            result = {
                "title": f"[å—é™çˆ¬å–] {keywords[0] if keywords else 'æŠ€æœ¯'}ç›¸å…³ä¿¡æ¯ {i+1}",
                "url": f"https://limited-crawl.local/{i+1}",
                "content": f"ç”±äºåˆè§„æ€§é™åˆ¶ï¼Œè¿™æ˜¯å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„å—é™æœç´¢ç»“æœ...",
                "source": "Limited Crawler",
                "crawl_date": datetime.now().isoformat(),
                "is_limited": True,
                "relevance_score": 0.3
            }
            results.append(result)
        
        return results
    
    async def _fallback_crawl(self, keywords: List[str], limit: int) -> List[Dict[str, Any]]:
        """é™çº§çˆ¬å–ï¼ˆçˆ¬å–å¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰."""
        logger.info("Using fallback crawling mode")
        
        results = []
        for i in range(min(limit, 2)):
            result = {
                "title": f"[é™çº§çˆ¬å–] {keywords[0] if keywords else 'æŠ€æœ¯'}åŸºç¡€ä¿¡æ¯ {i+1}",
                "url": f"https://fallback-crawl.local/{i+1}",
                "content": f"ç”±äºç½‘ç»œé—®é¢˜ï¼Œè¿™æ˜¯å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„é™çº§æœç´¢ç»“æœ...",
                "source": "Fallback Crawler",
                "crawl_date": datetime.now().isoformat(),
                "is_fallback": True,
                "relevance_score": 0.2
            }
            results.append(result)
        
        return results
    
    async def crawl_site(self, site_url: str, keywords: List[str]) -> List[Dict[str, Any]]:
        """çˆ¬å–æŒ‡å®šç½‘ç«™ï¼ˆå…¬å…±æ¥å£ï¼‰."""
        try:
            await self._ensure_session()
            
            # è§£æåŸŸå
            from urllib.parse import urlparse
            parsed_url = urlparse(site_url)
            domain = parsed_url.netloc
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„ç½‘ç«™
            if domain not in self.target_sites:
                logger.warning(f"Unsupported site: {domain}")
                return []
            
            # æ‰§è¡Œçˆ¬å–
            results = await self._crawl_site_with_keywords(domain, keywords, 10)
            return results
            
        except Exception as e:
            logger.error(f"Site crawling failed for {site_url}: {str(e)}")
            return []
    
    async def close(self):
        """å…³é—­HTTPä¼šè¯."""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def _get_mock_web_results(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–æ¨¡æ‹ŸWebæœç´¢ç»“æœ."""
        mock_results = []
        
        for i in range(min(limit, 5)):
            result = {
                "title": f"å…³äº'{query}'çš„ç½‘é¡µæœç´¢ç»“æœ {i+1}",
                "url": f"https://example.com/search/{i+1}",
                "content": f"è¿™æ˜¯å…³äº{query}çš„è¯¦ç»†ä¿¡æ¯ã€‚åŒ…å«ç›¸å…³çš„æŠ€æœ¯èµ„æ–™å’Œç ”ç©¶å†…å®¹ã€‚",
                "summary": f"å…³äº{query}çš„æ‘˜è¦ä¿¡æ¯",
                "source_domain": "example.com",
                "content_type": "webpage",
                "relevance_score": 0.6 - i * 0.1,
                "authority_score": 0.5,
                "freshness_score": 0.7,
                "source": "mock_web_search",
                "is_mock": True
            }
            mock_results.append(result)
        
        return mock_results
    
    async def _get_mock_ai_results(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–æ¨¡æ‹ŸAIæœç´¢ç»“æœ."""
        mock_results = []
        
        for i in range(min(limit, 3)):
            result = {
                "title": f"AIåˆ†æ: {query}",
                "content": f"åŸºäºAIåˆ†æï¼Œ{query}ç›¸å…³çš„æŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œåº”ç”¨å‰æ™¯å€¼å¾—å…³æ³¨ã€‚",
                "summary": f"AIå¯¹{query}çš„æ™ºèƒ½åˆ†æ",
                "content_type": "ai_analysis",
                "relevance_score": 0.8 - i * 0.1,
                "authority_score": 0.6,
                "freshness_score": 1.0,
                "source": "mock_ai_search",
                "is_mock": True
            }
            mock_results.append(result)
        
        return mock_results
    
    async def _get_fallback_results(self, keywords: List[str], search_type: str, limit: int) -> List[Dict[str, Any]]:
        """è·å–é™çº§æœç´¢ç»“æœ."""
        query = " ".join(keywords)
        
        fallback_results = []
        
        # åŸºç¡€é™çº§ç»“æœ
        for i in range(min(limit, 3)):
            result = {
                "title": f"[é™çº§æœç´¢] {query} - åŸºç¡€ä¿¡æ¯ {i+1}",
                "url": f"https://fallback.local/{i+1}",
                "content": f"ç”±äºæœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¿™æ˜¯å…³äº{query}çš„åŸºç¡€ä¿¡æ¯ã€‚å»ºè®®ç¨åé‡è¯•ã€‚",
                "summary": f"å…³äº{query}çš„åŸºç¡€ä¿¡æ¯",
                "content_type": "fallback",
                "relevance_score": 0.3,
                "authority_score": 0.2,
                "freshness_score": 0.1,
                "source": "fallback_search",
                "is_fallback": True
            }
            fallback_results.append(result)
        
        return fallback_results
    
    async def _optimize_results(self, results: List[Dict[str, Any]], keywords: List[str]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–æœç´¢ç»“æœï¼ˆå½“é‡æ’åºä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰."""
        if not results:
            return []
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        for result in results:
            relevance = result.get("relevance_score", 0.5)
            authority = result.get("authority_score", 0.5)
            freshness = result.get("freshness_score", 0.5)
            
            # ç»¼åˆè´¨é‡åˆ†æ•°
            quality_score = (
                relevance * 0.4 +
                authority * 0.3 +
                freshness * 0.3
            )
            
            result["quality_score"] = quality_score
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åº
        sorted_results = sorted(
            results,
            key=lambda x: x.get("quality_score", 0),
            reverse=True
        )
        
        return sorted_results