"""Patent analysis agent implementation."""

import asyncio
import logging
import json
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from collections import defaultdict
import re

from .base import PatentBaseAgent
from .trend_analyzer import TrendAnalyzer
from .tech_classifier import TechClassifier
from .competition_analyzer import CompetitionAnalyzer
from .quality_controller import AnalysisQualityController
from ...models.base import UserRequest, AgentResponse, Action
from ...models.config import AgentConfig
from ...models.enums import AgentType
from ...services.model_client import BaseModelClient


logger = logging.getLogger(__name__)


class PatentAnalysisAgent(PatentBaseAgent):
    """ä¸“åˆ©åˆ†æå¤„ç†Agentï¼Œè´Ÿè´£æ·±åº¦åˆ†æä¸“åˆ©æ•°æ®å¹¶ç”Ÿæˆæ´å¯Ÿ."""
    
    def __init__(self, config: AgentConfig, model_client: BaseModelClient):
        """åˆå§‹åŒ–ä¸“åˆ©åˆ†æAgent."""
        super().__init__(config, model_client)
        
        # åˆ†æç›¸å…³å…³é”®è¯
        self.analysis_keywords = [
            "åˆ†æ", "è¶‹åŠ¿", "ç«äº‰", "æŠ€æœ¯", "å‘å±•", "é¢„æµ‹", "æ´å¯Ÿ", "æŠ¥å‘Š",
            "ç»Ÿè®¡", "å¯¹æ¯”", "è¯„ä¼°", "ç ”ç©¶", "è°ƒç ”", "å¸‚åœº", "è¡Œä¸š",
            "analysis", "trend", "competition", "technology", "development",
            "prediction", "insight", "report", "statistics", "comparison"
        ]
        
        # åˆå§‹åŒ–åˆ†æç»„ä»¶
        self.trend_analyzer = TrendAnalyzer()
        self.tech_classifier = TechClassifier()
        self.competition_analyzer = CompetitionAnalyzer()
        self.quality_controller = AnalysisQualityController()
        
        # åˆ†æé…ç½®
        self.analysis_config = {
            "min_patents_for_trend": 5,
            "trend_analysis_years": 10,
            "top_applicants_limit": 20,
            "tech_clusters_limit": 15,
            "quality_threshold": 0.7
        }
    
    async def can_handle_request(self, request: UserRequest) -> float:
        """åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†åˆ†æç›¸å…³è¯·æ±‚."""
        # å…ˆè°ƒç”¨çˆ¶ç±»çš„ä¸“åˆ©ç›¸å…³åˆ¤æ–­
        base_confidence = await super().can_handle_request(request)
        
        content = request.content.lower()
        
        # æ£€æŸ¥åˆ†æå…³é”®è¯
        analysis_matches = sum(1 for keyword in self.analysis_keywords if keyword in content)
        analysis_score = min(analysis_matches * 0.25, 0.7)
        
        # æ£€æŸ¥åˆ†æç‰¹å®šæ¨¡å¼
        analysis_patterns = [
            r"(åˆ†æ|ç ”ç©¶).*?(ä¸“åˆ©|æŠ€æœ¯|è¶‹åŠ¿)",
            r"(ç»Ÿè®¡|å¯¹æ¯”).*?(ç”³è¯·|å‘æ˜|ä¸“åˆ©)",
            r"(ç«äº‰|å¸‚åœº).*?(åˆ†æ|æ ¼å±€|æ€åŠ¿)",
            r"(æŠ€æœ¯|è¡Œä¸š).*?(å‘å±•|è¶‹åŠ¿|æ–¹å‘)",
            r"(analysis|research).*?(patent|technology|trend)",
            r"(statistics|comparison).*?(application|invention|patent)",
            r"(competition|market).*?(analysis|landscape|situation)"
        ]
        
        pattern_score = 0
        for pattern in analysis_patterns:
            if re.search(pattern, content):
                pattern_score += 0.2
        
        # ç»¼åˆè¯„åˆ†
        total_score = min(base_confidence + analysis_score + pattern_score, 1.0)
        
        return max(total_score, 0.0)
    
    async def get_capabilities(self) -> List[str]:
        """è·å–åˆ†æAgentçš„èƒ½åŠ›åˆ—è¡¨."""
        base_capabilities = await super().get_capabilities()
        analysis_capabilities = [
            "ä¸“åˆ©è¶‹åŠ¿åˆ†æ",
            "æŠ€æœ¯åˆ†ç±»ç»Ÿè®¡",
            "ç«äº‰æ ¼å±€åˆ†æ",
            "ç”³è¯·äººåˆ†æ",
            "åœ°åŸŸåˆ†å¸ƒåˆ†æ",
            "æ—¶é—´åºåˆ—åˆ†æ",
            "æŠ€æœ¯èšç±»åˆ†æ",
            "å¸‚åœºé›†ä¸­åº¦è®¡ç®—",
            "åˆ†æç»“æœè´¨é‡æ§åˆ¶"
        ]
        return base_capabilities + analysis_capabilities
    
    async def estimate_processing_time(self, request: UserRequest) -> int:
        """ä¼°ç®—åˆ†æå¤„ç†æ—¶é—´."""
        content = request.content.lower()
        
        # ç®€å•åˆ†æï¼š30-45ç§’
        if any(word in content for word in ["ç®€å•", "åŸºç¡€", "æ¦‚è§ˆ"]):
            return 35
        
        # æ·±åº¦åˆ†æï¼š60-90ç§’
        if any(word in content for word in ["æ·±åº¦", "è¯¦ç»†", "å…¨é¢", "å®Œæ•´"]):
            return 75
        
        # å¤æ‚åˆ†æï¼š90-120ç§’
        if any(word in content for word in ["å¤æ‚", "å¤šç»´", "ç»¼åˆ", "ç³»ç»Ÿ"]):
            return 105
        
        # é»˜è®¤åˆ†ææ—¶é—´
        return 60
    
    async def _process_request_specific(self, request: UserRequest) -> AgentResponse:
        """å¤„ç†åˆ†æç›¸å…³çš„å…·ä½“è¯·æ±‚."""
        start_time = datetime.now()
        
        try:
            # è§£æåˆ†æè¯·æ±‚
            analysis_params = self._parse_analysis_request(request.content)
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_analysis_cache_key(analysis_params)
            cached_result = await self.get_from_cache(cache_key)
            
            if cached_result:
                self.logger.info("Returning cached analysis results")
                return AgentResponse(
                    agent_id=self.agent_id,
                    agent_type=self.agent_type,
                    response_content=cached_result["response_content"],
                    confidence=0.9,
                    metadata={
                        **cached_result.get("metadata", {}),
                        "from_cache": True
                    }
                )
            
            # è·å–ä¸“åˆ©æ•°æ®ï¼ˆæ¨¡æ‹Ÿä»æ•°æ®æ”¶é›†Agentè·å–ï¼‰
            patent_data = await self._get_patent_data_for_analysis(analysis_params)
            
            if not patent_data or len(patent_data) < self.analysis_config["min_patents_for_trend"]:
                return AgentResponse(
                    agent_id=self.agent_id,
                    agent_type=self.agent_type,
                    response_content="æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æã€‚å»ºè®®æ‰©å¤§æœç´¢èŒƒå›´æˆ–è°ƒæ•´å…³é”®è¯ã€‚",
                    confidence=0.3,
                    collaboration_needed=True,
                    metadata={"insufficient_data": True, "data_count": len(patent_data) if patent_data else 0}
                )
            
            # æ‰§è¡Œåˆ†æ
            analysis_results = await self._execute_comprehensive_analysis(patent_data, analysis_params)
            
            # è´¨é‡æ§åˆ¶
            quality_report = await self.quality_controller.validate_analysis_results(analysis_results)
            
            if quality_report["overall_quality"] < self.analysis_config["quality_threshold"]:
                self.logger.warning(f"Analysis quality below threshold: {quality_report['overall_quality']}")
                # å°è¯•æ”¹è¿›åˆ†æ
                analysis_results = await self._improve_analysis_quality(analysis_results, quality_report)
            
            # ç”Ÿæˆå“åº”å†…å®¹
            response_content = await self._generate_analysis_response(analysis_results, analysis_params)
            
            # ç”Ÿæˆåç»­åŠ¨ä½œ
            next_actions = self._generate_analysis_actions(analysis_results)
            
            # ç¼“å­˜ç»“æœ
            result_data = {
                "response_content": response_content,
                "metadata": {
                    "analysis_params": analysis_params,
                    "data_count": len(patent_data),
                    "processing_time": (datetime.now() - start_time).total_seconds(),
                    "quality_score": quality_report["overall_quality"],
                    "analysis_types": list(analysis_results.keys())
                }
            }
            await self.save_to_cache(cache_key, result_data)
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("analysis", duration, True)
            
            # é›†æˆç°æœ‰ç›‘æ§ç³»ç»Ÿè®°å½•åˆ†ææŒ‡æ ‡
            await self._log_analysis_metrics(analysis_params, analysis_results, duration)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=response_content,
                confidence=0.85,
                next_actions=next_actions,
                collaboration_needed=False,
                metadata=result_data["metadata"]
            )
            
        except Exception as e:
            self.logger.error(f"Error processing analysis request: {str(e)}")
            
            # è®°å½•å¤±è´¥æŒ‡æ ‡
            duration = (datetime.now() - start_time).total_seconds()
            self.log_performance_metrics("analysis", duration, False)
            
            return AgentResponse(
                agent_id=self.agent_id,
                agent_type=self.agent_type,
                response_content=f"åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
                confidence=0.0,
                collaboration_needed=True,
                metadata={
                    "error": str(e),
                    "processing_time": duration
                }
            )
    
    def _parse_analysis_request(self, content: str) -> Dict[str, Any]:
        """è§£æåˆ†æè¯·æ±‚å‚æ•°."""
        params = {
            "analysis_types": ["trend", "competition", "technology"],
            "keywords": [],
            "time_range": {"start_year": 2014, "end_year": 2024},
            "focus_areas": [],
            "depth": "standard"
        }
        
        content_lower = content.lower()
        
        # æå–å…³é”®è¯
        keywords = []
        quoted_keywords = re.findall(r'["""\'](.*?)["""\']', content)
        keywords.extend(quoted_keywords)
        
        # æŠ€æœ¯é¢†åŸŸå…³é”®è¯
        tech_patterns = [
            r'(äººå·¥æ™ºèƒ½|AI|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ )',
            r'(åŒºå—é“¾|blockchain)',
            r'(ç‰©è”ç½‘|IoT)',
            r'(5G|é€šä¿¡æŠ€æœ¯)',
            r'(æ–°èƒ½æº|ç”µæ± æŠ€æœ¯)',
            r'(ç”Ÿç‰©æŠ€æœ¯|åŸºå› )',
            r'(èŠ¯ç‰‡|åŠå¯¼ä½“)',
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, content)
            keywords.extend(matches)
        
        if not keywords:
            stop_words = {"çš„", "äº†", "åœ¨", "æ˜¯", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "ç­‰", "åˆ†æ", "ä¸“åˆ©"}
            words = content.split()
            keywords = [word for word in words if len(word) > 1 and word not in stop_words]
        
        params["keywords"] = keywords[:5]
        
        # åˆ¤æ–­åˆ†æç±»å‹
        if any(word in content_lower for word in ["è¶‹åŠ¿", "å‘å±•", "å˜åŒ–", "trend"]):
            if "trend" not in params["analysis_types"]:
                params["analysis_types"].append("trend")
        
        if any(word in content_lower for word in ["ç«äº‰", "ç”³è¯·äºº", "å…¬å¸", "competition"]):
            if "competition" not in params["analysis_types"]:
                params["analysis_types"].append("competition")
        
        if any(word in content_lower for word in ["æŠ€æœ¯", "åˆ†ç±»", "ipc", "technology"]):
            if "technology" not in params["analysis_types"]:
                params["analysis_types"].append("technology")
        
        if any(word in content_lower for word in ["åœ°åŸŸ", "å›½å®¶", "åœ°åŒº", "geographic"]):
            params["analysis_types"].append("geographic")
        
        # åˆ¤æ–­åˆ†ææ·±åº¦
        if any(word in content_lower for word in ["æ·±åº¦", "è¯¦ç»†", "å…¨é¢", "comprehensive"]):
            params["depth"] = "deep"
        elif any(word in content_lower for word in ["ç®€å•", "æ¦‚è§ˆ", "åŸºç¡€", "basic"]):
            params["depth"] = "basic"
        
        # æå–æ—¶é—´èŒƒå›´
        year_matches = re.findall(r'(\d{4})', content)
        if len(year_matches) >= 2:
            years = [int(y) for y in year_matches if 2000 <= int(y) <= 2024]
            if len(years) >= 2:
                params["time_range"]["start_year"] = min(years)
                params["time_range"]["end_year"] = max(years)
        
        return params
    
    def _generate_analysis_cache_key(self, analysis_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æç¼“å­˜é”®."""
        key_parts = [
            "analysis",
            "_".join(sorted(analysis_params["keywords"])),
            "_".join(sorted(analysis_params["analysis_types"])),
            f"{analysis_params['time_range']['start_year']}-{analysis_params['time_range']['end_year']}",
            analysis_params["depth"]
        ]
        return "_".join(key_parts).replace(" ", "_")
    
    async def _get_patent_data_for_analysis(self, analysis_params: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·å–ç”¨äºåˆ†æçš„ä¸“åˆ©æ•°æ®ï¼ˆæ¨¡æ‹Ÿä»æ•°æ®æ”¶é›†Agentè·å–ï¼‰."""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨ä¸“åˆ©æ•°æ®æ”¶é›†Agentæˆ–ä»ç¼“å­˜/æ•°æ®åº“è·å–æ•°æ®
        # ç°åœ¨æä¾›æ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
        
        keywords = analysis_params.get("keywords", ["æŠ€æœ¯"])
        start_year = analysis_params["time_range"]["start_year"]
        end_year = analysis_params["time_range"]["end_year"]
        
        # æ¨¡æ‹Ÿä¸“åˆ©æ•°æ®
        mock_patents = []
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„ä¸“åˆ©æ•°æ®
        applicants = [
            "åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸", "è…¾è®¯ç§‘æŠ€", "é˜¿é‡Œå·´å·´", "ç™¾åº¦", "å­—èŠ‚è·³åŠ¨",
            "Apple Inc.", "Google LLC", "Microsoft Corporation", "Samsung Electronics",
            "IBM Corporation", "Intel Corporation", "NVIDIA Corporation"
        ]
        
        ipc_classes = [
            "G06F", "H04L", "G06N", "H04W", "G06Q", "H01L", "G06K", "H04N", "G06T", "G01S"
        ]
        
        countries = ["CN", "US", "JP", "KR", "DE", "GB", "FR"]
        
        import random
        random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡ç°
        
        for i in range(100):  # ç”Ÿæˆ100ä¸ªæ¨¡æ‹Ÿä¸“åˆ©
            year = random.randint(start_year, end_year)
            month = random.randint(1, 12)
            day = random.randint(1, 28)
            
            patent = {
                "application_number": f"CN{year}{i:06d}",
                "title": f"å…³äº{keywords[0] if keywords else 'æŠ€æœ¯'}çš„{random.choice(['æ–¹æ³•', 'ç³»ç»Ÿ', 'è£…ç½®', 'ç®—æ³•'])} {i+1}",
                "abstract": f"æœ¬å‘æ˜æ¶‰åŠ{keywords[0] if keywords else 'æŠ€æœ¯'}é¢†åŸŸï¼Œæä¾›äº†ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆ...",
                "applicants": [random.choice(applicants)],
                "inventors": [f"å‘æ˜äºº{i+1}", f"å‘æ˜äºº{i+2}"],
                "application_date": f"{year}-{month:02d}-{day:02d}",
                "publication_date": f"{year}-{month+6 if month <= 6 else month-6:02d}-{day:02d}",
                "ipc_classes": [random.choice(ipc_classes)],
                "country": random.choice(countries),
                "status": random.choice(["å·²æˆæƒ", "å®¡æŸ¥ä¸­", "å·²å…¬å¼€"])
            }
            mock_patents.append(patent)
        
        self.logger.info(f"Generated {len(mock_patents)} mock patents for analysis")
        return mock_patents
    
    async def _execute_comprehensive_analysis(self, patent_data: List[Dict[str, Any]], analysis_params: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç»¼åˆåˆ†æ."""
        results = {}
        
        try:
            # å¹¶è¡Œæ‰§è¡Œä¸åŒç±»å‹çš„åˆ†æ
            analysis_tasks = []
            
            if "trend" in analysis_params["analysis_types"]:
                analysis_tasks.append(("trend", self.trend_analyzer.analyze_trends(patent_data, analysis_params)))
            
            if "technology" in analysis_params["analysis_types"]:
                analysis_tasks.append(("technology", self.tech_classifier.classify_technologies(patent_data, analysis_params)))
            
            if "competition" in analysis_params["analysis_types"]:
                analysis_tasks.append(("competition", self.competition_analyzer.analyze_competition(patent_data, analysis_params)))
            
            if "geographic" in analysis_params["analysis_types"]:
                analysis_tasks.append(("geographic", self._analyze_geographic_distribution(patent_data)))
            
            # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
            completed_analyses = await asyncio.gather(
                *[task for _, task in analysis_tasks],
                return_exceptions=True
            )
            
            # å¤„ç†åˆ†æç»“æœ
            for i, (analysis_type, _) in enumerate(analysis_tasks):
                result = completed_analyses[i]
                if isinstance(result, Exception):
                    self.logger.error(f"Analysis {analysis_type} failed: {str(result)}")
                    results[analysis_type] = {"error": str(result), "success": False}
                else:
                    results[analysis_type] = result
                    results[analysis_type]["success"] = True
            
            # ç”Ÿæˆç»¼åˆæ´å¯Ÿ
            if len([r for r in results.values() if r.get("success", False)]) >= 2:
                results["insights"] = await self._generate_comprehensive_insights(results, patent_data)
            
            return results
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive analysis: {str(e)}")
            return {"error": str(e), "success": False}
    
    async def _analyze_geographic_distribution(self, patent_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æåœ°åŸŸåˆ†å¸ƒ."""
        try:
            country_counts = defaultdict(int)
            
            for patent in patent_data:
                country = patent.get("country", "Unknown")
                country_counts[country] += 1
            
            # è®¡ç®—ç™¾åˆ†æ¯”
            total_patents = len(patent_data)
            country_percentages = {
                country: (count / total_patents) * 100
                for country, count in country_counts.items()
            }
            
            # æ’åº
            sorted_countries = sorted(
                country_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )
            
            return {
                "country_distribution": dict(country_counts),
                "country_percentages": country_percentages,
                "top_countries": sorted_countries[:10],
                "total_countries": len(country_counts),
                "analysis_summary": f"ä¸“åˆ©ç”³è¯·ä¸»è¦é›†ä¸­åœ¨{sorted_countries[0][0]}ç­‰{len(country_counts)}ä¸ªå›½å®¶/åœ°åŒº"
            }
            
        except Exception as e:
            self.logger.error(f"Error in geographic analysis: {str(e)}")
            return {"error": str(e)}
    
    async def _generate_comprehensive_insights(self, analysis_results: Dict[str, Any], patent_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ´å¯Ÿ."""
        insights = {
            "key_findings": [],
            "trends": [],
            "recommendations": [],
            "risk_factors": []
        }
        
        try:
            # åŸºäºè¶‹åŠ¿åˆ†æçš„æ´å¯Ÿ
            if "trend" in analysis_results and analysis_results["trend"].get("success"):
                trend_data = analysis_results["trend"]
                if trend_data.get("trend_direction") == "increasing":
                    insights["key_findings"].append("è¯¥æŠ€æœ¯é¢†åŸŸä¸“åˆ©ç”³è¯·é‡å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œæ˜¾ç¤ºå‡ºå¼ºåŠ²çš„åˆ›æ–°æ´»åŠ›")
                    insights["trends"].append("æŠ€æœ¯å‘å±•å¤„äºå¿«é€Ÿå¢é•¿æœŸ")
                elif trend_data.get("trend_direction") == "decreasing":
                    insights["key_findings"].append("ä¸“åˆ©ç”³è¯·é‡å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå¯èƒ½è¡¨æ˜æŠ€æœ¯æˆç†Ÿæˆ–å¸‚åœºé¥±å’Œ")
                    insights["risk_factors"].append("æŠ€æœ¯å‘å±•å¯èƒ½è¿›å…¥å¹³å°æœŸæˆ–è¡°é€€æœŸ")
            
            # åŸºäºç«äº‰åˆ†æçš„æ´å¯Ÿ
            if "competition" in analysis_results and analysis_results["competition"].get("success"):
                comp_data = analysis_results["competition"]
                market_concentration = comp_data.get("market_concentration", 0)
                
                if market_concentration > 0.7:
                    insights["key_findings"].append("å¸‚åœºé›†ä¸­åº¦è¾ƒé«˜ï¼Œå°‘æ•°ä¼ä¸šå æ®ä¸»å¯¼åœ°ä½")
                    insights["risk_factors"].append("å¸‚åœºç«äº‰æ¿€çƒˆï¼Œæ–°è¿›å…¥è€…é¢ä¸´è¾ƒé«˜å£å’")
                elif market_concentration < 0.3:
                    insights["key_findings"].append("å¸‚åœºç›¸å¯¹åˆ†æ•£ï¼Œç«äº‰æ ¼å±€è¾ƒä¸ºå¼€æ”¾")
                    insights["recommendations"].append("é€‚åˆæ–°æŠ€æœ¯ä¼ä¸šè¿›å…¥å’Œå‘å±•")
            
            # åŸºäºæŠ€æœ¯åˆ†æçš„æ´å¯Ÿ
            if "technology" in analysis_results and analysis_results["technology"].get("success"):
                tech_data = analysis_results["technology"]
                main_techs = tech_data.get("main_technologies", [])
                
                if main_techs:
                    insights["trends"].append(f"ä¸»è¦æŠ€æœ¯æ–¹å‘é›†ä¸­åœ¨{', '.join(main_techs[:3])}ç­‰é¢†åŸŸ")
                    insights["recommendations"].append("å»ºè®®é‡ç‚¹å…³æ³¨ä¸»æµæŠ€æœ¯æ–¹å‘çš„å‘å±•æœºä¼š")
            
            # ç»¼åˆå»ºè®®
            insights["recommendations"].extend([
                "æŒç»­ç›‘æ§æŠ€æœ¯å‘å±•è¶‹åŠ¿å’Œç«äº‰åŠ¨æ€",
                "å…³æ³¨ä¸»è¦ç«äº‰å¯¹æ‰‹çš„ä¸“åˆ©å¸ƒå±€ç­–ç•¥",
                "ç»“åˆå¸‚åœºéœ€æ±‚åˆ¶å®šå·®å¼‚åŒ–æŠ€æœ¯è·¯çº¿"
            ])
            
            return insights
            
        except Exception as e:
            self.logger.error(f"Error generating insights: {str(e)}")
            return {"error": str(e)}
    
    async def _improve_analysis_quality(self, analysis_results: Dict[str, Any], quality_report: Dict[str, Any]) -> Dict[str, Any]:
        """æ”¹è¿›åˆ†æè´¨é‡."""
        try:
            # æ ¹æ®è´¨é‡æŠ¥å‘Šæ”¹è¿›åˆ†æç»“æœ
            improved_results = analysis_results.copy()
            
            # å¦‚æœæ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œæ·»åŠ æ•°æ®è´¨é‡è¯´æ˜
            if quality_report.get("data_completeness", 1.0) < 0.8:
                for analysis_type in improved_results:
                    if isinstance(improved_results[analysis_type], dict):
                        improved_results[analysis_type]["data_quality_note"] = "éƒ¨åˆ†æ•°æ®å¯èƒ½ä¸å®Œæ•´ï¼Œåˆ†æç»“æœä»…ä¾›å‚è€ƒ"
            
            # å¦‚æœç½®ä¿¡åº¦è¾ƒä½ï¼Œæ·»åŠ ä¸ç¡®å®šæ€§è¯´æ˜
            if quality_report.get("confidence_level", 1.0) < 0.7:
                for analysis_type in improved_results:
                    if isinstance(improved_results[analysis_type], dict):
                        improved_results[analysis_type]["uncertainty_note"] = "åˆ†æç»“æœå­˜åœ¨ä¸€å®šä¸ç¡®å®šæ€§ï¼Œå»ºè®®ç»“åˆå…¶ä»–ä¿¡æ¯æº"
            
            return improved_results
            
        except Exception as e:
            self.logger.error(f"Error improving analysis quality: {str(e)}")
            return analysis_results
    
    async def _generate_analysis_response(self, analysis_results: Dict[str, Any], analysis_params: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æå“åº”å†…å®¹."""
        try:
            response_parts = []
            
            # æ·»åŠ åˆ†ææ¦‚è¿°
            keywords_str = "ã€".join(analysis_params.get("keywords", ["ç›¸å…³æŠ€æœ¯"]))
            time_range = analysis_params.get("time_range", {})
            start_year = time_range.get("start_year", 2014)
            end_year = time_range.get("end_year", 2024)
            
            response_parts.append(f"## ä¸“åˆ©åˆ†ææŠ¥å‘Š")
            response_parts.append(f"**åˆ†æä¸»é¢˜**: {keywords_str}")
            response_parts.append(f"**æ—¶é—´èŒƒå›´**: {start_year}å¹´-{end_year}å¹´")
            response_parts.append("")
            
            # æ·»åŠ å„é¡¹åˆ†æç»“æœ
            if "trend" in analysis_results and analysis_results["trend"].get("success"):
                trend_data = analysis_results["trend"]
                response_parts.append("### ğŸ“ˆ è¶‹åŠ¿åˆ†æ")
                response_parts.append(f"- **æ€»ä½“è¶‹åŠ¿**: {trend_data.get('trend_direction', 'ç¨³å®š')}")
                
                yearly_counts = trend_data.get("yearly_counts", {})
                if yearly_counts:
                    max_year = max(yearly_counts.keys(), key=lambda x: yearly_counts[x])
                    response_parts.append(f"- **å³°å€¼å¹´ä»½**: {max_year}å¹´ï¼ˆ{yearly_counts[max_year]}ä»¶ï¼‰")
                
                growth_rates = trend_data.get("growth_rates", {})
                if growth_rates:
                    recent_years = sorted(growth_rates.keys())[-3:]
                    avg_growth = sum(growth_rates[year] for year in recent_years) / len(recent_years)
                    response_parts.append(f"- **è¿‘æœŸå¢é•¿ç‡**: {avg_growth:.1f}%")
                
                response_parts.append("")
            
            if "competition" in analysis_results and analysis_results["competition"].get("success"):
                comp_data = analysis_results["competition"]
                response_parts.append("### ğŸ¢ ç«äº‰åˆ†æ")
                
                top_applicants = comp_data.get("top_applicants", [])
                if top_applicants:
                    response_parts.append("- **ä¸»è¦ç”³è¯·äºº**:")
                    for i, (applicant, count) in enumerate(top_applicants[:5]):
                        response_parts.append(f"  {i+1}. {applicant}: {count}ä»¶")
                
                market_concentration = comp_data.get("market_concentration", 0)
                response_parts.append(f"- **å¸‚åœºé›†ä¸­åº¦**: {market_concentration:.2f}")
                
                response_parts.append("")
            
            if "technology" in analysis_results and analysis_results["technology"].get("success"):
                tech_data = analysis_results["technology"]
                response_parts.append("### ğŸ”¬ æŠ€æœ¯åˆ†æ")
                
                main_technologies = tech_data.get("main_technologies", [])
                if main_technologies:
                    response_parts.append("- **ä¸»è¦æŠ€æœ¯é¢†åŸŸ**:")
                    for tech in main_technologies[:5]:
                        response_parts.append(f"  â€¢ {tech}")
                
                ipc_distribution = tech_data.get("ipc_distribution", {})
                if ipc_distribution:
                    top_ipc = sorted(ipc_distribution.items(), key=lambda x: x[1], reverse=True)[:3]
                    response_parts.append("- **ä¸»è¦IPCåˆ†ç±»**:")
                    for ipc, count in top_ipc:
                        response_parts.append(f"  â€¢ {ipc}: {count}ä»¶")
                
                response_parts.append("")
            
            if "geographic" in analysis_results and analysis_results["geographic"].get("success"):
                geo_data = analysis_results["geographic"]
                response_parts.append("### ğŸŒ åœ°åŸŸåˆ†æ")
                
                top_countries = geo_data.get("top_countries", [])
                if top_countries:
                    response_parts.append("- **ä¸»è¦ç”³è¯·å›½å®¶/åœ°åŒº**:")
                    for country, count in top_countries[:5]:
                        percentage = geo_data.get("country_percentages", {}).get(country, 0)
                        response_parts.append(f"  â€¢ {country}: {count}ä»¶ ({percentage:.1f}%)")
                
                response_parts.append("")
            
            # æ·»åŠ ç»¼åˆæ´å¯Ÿ
            if "insights" in analysis_results:
                insights = analysis_results["insights"]
                response_parts.append("### ğŸ’¡ å…³é”®æ´å¯Ÿ")
                
                key_findings = insights.get("key_findings", [])
                for finding in key_findings:
                    response_parts.append(f"- {finding}")
                
                if insights.get("recommendations"):
                    response_parts.append("\n### ğŸ“‹ å»ºè®®")
                    for rec in insights["recommendations"]:
                        response_parts.append(f"- {rec}")
                
                if insights.get("risk_factors"):
                    response_parts.append("\n### âš ï¸ é£é™©å› ç´ ")
                    for risk in insights["risk_factors"]:
                        response_parts.append(f"- {risk}")
            
            # æ·»åŠ æ•°æ®è´¨é‡è¯´æ˜
            response_parts.append("\n---")
            response_parts.append("*æœ¬åˆ†æåŸºäºä¸“åˆ©æ•°æ®åº“ä¿¡æ¯ï¼Œç»“æœä»…ä¾›å‚è€ƒã€‚å»ºè®®ç»“åˆå¸‚åœºè°ƒç ”ç­‰å…¶ä»–ä¿¡æ¯æºè¿›è¡Œç»¼åˆåˆ¤æ–­ã€‚*")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            self.logger.error(f"Error generating analysis response: {str(e)}")
            return f"åˆ†ææŠ¥å‘Šç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
    
    def _generate_analysis_actions(self, analysis_results: Dict[str, Any]) -> List[Action]:
        """ç”Ÿæˆåˆ†æåç»­åŠ¨ä½œ."""
        actions = []
        
        try:
            # åŸºç¡€åç»­åŠ¨ä½œ
            actions.append(Action(
                action_type="generate_detailed_report",
                parameters={"format": "pdf", "include_charts": True},
                description="ç”Ÿæˆè¯¦ç»†çš„PDFåˆ†ææŠ¥å‘Š"
            ))
            
            actions.append(Action(
                action_type="export_data",
                parameters={"format": "excel", "include_raw_data": True},
                description="å¯¼å‡ºåˆ†ææ•°æ®åˆ°Excel"
            ))
            
            # åŸºäºåˆ†æç»“æœçš„ç‰¹å®šåŠ¨ä½œ
            if "trend" in analysis_results and analysis_results["trend"].get("success"):
                actions.append(Action(
                    action_type="trend_monitoring",
                    parameters={"frequency": "monthly", "alert_threshold": 0.2},
                    description="è®¾ç½®è¶‹åŠ¿ç›‘æ§å’Œé¢„è­¦"
                ))
            
            if "competition" in analysis_results and analysis_results["competition"].get("success"):
                actions.append(Action(
                    action_type="competitor_tracking",
                    parameters={"top_competitors": 5, "update_frequency": "weekly"},
                    description="è·Ÿè¸ªä¸»è¦ç«äº‰å¯¹æ‰‹åŠ¨æ€"
                ))
            
            return actions
            
        except Exception as e:
            self.logger.error(f"Error generating analysis actions: {str(e)}")
            return []
    
    async def _log_analysis_metrics(self, analysis_params: Dict[str, Any], analysis_results: Dict[str, Any], duration: float):
        """è®°å½•åˆ†ææŒ‡æ ‡åˆ°ç›‘æ§ç³»ç»Ÿ."""
        try:
            # è¿™é‡Œåº”è¯¥é›†æˆåˆ°ç°æœ‰çš„MonitoringSystem
            metrics = {
                "analysis_duration": duration,
                "analysis_types": len(analysis_params.get("analysis_types", [])),
                "keywords_count": len(analysis_params.get("keywords", [])),
                "successful_analyses": len([r for r in analysis_results.values() if isinstance(r, dict) and r.get("success", False)]),
                "failed_analyses": len([r for r in analysis_results.values() if isinstance(r, dict) and not r.get("success", True)]),
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"Analysis metrics: {json.dumps(metrics, ensure_ascii=False)}")
            
        except Exception as e:
            self.logger.error(f"Error logging analysis metrics: {str(e)}")


class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨ï¼Œå®ç°æ—¶é—´åºåˆ—åˆ†æ."""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TrendAnalyzer")
    
    async def analyze_trends(self, patent_data: List[Dict[str, Any]], analysis_params: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æä¸“åˆ©ç”³è¯·è¶‹åŠ¿."""
        try:
            # æŒ‰å¹´ä»½ç»Ÿè®¡ç”³è¯·é‡
            yearly_counts = defaultdict(int)
            
            for patent in patent_data:
                app_date = patent.get("application_date", "")
                if app_date:
                    try:
                        year = int(app_date.split("-")[0])
                        yearly_counts[year] += 1
                    except (ValueError, IndexError):
                        continue
            
            if not yearly_counts:
                return {"error": "æ— æ³•æå–æœ‰æ•ˆçš„ç”³è¯·æ—¥æœŸä¿¡æ¯"}
            
            # è®¡ç®—å¢é•¿ç‡
            growth_rates = self._calculate_growth_rates(yearly_counts)
            
            # åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
            trend_direction = self._determine_trend_direction(growth_rates)
            
            # é¢„æµ‹æœªæ¥è¶‹åŠ¿
            future_prediction = self._predict_future_trend(yearly_counts, growth_rates)
            
            return {
                "yearly_counts": dict(yearly_counts),
                "growth_rates": growth_rates,
                "trend_direction": trend_direction,
                "future_prediction": future_prediction,
                "analysis_summary": self._generate_trend_summary(yearly_counts, growth_rates, trend_direction)
            }
            
        except Exception as e:
            self.logger.error(f"Error in trend analysis: {str(e)}")
            return {"error": str(e)}
    
    def _calculate_growth_rates(self, yearly_counts: Dict[int, int]) -> Dict[int, float]:
        """è®¡ç®—å¹´åº¦å¢é•¿ç‡."""
        growth_rates = {}
        sorted_years = sorted(yearly_counts.keys())
        
        for i in range(1, len(sorted_years)):
            prev_year = sorted_years[i-1]
            curr_year = sorted_years[i]
            
            prev_count = yearly_counts[prev_year]
            curr_count = yearly_counts[curr_year]
            
            if prev_count > 0:
                growth_rate = ((curr_count - prev_count) / prev_count) * 100
                growth_rates[curr_year] = growth_rate
            else:
                growth_rates[curr_year] = 0.0
        
        return growth_rates
    
    def _determine_trend_direction(self, growth_rates: Dict[int, float]) -> str:
        """åˆ¤æ–­è¶‹åŠ¿æ–¹å‘."""
        if not growth_rates:
            return "stable"
        
        recent_rates = list(growth_rates.values())[-3:]  # æœ€è¿‘3å¹´çš„å¢é•¿ç‡
        avg_growth = sum(recent_rates) / len(recent_rates)
        
        if avg_growth > 10:
            return "rapidly_increasing"
        elif avg_growth > 0:
            return "increasing"
        elif avg_growth > -10:
            return "stable"
        else:
            return "decreasing"
    
    def _predict_future_trend(self, yearly_counts: Dict[int, int], growth_rates: Dict[int, float]) -> Dict[str, Any]:
        """é¢„æµ‹æœªæ¥è¶‹åŠ¿."""
        try:
            if not yearly_counts or not growth_rates:
                return {"prediction": "insufficient_data"}
            
            # ç®€å•çš„çº¿æ€§é¢„æµ‹
            recent_years = sorted(yearly_counts.keys())[-3:]
            recent_counts = [yearly_counts[year] for year in recent_years]
            
            if len(recent_counts) >= 2:
                # è®¡ç®—å¹³å‡å¢é•¿
                avg_change = (recent_counts[-1] - recent_counts[0]) / (len(recent_counts) - 1)
                
                # é¢„æµ‹ä¸‹ä¸€å¹´
                next_year = max(yearly_counts.keys()) + 1
                predicted_count = max(0, int(recent_counts[-1] + avg_change))
                
                return {
                    "next_year": next_year,
                    "predicted_count": predicted_count,
                    "confidence": "medium",
                    "method": "linear_trend"
                }
            
            return {"prediction": "insufficient_data"}
            
        except Exception as e:
            self.logger.error(f"Error in trend prediction: {str(e)}")
            return {"prediction": "error", "error": str(e)}
    
    def _generate_trend_summary(self, yearly_counts: Dict[int, int], growth_rates: Dict[int, float], trend_direction: str) -> str:
        """ç”Ÿæˆè¶‹åŠ¿åˆ†ææ‘˜è¦."""
        try:
            total_patents = sum(yearly_counts.values())
            years_span = max(yearly_counts.keys()) - min(yearly_counts.keys()) + 1
            
            direction_desc = {
                "rapidly_increasing": "å¿«é€Ÿå¢é•¿",
                "increasing": "ç¨³æ­¥å¢é•¿", 
                "stable": "ç›¸å¯¹ç¨³å®š",
                "decreasing": "ä¸‹é™è¶‹åŠ¿"
            }
            
            return f"åœ¨{years_span}å¹´æœŸé—´å…±æœ‰{total_patents}ä»¶ä¸“åˆ©ç”³è¯·ï¼Œæ•´ä½“å‘ˆ{direction_desc.get(trend_direction, 'æœªçŸ¥')}æ€åŠ¿ã€‚"
            
        except Exception as e:
            return f"è¶‹åŠ¿åˆ†ææ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"


class TechClassifier:
    """æŠ€æœ¯åˆ†ç±»å™¨ï¼Œå®ç°IPCåˆ†ç±»ç»Ÿè®¡å’ŒæŠ€æœ¯èšç±»."""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.TechClassifier")
    
    async def classify_technologies(self, patent_data: List[Dict[str, Any]], analysis_params: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯åˆ†ç±»åˆ†å¸ƒ."""
        try:
            # IPCåˆ†ç±»ç»Ÿè®¡
            ipc_counts = defaultdict(int)
            
            for patent in patent_data:
                ipc_classes = patent.get("ipc_classes", [])
                for ipc in ipc_classes:
                    # æå–ä¸»åˆ†ç±»ï¼ˆå‰4ä½ï¼‰
                    main_ipc = ipc[:4] if len(ipc) >= 4 else ipc
                    ipc_counts[main_ipc] += 1
            
            # å…³é”®è¯æå–å’Œèšç±»
            keywords = self._extract_technology_keywords(patent_data)
            tech_clusters = self._cluster_technologies(keywords)
            
            # è¯†åˆ«ä¸»è¦æŠ€æœ¯é¢†åŸŸ
            main_technologies = self._identify_main_technologies(ipc_counts, tech_clusters)
            
            return {
                "ipc_distribution": dict(ipc_counts),
                "keyword_clusters": tech_clusters,
                "main_technologies": main_technologies,
                "technology_diversity": len(ipc_counts),
                "analysis_summary": self._generate_tech_summary(ipc_counts, main_technologies)
            }
            
        except Exception as e:
            self.logger.error(f"Error in technology classification: {str(e)}")
            return {"error": str(e)}
    
    def _extract_technology_keywords(self, patent_data: List[Dict[str, Any]]) -> List[str]:
        """æå–æŠ€æœ¯å…³é”®è¯."""
        keywords = []
        
        # æŠ€æœ¯ç›¸å…³çš„å…³é”®è¯æ¨¡å¼
        tech_patterns = [
            r'(äººå·¥æ™ºèƒ½|æœºå™¨å­¦ä¹ |æ·±åº¦å­¦ä¹ |ç¥ç»ç½‘ç»œ)',
            r'(åŒºå—é“¾|åˆ†å¸ƒå¼|åŠ å¯†|å“ˆå¸Œ)',
            r'(ç‰©è”ç½‘|ä¼ æ„Ÿå™¨|æ— çº¿|é€šä¿¡)',
            r'(å¤§æ•°æ®|æ•°æ®æŒ–æ˜|æ•°æ®åˆ†æ)',
            r'(äº‘è®¡ç®—|è¾¹ç¼˜è®¡ç®—|åˆ†å¸ƒå¼è®¡ç®—)',
            r'(5G|é€šä¿¡|ç½‘ç»œ|åè®®)',
            r'(èŠ¯ç‰‡|åŠå¯¼ä½“|é›†æˆç”µè·¯)',
            r'(æ–°èƒ½æº|ç”µæ± |å¤ªé˜³èƒ½|é£èƒ½)',
            r'(ç”Ÿç‰©æŠ€æœ¯|åŸºå› |è›‹ç™½è´¨|åŒ»ç–—)',
            r'(è‡ªåŠ¨é©¾é©¶|æ™ºèƒ½æ±½è½¦|å¯¼èˆª)'
        ]
        
        for patent in patent_data:
            title = patent.get("title", "")
            abstract = patent.get("abstract", "")
            text = f"{title} {abstract}"
            
            for pattern in tech_patterns:
                matches = re.findall(pattern, text)
                keywords.extend(matches)
        
        return keywords
    
    def _cluster_technologies(self, keywords: List[str]) -> List[Dict[str, Any]]:
        """æŠ€æœ¯èšç±»åˆ†æ."""
        try:
            # ç®€å•çš„å…³é”®è¯é¢‘æ¬¡èšç±»
            keyword_counts = defaultdict(int)
            for keyword in keywords:
                keyword_counts[keyword] += 1
            
            # æŒ‰é¢‘æ¬¡åˆ†ç»„
            clusters = []
            
            # é«˜é¢‘æŠ€æœ¯ï¼ˆå‡ºç°5æ¬¡ä»¥ä¸Šï¼‰
            high_freq = {k: v for k, v in keyword_counts.items() if v >= 5}
            if high_freq:
                clusters.append({
                    "cluster_name": "æ ¸å¿ƒæŠ€æœ¯",
                    "keywords": list(high_freq.keys()),
                    "frequency": sum(high_freq.values()),
                    "importance": "high"
                })
            
            # ä¸­é¢‘æŠ€æœ¯ï¼ˆå‡ºç°2-4æ¬¡ï¼‰
            mid_freq = {k: v for k, v in keyword_counts.items() if 2 <= v < 5}
            if mid_freq:
                clusters.append({
                    "cluster_name": "é‡è¦æŠ€æœ¯",
                    "keywords": list(mid_freq.keys()),
                    "frequency": sum(mid_freq.values()),
                    "importance": "medium"
                })
            
            # æ–°å…´æŠ€æœ¯ï¼ˆå‡ºç°1æ¬¡ï¼‰
            low_freq = {k: v for k, v in keyword_counts.items() if v == 1}
            if low_freq:
                clusters.append({
                    "cluster_name": "æ–°å…´æŠ€æœ¯",
                    "keywords": list(low_freq.keys())[:10],  # é™åˆ¶æ•°é‡
                    "frequency": sum(low_freq.values()),
                    "importance": "low"
                })
            
            return clusters
            
        except Exception as e:
            self.logger.error(f"Error in technology clustering: {str(e)}")
            return []
    
    def _identify_main_technologies(self, ipc_counts: Dict[str, int], tech_clusters: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«ä¸»è¦æŠ€æœ¯é¢†åŸŸ."""
        main_techs = []
        
        # åŸºäºIPCåˆ†ç±»è¯†åˆ«
        ipc_mapping = {
            "G06F": "è®¡ç®—æœºæŠ€æœ¯",
            "H04L": "é€šä¿¡æŠ€æœ¯", 
            "G06N": "äººå·¥æ™ºèƒ½",
            "H04W": "æ— çº¿é€šä¿¡",
            "G06Q": "å•†ä¸šæ–¹æ³•",
            "H01L": "åŠå¯¼ä½“æŠ€æœ¯",
            "G06K": "å›¾åƒè¯†åˆ«",
            "H04N": "å›¾åƒé€šä¿¡",
            "G06T": "å›¾åƒå¤„ç†",
            "G01S": "å®šä½æŠ€æœ¯"
        }
        
        # è·å–å‰5ä¸ªIPCåˆ†ç±»
        top_ipcs = sorted(ipc_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        for ipc, count in top_ipcs:
            tech_name = ipc_mapping.get(ipc, f"æŠ€æœ¯é¢†åŸŸ{ipc}")
            main_techs.append(tech_name)
        
        # åŸºäºæŠ€æœ¯èšç±»è¡¥å……
        for cluster in tech_clusters:
            if cluster.get("importance") == "high":
                main_techs.extend(cluster.get("keywords", [])[:2])
        
        return list(set(main_techs))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _generate_tech_summary(self, ipc_counts: Dict[str, int], main_technologies: List[str]) -> str:
        """ç”ŸæˆæŠ€æœ¯åˆ†ææ‘˜è¦."""
        try:
            total_classes = len(ipc_counts)
            main_tech_str = "ã€".join(main_technologies[:3])
            
            return f"æ¶‰åŠ{total_classes}ä¸ªä¸»è¦æŠ€æœ¯åˆ†ç±»ï¼Œä¸»è¦é›†ä¸­åœ¨{main_tech_str}ç­‰é¢†åŸŸã€‚"
            
        except Exception as e:
            return f"æŠ€æœ¯åˆ†ææ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"


class CompetitionAnalyzer:
    """ç«äº‰åˆ†æå™¨ï¼Œè¿›è¡Œç”³è¯·äººåˆ†æå’Œå¸‚åœºé›†ä¸­åº¦è®¡ç®—."""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.CompetitionAnalyzer")
    
    async def analyze_competition(self, patent_data: List[Dict[str, Any]], analysis_params: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æç«äº‰æ ¼å±€."""
        try:
            # ç”³è¯·äººç»Ÿè®¡
            applicant_counts = defaultdict(int)
            
            for patent in patent_data:
                applicants = patent.get("applicants", [])
                for applicant in applicants:
                    # æ¸…ç†ç”³è¯·äººåç§°
                    clean_name = self._clean_applicant_name(applicant)
                    applicant_counts[clean_name] += 1
            
            # æ’åºè·å–ä¸»è¦ç”³è¯·äºº
            top_applicants = sorted(
                applicant_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )[:20]
            
            # è®¡ç®—å¸‚åœºé›†ä¸­åº¦
            market_concentration = self._calculate_market_concentration(applicant_counts)
            
            # åˆ†æç«äº‰æ ¼å±€
            competition_landscape = self._analyze_competition_landscape(applicant_counts, top_applicants)
            
            return {
                "applicant_distribution": dict(applicant_counts),
                "top_applicants": top_applicants,
                "market_concentration": market_concentration,
                "competition_landscape": competition_landscape,
                "total_applicants": len(applicant_counts),
                "analysis_summary": self._generate_competition_summary(top_applicants, market_concentration)
            }
            
        except Exception as e:
            self.logger.error(f"Error in competition analysis: {str(e)}")
            return {"error": str(e)}
    
    def _clean_applicant_name(self, applicant: str) -> str:
        """æ¸…ç†ç”³è¯·äººåç§°."""
        # ç§»é™¤å¸¸è§çš„å…¬å¸åç¼€å˜ä½“
        suffixes = [
            "æœ‰é™å…¬å¸", "è‚¡ä»½æœ‰é™å…¬å¸", "ç§‘æŠ€æœ‰é™å…¬å¸", "æŠ€æœ¯æœ‰é™å…¬å¸",
            "Inc.", "LLC", "Corporation", "Corp.", "Ltd.", "Co.", "Company"
        ]
        
        clean_name = applicant.strip()
        for suffix in suffixes:
            if clean_name.endswith(suffix):
                clean_name = clean_name[:-len(suffix)].strip()
        
        return clean_name
    
    def _calculate_market_concentration(self, applicant_counts: Dict[str, int]) -> float:
        """è®¡ç®—å¸‚åœºé›†ä¸­åº¦ï¼ˆHHIæŒ‡æ•°ï¼‰."""
        try:
            total_patents = sum(applicant_counts.values())
            if total_patents == 0:
                return 0.0
            
            # è®¡ç®—HHIæŒ‡æ•°
            hhi = sum((count / total_patents) ** 2 for count in applicant_counts.values())
            
            return hhi
            
        except Exception as e:
            self.logger.error(f"Error calculating market concentration: {str(e)}")
            return 0.0
    
    def _analyze_competition_landscape(self, applicant_counts: Dict[str, int], top_applicants: List[tuple]) -> Dict[str, Any]:
        """åˆ†æç«äº‰æ ¼å±€."""
        try:
            total_patents = sum(applicant_counts.values())
            
            # è®¡ç®—å‰Nåç”³è¯·äººçš„å¸‚åœºä»½é¢
            top5_share = sum(count for _, count in top_applicants[:5]) / total_patents if total_patents > 0 else 0
            top10_share = sum(count for _, count in top_applicants[:10]) / total_patents if total_patents > 0 else 0
            
            # åˆ¤æ–­ç«äº‰æ ¼å±€ç±»å‹
            if top5_share > 0.8:
                landscape_type = "é«˜åº¦é›†ä¸­"
            elif top5_share > 0.6:
                landscape_type = "ä¸­åº¦é›†ä¸­"
            elif top5_share > 0.4:
                landscape_type = "é€‚åº¦ç«äº‰"
            else:
                landscape_type = "å……åˆ†ç«äº‰"
            
            # è¯†åˆ«ä¸»è¦ç«äº‰è€…ç±»å‹
            competitor_types = self._classify_competitors(top_applicants[:10])
            
            return {
                "landscape_type": landscape_type,
                "top5_market_share": top5_share,
                "top10_market_share": top10_share,
                "competitor_types": competitor_types,
                "competition_intensity": self._assess_competition_intensity(top5_share, len(applicant_counts))
            }
            
        except Exception as e:
            self.logger.error(f"Error analyzing competition landscape: {str(e)}")
            return {}
    
    def _classify_competitors(self, top_applicants: List[tuple]) -> Dict[str, List[str]]:
        """åˆ†ç±»ç«äº‰è€…ç±»å‹."""
        competitor_types = {
            "å¤§å‹ä¼ä¸š": [],
            "ç§‘æŠ€å…¬å¸": [],
            "ç ”ç©¶æœºæ„": [],
            "å…¶ä»–": []
        }
        
        # ç®€å•çš„åˆ†ç±»è§„åˆ™
        for applicant, count in top_applicants:
            applicant_lower = applicant.lower()
            
            if any(keyword in applicant_lower for keyword in ["å¤§å­¦", "å­¦é™¢", "ç ”ç©¶é™¢", "ç§‘å­¦é™¢", "university", "institute"]):
                competitor_types["ç ”ç©¶æœºæ„"].append(applicant)
            elif any(keyword in applicant_lower for keyword in ["ç§‘æŠ€", "æŠ€æœ¯", "technology", "tech"]):
                competitor_types["ç§‘æŠ€å…¬å¸"].append(applicant)
            elif any(keyword in applicant_lower for keyword in ["é›†å›¢", "æ§è‚¡", "group", "holdings"]):
                competitor_types["å¤§å‹ä¼ä¸š"].append(applicant)
            else:
                competitor_types["å…¶ä»–"].append(applicant)
        
        return competitor_types
    
    def _assess_competition_intensity(self, top5_share: float, total_competitors: int) -> str:
        """è¯„ä¼°ç«äº‰æ¿€çƒˆç¨‹åº¦."""
        if top5_share > 0.7 and total_competitors < 20:
            return "ä½"
        elif top5_share > 0.5 and total_competitors < 50:
            return "ä¸­ç­‰"
        elif top5_share < 0.3 and total_competitors > 100:
            return "æ¿€çƒˆ"
        else:
            return "ä¸­ç­‰"
    
    def _generate_competition_summary(self, top_applicants: List[tuple], market_concentration: float) -> str:
        """ç”Ÿæˆç«äº‰åˆ†ææ‘˜è¦."""
        try:
            if not top_applicants:
                return "ç«äº‰åˆ†ææ•°æ®ä¸è¶³"
            
            top_applicant = top_applicants[0][0]
            top_count = top_applicants[0][1]
            
            concentration_desc = "é«˜" if market_concentration > 0.5 else "ä¸­" if market_concentration > 0.2 else "ä½"
            
            return f"å¸‚åœºé›†ä¸­åº¦{concentration_desc}ï¼Œ{top_applicant}ä»¥{top_count}ä»¶ä¸“åˆ©é¢†å…ˆï¼Œå…±æœ‰{len(top_applicants)}ä¸ªä¸»è¦ç«äº‰è€…ã€‚"
            
        except Exception as e:
            return f"ç«äº‰åˆ†ææ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"


class AnalysisQualityController:
    """åˆ†æç»“æœè´¨é‡æ§åˆ¶ç³»ç»Ÿ."""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AnalysisQualityController")
    
    async def validate_analysis_results(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯åˆ†æç»“æœè´¨é‡."""
        try:
            quality_report = {
                "overall_quality": 0.0,
                "data_completeness": 0.0,
                "confidence_level": 0.0,
                "consistency_score": 0.0,
                "issues": [],
                "recommendations": []
            }
            
            # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
            completeness_score = self._check_data_completeness(analysis_results)
            quality_report["data_completeness"] = completeness_score
            
            # æ£€æŸ¥ç½®ä¿¡åº¦
            confidence_score = self._check_confidence_level(analysis_results)
            quality_report["confidence_level"] = confidence_score
            
            # æ£€æŸ¥ä¸€è‡´æ€§
            consistency_score = self._check_consistency(analysis_results)
            quality_report["consistency_score"] = consistency_score
            
            # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
            quality_report["overall_quality"] = (
                completeness_score * 0.4 +
                confidence_score * 0.3 +
                consistency_score * 0.3
            )
            
            # ç”Ÿæˆé—®é¢˜å’Œå»ºè®®
            quality_report["issues"] = self._identify_quality_issues(analysis_results, quality_report)
            quality_report["recommendations"] = self._generate_quality_recommendations(quality_report)
            
            return quality_report
            
        except Exception as e:
            self.logger.error(f"Error validating analysis results: {str(e)}")
            return {
                "overall_quality": 0.0,
                "error": str(e)
            }
    
    def _check_data_completeness(self, analysis_results: Dict[str, Any]) -> float:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§."""
        try:
            required_analyses = ["trend", "competition", "technology"]
            completed_analyses = 0
            
            for analysis_type in required_analyses:
                if analysis_type in analysis_results and analysis_results[analysis_type].get("success", False):
                    completed_analyses += 1
            
            return completed_analyses / len(required_analyses)
            
        except Exception:
            return 0.0
    
    def _check_confidence_level(self, analysis_results: Dict[str, Any]) -> float:
        """æ£€æŸ¥ç½®ä¿¡åº¦æ°´å¹³."""
        try:
            confidence_scores = []
            
            for analysis_type, result in analysis_results.items():
                if isinstance(result, dict) and result.get("success", False):
                    # åŸºäºæ•°æ®é‡å’Œç»“æœå®Œæ•´æ€§è¯„ä¼°ç½®ä¿¡åº¦
                    if analysis_type == "trend":
                        yearly_data = result.get("yearly_counts", {})
                        if len(yearly_data) >= 5:
                            confidence_scores.append(0.8)
                        elif len(yearly_data) >= 3:
                            confidence_scores.append(0.6)
                        else:
                            confidence_scores.append(0.4)
                    
                    elif analysis_type == "competition":
                        top_applicants = result.get("top_applicants", [])
                        if len(top_applicants) >= 10:
                            confidence_scores.append(0.8)
                        elif len(top_applicants) >= 5:
                            confidence_scores.append(0.6)
                        else:
                            confidence_scores.append(0.4)
                    
                    else:
                        confidence_scores.append(0.7)  # é»˜è®¤ç½®ä¿¡åº¦
            
            return sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0
            
        except Exception:
            return 0.0
    
    def _check_consistency(self, analysis_results: Dict[str, Any]) -> float:
        """æ£€æŸ¥ç»“æœä¸€è‡´æ€§."""
        try:
            # ç®€å•çš„ä¸€è‡´æ€§æ£€æŸ¥
            consistency_score = 0.8  # åŸºç¡€åˆ†æ•°
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„æ•°æ®å†²çª
            # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
            
            return consistency_score
            
        except Exception:
            return 0.0
    
    def _identify_quality_issues(self, analysis_results: Dict[str, Any], quality_report: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜."""
        issues = []
        
        if quality_report["data_completeness"] < 0.7:
            issues.append("éƒ¨åˆ†åˆ†ææ¨¡å—æ•°æ®ä¸å®Œæ•´")
        
        if quality_report["confidence_level"] < 0.6:
            issues.append("åˆ†æç»“æœç½®ä¿¡åº¦è¾ƒä½")
        
        if quality_report["consistency_score"] < 0.7:
            issues.append("åˆ†æç»“æœå­˜åœ¨ä¸€è‡´æ€§é—®é¢˜")
        
        return issues
    
    def _generate_quality_recommendations(self, quality_report: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®."""
        recommendations = []
        
        if quality_report["data_completeness"] < 0.8:
            recommendations.append("å»ºè®®å¢åŠ æ•°æ®æºæˆ–æ‰©å¤§æ•°æ®æ”¶é›†èŒƒå›´")
        
        if quality_report["confidence_level"] < 0.7:
            recommendations.append("å»ºè®®ç»“åˆå…¶ä»–åˆ†ææ–¹æ³•æé«˜ç»“æœå¯ä¿¡åº¦")
        
        if quality_report["overall_quality"] < 0.7:
            recommendations.append("å»ºè®®è¿›è¡Œäººå·¥å®¡æ ¸å’ŒéªŒè¯")
        
        return recommendations