"""Patent analysis agent."""

import logging
from typing import Any, Dict, List, Optional
from datetime import datetime
from collections import defaultdict

from .base import PatentBaseAgent
from ...models.enums import AgentType
from ...models.config import AgentConfig
from ...services.model_client import BaseModelClient

from ..models.requests import PatentAnalysisRequest
from ..models.results import PatentAnalysisResult, TrendAnalysis, TechClassification, CompetitionAnalysis
from ..models.patent_data import PatentDataset
from ..models.external_data import EnhancedData


logger = logging.getLogger(__name__)


class PatentAnalysisAgent(PatentBaseAgent):
    """ä¸“åˆ©åˆ†æå¤„ç†æ™ºèƒ½ä½“ï¼Œè´Ÿè´£ä¸“åˆ©æ•°æ®åˆ†æ."""
    
    agent_type = AgentType.PATENT_ANALYSIS
    
    def __init__(self, config: AgentConfig, model_client: BaseModelClient):
        """åˆå§‹åŒ–ä¸“åˆ©åˆ†ææ™ºèƒ½ä½“."""
        super().__init__(config, model_client)
        
        # åˆ†æä¸“ç”¨é…ç½®
        self.analysis_config = {
            'enable_trend_analysis': True,
            'enable_tech_classification': True,
            'enable_competition_analysis': True,
            'enable_geographic_analysis': True,
            'min_patents_for_analysis': 10,
            'confidence_threshold': 0.7
        }
        
        self.logger = logging.getLogger(f"{__name__}.PatentAnalysisAgent")
    
    async def _get_specific_capabilities(self) -> List[str]:
        """è·å–åˆ†ææ™ºèƒ½ä½“çš„ç‰¹å®šèƒ½åŠ›."""
        return [
            "ä¸“åˆ©è¶‹åŠ¿åˆ†æ",
            "æŠ€æœ¯åˆ†ç±»ç»Ÿè®¡",
            "ç«äº‰æ ¼å±€åˆ†æ",
            "åœ°åŸŸåˆ†å¸ƒåˆ†æ",
            "IPCåˆ†ç±»åˆ†æ",
            "ç”³è¯·äººç»Ÿè®¡åˆ†æ",
            "æ—¶é—´åºåˆ—åˆ†æ",
            "æŠ€æœ¯æ¼”è¿›é¢„æµ‹"
        ]
    
    async def _process_patent_request(self, request: PatentAnalysisRequest) -> Dict[str, Any]:
        """å¤„ç†ä¸“åˆ©åˆ†æè¯·æ±‚."""
        start_time = datetime.now()
        
        try:
            self.logger.info(f"Starting patent analysis for request {request.request_id}")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"patent_analysis_{hash(str(request.keywords))}_{hash(str(request.analysis_types))}"
            cached_result = await self._get_from_cache(cache_key)
            if cached_result:
                self.logger.info(f"Using cached analysis results for request {request.request_id}")
                return cached_result
            
            # æ¨¡æ‹Ÿè·å–ä¸“åˆ©æ•°æ®å’Œå¢å¼ºæ•°æ®ï¼ˆå®é™…å®ç°ä¸­ä¼šä»å…¶ä»–Agentè·å–ï¼‰
            patent_dataset = await self._get_patent_dataset(request)
            enhanced_data = await self._get_enhanced_data(request)
            
            if not patent_dataset or len(patent_dataset.patents) < self.analysis_config['min_patents_for_analysis']:
                raise Exception(f"Insufficient patent data for analysis (minimum {self.analysis_config['min_patents_for_analysis']} required)")
            
            # æ‰§è¡Œå„ç§åˆ†æ
            analysis_result = await self._perform_comprehensive_analysis(
                patent_dataset, enhanced_data, request
            )
            
            # ç”Ÿæˆæ´å¯Ÿå’Œå»ºè®®
            insights = await self._generate_insights(analysis_result, enhanced_data)
            recommendations = await self._generate_recommendations(analysis_result, enhanced_data)
            
            # è¯„ä¼°åˆ†æè´¨é‡
            quality_score = await self._evaluate_analysis_quality(analysis_result, patent_dataset)
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            result = {
                "status": "completed",
                "analysis_result": analysis_result,
                "insights": insights,
                "recommendations": recommendations,
                "quality_score": quality_score,
                "processing_time": processing_time,
                "total_patents_analyzed": len(patent_dataset.patents),
                "analysis_types": [at.value for at in request.analysis_types]
            }
            
            # ä¿å­˜åˆ°ç¼“å­˜
            await self._save_to_cache(cache_key, result)
            
            self.logger.info(f"Patent analysis completed for request {request.request_id}")
            return result
            
        except Exception as e:
            self.logger.error(f"Patent analysis failed for request {request.request_id}: {str(e)}")
            processing_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "status": "failed",
                "error": str(e),
                "processing_time": processing_time,
                "total_patents_analyzed": 0
            }
    
    async def _get_patent_dataset(self, request: PatentAnalysisRequest) -> Optional[PatentDataset]:
        """è·å–ä¸“åˆ©æ•°æ®é›†ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰."""
        # å®é™…å®ç°ä¸­ä¼šä»PatentDataCollectionAgentè·å–æ•°æ®
        from ..models.patent_data import PatentData
        
        # ç”Ÿæˆæ¨¡æ‹Ÿä¸“åˆ©æ•°æ®ç”¨äºåˆ†æ
        patents = []
        for i in range(100):  # æ¨¡æ‹Ÿ100ä¸ªä¸“åˆ©
            year = 2020 + (i % 5)  # 2020-2024å¹´çš„æ•°æ®
            patent = PatentData(
                application_number=f"US{year}{1000 + i}",
                title=f"Patent for {' '.join(request.keywords)} technology - {i}",
                abstract=f"This patent describes {' '.join(request.keywords)} implementation...",
                applicants=[f"Company {chr(65 + i % 10)}", f"Corp {i % 5}"][:(i % 2 + 1)],
                inventors=[f"Inventor {i % 20}"],
                application_date=datetime(year, 1 + i % 12, 1 + i % 28),
                country=["US", "CN", "JP", "DE", "KR"][i % 5],
                status=["Published", "Granted", "Pending"][i % 3],
                ipc_classes=[f"G06F{i % 20}/00", f"H04L{i % 15}/00"][:(i % 2 + 1)]
            )
            patents.append(patent)
        
        return PatentDataset(
            patents=patents,
            total_count=len(patents),
            search_keywords=request.keywords,
            collection_date=datetime.now(),
            data_sources=["google_patents", "patent_public_api"]
        )
    
    async def _get_enhanced_data(self, request: PatentAnalysisRequest) -> Optional[EnhancedData]:
        """è·å–å¢å¼ºæ•°æ®ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰."""
        # å®é™…å®ç°ä¸­ä¼šä»PatentSearchAgentè·å–æ•°æ®
        return EnhancedData(
            academic_data=None,  # ç®€åŒ–æ¨¡æ‹Ÿ
            web_intelligence=None,
            collection_date=datetime.now()
        )
    
    async def _perform_comprehensive_analysis(
        self, 
        patent_dataset: PatentDataset, 
        enhanced_data: Optional[EnhancedData],
        request: PatentAnalysisRequest
    ) -> PatentAnalysisResult:
        """æ‰§è¡Œç»¼åˆä¸“åˆ©åˆ†æ."""
        
        analysis_result = PatentAnalysisResult(
            request_id=request.request_id,
            analysis_date=datetime.now(),
            total_patents_analyzed=len(patent_dataset.patents),
            data_sources_used=patent_dataset.data_sources
        )
        
        # è¶‹åŠ¿åˆ†æ
        if any(at.value == "trend_analysis" or at.value == "comprehensive" for at in request.analysis_types):
            analysis_result.trend_analysis = await self._analyze_trends(patent_dataset)
        
        # æŠ€æœ¯åˆ†ç±»åˆ†æ
        if any(at.value == "tech_classification" or at.value == "comprehensive" for at in request.analysis_types):
            analysis_result.tech_classification = await self._classify_technologies(patent_dataset)
        
        # ç«äº‰åˆ†æ
        if any(at.value == "competition_analysis" or at.value == "comprehensive" for at in request.analysis_types):
            analysis_result.competition_analysis = await self._analyze_competition(patent_dataset)
        
        # åœ°åŸŸåˆ†æ
        if any(at.value == "geographic_analysis" or at.value == "comprehensive" for at in request.analysis_types):
            from ..models.results import GeographicAnalysisModel
            analysis_result.geographic_analysis = await self._analyze_geography(patent_dataset)
        
        return analysis_result
    
    async def _analyze_trends(self, patent_dataset: PatentDataset) -> TrendAnalysis:
        """æ‰§è¡Œè¶‹åŠ¿åˆ†æ."""
        from ..models.results import TrendAnalysisModel
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡ä¸“åˆ©ç”³è¯·é‡
        yearly_counts = defaultdict(int)
        for patent in patent_dataset.patents:
            if patent.application_date:
                year = patent.application_date.year
                yearly_counts[year] += 1
        
        # è®¡ç®—å¢é•¿ç‡
        growth_rates = {}
        years = sorted(yearly_counts.keys())
        for i in range(1, len(years)):
            prev_year = years[i-1]
            curr_year = years[i]
            if yearly_counts[prev_year] > 0:
                growth_rate = (yearly_counts[curr_year] - yearly_counts[prev_year]) / yearly_counts[prev_year]
                growth_rates[curr_year] = growth_rate
        
        # ç¡®å®šè¶‹åŠ¿æ–¹å‘
        if len(growth_rates) > 0:
            avg_growth = sum(growth_rates.values()) / len(growth_rates)
            if avg_growth > 0.1:
                trend_direction = "increasing"
            elif avg_growth < -0.1:
                trend_direction = "decreasing"
            else:
                trend_direction = "stable"
        else:
            trend_direction = "stable"
        
        # æ‰¾åˆ°å³°å€¼å¹´ä»½
        peak_year = max(yearly_counts.keys(), key=lambda y: yearly_counts[y]) if yearly_counts else None
        
        return TrendAnalysisModel(
            yearly_counts=dict(yearly_counts),
            growth_rates=growth_rates,
            trend_direction=trend_direction,
            peak_year=peak_year,
            total_patents=sum(yearly_counts.values()),
            average_annual_growth=sum(growth_rates.values()) / len(growth_rates) if growth_rates else 0.0
        )
    
    async def _classify_technologies(self, patent_dataset: PatentDataset) -> TechClassification:
        """æ‰§è¡ŒæŠ€æœ¯åˆ†ç±»åˆ†æ."""
        from ..models.results import TechClassificationModel
        
        # IPCåˆ†ç±»ç»Ÿè®¡
        ipc_counts = defaultdict(int)
        for patent in patent_dataset.patents:
            for ipc in patent.ipc_classes or []:
                # æå–ä¸»åˆ†ç±»ï¼ˆå‰4ä½ï¼‰
                main_ipc = ipc[:4] if len(ipc) >= 4 else ipc
                ipc_counts[main_ipc] += 1
        
        # å…³é”®è¯èšç±»ï¼ˆç®€åŒ–å®ç°ï¼‰
        keyword_clusters = []
        main_keywords = set()
        for patent in patent_dataset.patents:
            # ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯
            title_words = patent.title.lower().split()
            for word in title_words:
                if len(word) > 4:  # è¿‡æ»¤çŸ­è¯
                    main_keywords.add(word)
        
        # åˆ›å»ºå…³é”®è¯èšç±»
        keyword_list = list(main_keywords)[:10]  # å–å‰10ä¸ªå…³é”®è¯
        for i, keyword in enumerate(keyword_list):
            cluster = {
                "cluster_id": i,
                "main_keyword": keyword,
                "related_keywords": keyword_list[max(0, i-2):i+3],
                "patent_count": sum(1 for p in patent_dataset.patents if keyword in p.title.lower())
            }
            keyword_clusters.append(cluster)
        
        # è¯†åˆ«ä¸»è¦æŠ€æœ¯
        main_technologies = []
        sorted_ipc = sorted(ipc_counts.items(), key=lambda x: x[1], reverse=True)
        for ipc, count in sorted_ipc[:5]:  # å–å‰5ä¸ªIPCåˆ†ç±»
            main_technologies.append(f"{ipc} ({count} patents)")
        
        return TechClassificationModel(
            ipc_distribution=dict(ipc_counts),
            keyword_clusters=keyword_clusters,
            main_technologies=main_technologies
        )
    
    async def _analyze_competition(self, patent_dataset: PatentDataset) -> CompetitionAnalysis:
        """æ‰§è¡Œç«äº‰åˆ†æ."""
        from ..models.results import CompetitionAnalysisModel
        
        # ç”³è¯·äººç»Ÿè®¡
        applicant_counts = defaultdict(int)
        for patent in patent_dataset.patents:
            for applicant in patent.applicants or []:
                applicant_counts[applicant] += 1
        
        # æ’åºè·å–é¡¶çº§ç”³è¯·äºº
        top_applicants = sorted(applicant_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # è®¡ç®—å¸‚åœºé›†ä¸­åº¦ï¼ˆHHIæŒ‡æ•°ï¼‰
        total_patents = sum(applicant_counts.values())
        if total_patents > 0:
            hhi_index = sum((count / total_patents) ** 2 for count in applicant_counts.values())
            market_concentration = hhi_index
        else:
            hhi_index = 0.0
            market_concentration = 0.0
        
        return CompetitionAnalysisModel(
            applicant_distribution=dict(applicant_counts),
            top_applicants=top_applicants,
            market_concentration=market_concentration,
            hhi_index=hhi_index
        )
    
    async def _analyze_geography(self, patent_dataset: PatentDataset) -> Dict[str, Any]:
        """æ‰§è¡Œåœ°åŸŸåˆ†æ."""
        # å›½å®¶åˆ†å¸ƒç»Ÿè®¡
        country_counts = defaultdict(int)
        for patent in patent_dataset.patents:
            if patent.country:
                country_counts[patent.country] += 1
        
        # æ’åºè·å–é¡¶çº§å›½å®¶
        top_countries = sorted(country_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # è®¡ç®—å…¨çƒåŒ–æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        total_patents = sum(country_counts.values())
        if total_patents > 0 and len(country_counts) > 1:
            # åŸºäºå›½å®¶æ•°é‡å’Œåˆ†å¸ƒå‡åŒ€åº¦è®¡ç®—
            globalization_index = len(country_counts) / 10  # å‡è®¾æœ€å¤š10ä¸ªä¸»è¦å›½å®¶
            globalization_index = min(globalization_index, 1.0)
        else:
            globalization_index = 0.0
        
        return {
            "country_distribution": dict(country_counts),
            "top_countries": top_countries,
            "globalization_index": globalization_index,
            "regional_trends": {}  # ç®€åŒ–å®ç°
        }
    
    async def _generate_insights(self, analysis_result: PatentAnalysisResult, enhanced_data: Optional[EnhancedData]) -> List[str]:
        """ç”Ÿæˆåˆ†ææ´å¯Ÿ."""
        insights = []
        
        # è¶‹åŠ¿æ´å¯Ÿ
        if analysis_result.trend_analysis:
            trend = analysis_result.trend_analysis
            if trend.trend_direction == "increasing":
                insights.append(f"ä¸“åˆ©ç”³è¯·å‘ˆä¸Šå‡è¶‹åŠ¿ï¼Œå¹³å‡å¹´å¢é•¿ç‡ä¸º {trend.average_annual_growth:.1%}")
            elif trend.trend_direction == "decreasing":
                insights.append(f"ä¸“åˆ©ç”³è¯·å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œéœ€è¦å…³æ³¨æŠ€æœ¯å‘å±•æ”¾ç¼“")
            else:
                insights.append("ä¸“åˆ©ç”³è¯·é‡ä¿æŒç›¸å¯¹ç¨³å®š")
            
            if trend.peak_year:
                insights.append(f"{trend.peak_year}å¹´æ˜¯ä¸“åˆ©ç”³è¯·çš„å³°å€¼å¹´ä»½")
        
        # æŠ€æœ¯æ´å¯Ÿ
        if analysis_result.tech_classification:
            tech = analysis_result.tech_classification
            if tech.main_technologies:
                main_tech = tech.main_technologies[0].split('(')[0].strip()
                insights.append(f"ä¸»è¦æŠ€æœ¯é¢†åŸŸé›†ä¸­åœ¨ {main_tech}")
        
        # ç«äº‰æ´å¯Ÿ
        if analysis_result.competition_analysis:
            comp = analysis_result.competition_analysis
            if comp.top_applicants:
                top_applicant = comp.top_applicants[0][0]
                insights.append(f"{top_applicant} æ˜¯è¯¥é¢†åŸŸçš„ä¸»è¦ä¸“åˆ©ç”³è¯·äºº")
            
            if comp.market_concentration > 0.5:
                insights.append("å¸‚åœºé›†ä¸­åº¦è¾ƒé«˜ï¼Œå­˜åœ¨æ˜æ˜¾çš„æŠ€æœ¯é¢†å¯¼è€…")
            else:
                insights.append("å¸‚åœºç«äº‰ç›¸å¯¹åˆ†æ•£ï¼ŒæŠ€æœ¯å‘å±•å¤šå…ƒåŒ–")
        
        return insights
    
    async def _generate_recommendations(self, analysis_result: PatentAnalysisResult, enhanced_data: Optional[EnhancedData]) -> List[str]:
        """ç”Ÿæˆå»ºè®®."""
        recommendations = []
        
        # åŸºäºè¶‹åŠ¿çš„å»ºè®®
        if analysis_result.trend_analysis:
            if analysis_result.trend_analysis.trend_direction == "increasing":
                recommendations.append("å»ºè®®åŠ å¤§ç ”å‘æŠ•å…¥ï¼ŒæŠ“ä½æŠ€æœ¯å‘å±•æœºé‡")
            elif analysis_result.trend_analysis.trend_direction == "decreasing":
                recommendations.append("å»ºè®®å…³æ³¨æ–°å…´æŠ€æœ¯æ–¹å‘ï¼Œå¯»æ‰¾çªç ´ç‚¹")
        
        # åŸºäºç«äº‰çš„å»ºè®®
        if analysis_result.competition_analysis:
            if analysis_result.competition_analysis.market_concentration > 0.6:
                recommendations.append("å¸‚åœºé›†ä¸­åº¦é«˜ï¼Œå»ºè®®å¯»æ‰¾ç»†åˆ†é¢†åŸŸæœºä¼š")
            else:
                recommendations.append("å¸‚åœºç«äº‰æ¿€çƒˆï¼Œå»ºè®®åŠ å¼ºæŠ€æœ¯å·®å¼‚åŒ–")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºè®®æŒç»­ç›‘æ§ä¸“åˆ©åŠ¨æ€ï¼ŒåŠæ—¶è°ƒæ•´æŠ€æœ¯ç­–ç•¥",
            "è€ƒè™‘å»ºç«‹ä¸“åˆ©é¢„è­¦æœºåˆ¶ï¼Œé˜²èŒƒä¾µæƒé£é™©",
            "åŠ å¼ºå›½é™…ä¸“åˆ©å¸ƒå±€ï¼Œæå‡å…¨çƒç«äº‰åŠ›"
        ])
        
        return recommendations
    
    async def _evaluate_analysis_quality(self, analysis_result: PatentAnalysisResult, patent_dataset: PatentDataset) -> float:
        """è¯„ä¼°åˆ†æè´¨é‡."""
        quality_factors = []
        
        # æ•°æ®å®Œæ•´æ€§
        complete_patents = sum(1 for p in patent_dataset.patents 
                             if p.application_number and p.title and p.applicants)
        data_completeness = complete_patents / len(patent_dataset.patents)
        quality_factors.append(data_completeness * 0.3)
        
        # åˆ†æè¦†ç›–åº¦
        analysis_coverage = 0
        if analysis_result.trend_analysis:
            analysis_coverage += 0.25
        if analysis_result.tech_classification:
            analysis_coverage += 0.25
        if analysis_result.competition_analysis:
            analysis_coverage += 0.25
        if analysis_result.geographic_analysis:
            analysis_coverage += 0.25
        quality_factors.append(analysis_coverage * 0.4)
        
        # æ•°æ®é‡å……è¶³æ€§
        data_sufficiency = min(len(patent_dataset.patents) / 100, 1.0)  # 100ä¸ªä¸“åˆ©ä¸ºæ»¡åˆ†
        quality_factors.append(data_sufficiency * 0.3)
        
        return sum(quality_factors)
    
    async def _generate_response_content(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†æå“åº”å†…å®¹."""
        if result.get("status") == "completed":
            total_patents = result.get("total_patents_analyzed", 0)
            quality_score = result.get("quality_score", 0.0)
            processing_time = result.get("processing_time", 0.0)
            insights = result.get("insights", [])
            recommendations = result.get("recommendations", [])
            
            content = f"""ä¸“åˆ©åˆ†æå·²å®Œæˆï¼

ğŸ“Š åˆ†ææ¦‚å†µ:
â€¢ åˆ†æä¸“åˆ©æ•°é‡: {total_patents}
â€¢ åˆ†æè´¨é‡è¯„åˆ†: {quality_score:.2f}/1.0
â€¢ å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’

ğŸ” å…³é”®æ´å¯Ÿ:
"""
            for i, insight in enumerate(insights[:3], 1):
                content += f"{i}. {insight}\n"
            
            content += f"""
ğŸ’¡ å»ºè®®:
"""
            for i, rec in enumerate(recommendations[:3], 1):
                content += f"{i}. {rec}\n"
            
            content += "\nè¯¦ç»†åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆï¼Œå¯è¿›è¡Œè¿›ä¸€æ­¥çš„æŠ¥å‘Šç”Ÿæˆå’Œå¯è§†åŒ–ã€‚"
            
            return content
        
        elif result.get("status") == "failed":
            error = result.get("error", "æœªçŸ¥é”™è¯¯")
            return f"ä¸“åˆ©åˆ†æå¤±è´¥: {error}"
        
        else:
            return f"ä¸“åˆ©åˆ†æçŠ¶æ€: {result.get('status', 'unknown')}"