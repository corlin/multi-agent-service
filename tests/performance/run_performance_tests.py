#!/usr/bin/env python3
"""
ä¸“åˆ©ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æ‰§è¡Œè„šæœ¬

ä½¿ç”¨uv runæ‰§è¡Œå®Œæ•´çš„ä¸“åˆ©ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…æ‹¬ï¼š
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- å¹¶å‘å‹åŠ›æµ‹è¯•  
- å¤§æ•°æ®é‡å¤„ç†æµ‹è¯•
- Agentè´Ÿè½½æµ‹è¯•
- èµ„æºç›‘æ§æµ‹è¯•
"""

import asyncio
import sys
import os
import argparse
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from tests.performance.benchmark_runner import BenchmarkRunner, BenchmarkConfig


class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = output_dir or "tests/performance/results"
        self.results_dir = Path(self.output_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_dir = self.results_dir / f"session_{self.timestamp}"
        self.session_dir.mkdir(exist_ok=True)
        
    async def run_full_performance_suite(self, config_file: str = None, test_filter: List[str] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        
        print(f"{'='*80}")
        print("ä¸“åˆ©ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å¥—ä»¶")
        print(f"{'='*80}")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ç»“æœç›®å½•: {self.session_dir}")
        print(f"{'='*80}")
        
        # åŠ è½½æµ‹è¯•é…ç½®
        configs = self._load_test_configs(config_file)
        
        # åº”ç”¨æµ‹è¯•è¿‡æ»¤å™¨
        if test_filter:
            configs = [cfg for cfg in configs if cfg.test_name in test_filter]
            print(f"åº”ç”¨æµ‹è¯•è¿‡æ»¤å™¨ï¼Œè¿è¡Œ {len(configs)} ä¸ªæµ‹è¯•")
        
        # ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥
        await self._check_system_requirements()
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_runner = BenchmarkRunner(str(self.session_dir))
        benchmark_report = await benchmark_runner.run_all_benchmarks(configs)
        
        # è¿è¡Œä¸“é¡¹æ€§èƒ½æµ‹è¯•
        specialized_results = await self._run_specialized_tests()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = self._generate_comprehensive_report(
            benchmark_report, 
            specialized_results
        )
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        await self._save_comprehensive_report(comprehensive_report)
        
        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        self._print_test_summary(comprehensive_report)
        
        return comprehensive_report
    
    def _load_test_configs(self, config_file: str = None) -> List[BenchmarkConfig]:
        """åŠ è½½æµ‹è¯•é…ç½®"""
        if config_file and Path(config_file).exists():
            config_path = Path(config_file)
        else:
            config_path = Path("tests/performance/config/benchmark_config.json")
        
        if config_path.exists():
            print(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            return [BenchmarkConfig(**cfg) for cfg in config_data]
        else:
            print("ä½¿ç”¨é»˜è®¤é…ç½®")
            from tests.performance.benchmark_runner import create_default_benchmark_configs
            return create_default_benchmark_configs()
    
    async def _check_system_requirements(self):
        """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
        import psutil
        
        print("\nç³»ç»Ÿç¯å¢ƒæ£€æŸ¥:")
        print(f"  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()}")
        print(f"  å†…å­˜æ€»é‡: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.2f} GB")
        print(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.2f} GB")
        print(f"  Pythonç‰ˆæœ¬: {sys.version}")
        print(f"  å¹³å°: {sys.platform}")
        
        # æ£€æŸ¥æœ€ä½è¦æ±‚
        if psutil.virtual_memory().available < 2 * 1024 * 1024 * 1024:  # 2GB
            print("âš ï¸  è­¦å‘Š: å¯ç”¨å†…å­˜ä¸è¶³2GBï¼Œå¯èƒ½å½±å“æµ‹è¯•ç»“æœ")
        
        if psutil.cpu_count() < 2:
            print("âš ï¸  è­¦å‘Š: CPUæ ¸å¿ƒæ•°å°‘äº2ä¸ªï¼Œå¹¶å‘æµ‹è¯•å¯èƒ½å—é™")
        
        print("âœ… ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥å®Œæˆ\n")
    
    async def _run_specialized_tests(self) -> Dict[str, Any]:
        """è¿è¡Œä¸“é¡¹æ€§èƒ½æµ‹è¯•"""
        print(f"\n{'='*60}")
        print("è¿è¡Œä¸“é¡¹æ€§èƒ½æµ‹è¯•")
        print(f"{'='*60}")
        
        specialized_results = {}
        
        # 1. å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•
        print("\nğŸ” è¿è¡Œå†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•...")
        try:
            memory_leak_result = await self._run_memory_leak_test()
            specialized_results['memory_leak_test'] = memory_leak_result
            print("âœ… å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•å®Œæˆ")
        except Exception as e:
            print(f"âŒ å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
            specialized_results['memory_leak_test'] = {'error': str(e)}
        
        # 2. æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•
        print("\nğŸ“Š è¿è¡Œæ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•...")
        try:
            regression_result = await self._run_regression_test()
            specialized_results['regression_test'] = regression_result
            print("âœ… æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•å®Œæˆ")
        except Exception as e:
            print(f"âŒ æ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•å¤±è´¥: {e}")
            specialized_results['regression_test'] = {'error': str(e)}
        
        # 3. æé™å‹åŠ›æµ‹è¯•
        print("\nğŸš€ è¿è¡Œæé™å‹åŠ›æµ‹è¯•...")
        try:
            extreme_stress_result = await self._run_extreme_stress_test()
            specialized_results['extreme_stress_test'] = extreme_stress_result
            print("âœ… æé™å‹åŠ›æµ‹è¯•å®Œæˆ")
        except Exception as e:
            print(f"âŒ æé™å‹åŠ›æµ‹è¯•å¤±è´¥: {e}")
            specialized_results['extreme_stress_test'] = {'error': str(e)}
        
        return specialized_results
    
    async def _run_memory_leak_test(self) -> Dict[str, Any]:
        """è¿è¡Œå†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•"""
        from unittest.mock import AsyncMock, patch
        import psutil
        import gc
        
        with patch('src.multi_agent_service.patent.agents.coordinator.PatentCoordinatorAgent') as mock_coordinator:
            mock_coordinator.return_value.process_request = AsyncMock(return_value={
                'status': 'success',
                'analysis_id': 'memory-leak-test',
                'results': {'patents_found': 50}
            })
            
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            memory_samples = [initial_memory]
            
            # æ‰§è¡Œå¤šè½®å†…å­˜å¯†é›†å‹æ“ä½œ
            for cycle in range(8):
                from src.multi_agent_service.patent.agents.coordinator import PatentCoordinatorAgent
                
                # åˆ›å»ºå¤šä¸ªAgentå®ä¾‹
                agents = []
                for i in range(15):
                    agent_config = {'agent_id': f'memory-leak-{cycle}-{i}', 'model_config': {}}
                    agent = PatentCoordinatorAgent(agent_config)
                    agents.append(agent)
                
                # å¹¶å‘æ‰§è¡Œä»»åŠ¡
                tasks = []
                for i, agent in enumerate(agents):
                    request = {
                        'keywords': [f'memory-leak-{cycle}-{i}', 'test'],
                        'limit': 80
                    }
                    tasks.append(agent.process_request(request))
                
                await asyncio.gather(*tasks, return_exceptions=True)
                
                # æ¸…ç†å¼•ç”¨
                del agents
                del tasks
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                gc.collect()
                
                # è®°å½•å†…å­˜ä½¿ç”¨
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_samples.append(current_memory)
                
                # çŸ­æš‚ç­‰å¾…
                await asyncio.sleep(0.5)
            
            # åˆ†æå†…å­˜ä½¿ç”¨è¶‹åŠ¿
            final_memory = memory_samples[-1]
            memory_increase = final_memory - initial_memory
            
            # è®¡ç®—å†…å­˜å¢é•¿è¶‹åŠ¿
            n = len(memory_samples)
            x_sum = sum(range(n))
            y_sum = sum(memory_samples)
            xy_sum = sum(i * memory_samples[i] for i in range(n))
            x2_sum = sum(i * i for i in range(n))
            
            slope = (n * xy_sum - x_sum * y_sum) / (n * x2_sum - x_sum * x_sum)
            
            return {
                'initial_memory_mb': initial_memory,
                'final_memory_mb': final_memory,
                'memory_increase_mb': memory_increase,
                'memory_growth_trend': slope,
                'memory_samples': memory_samples,
                'cycles_tested': len(memory_samples) - 1,
                'leak_detected': memory_increase > 100 or slope > 3.0,
                'max_memory_mb': max(memory_samples),
                'avg_memory_mb': sum(memory_samples) / len(memory_samples)
            }
    
    async def _run_regression_test(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½å›å½’æ£€æµ‹æµ‹è¯•"""
        from tests.performance.test_patent_stress import StressTestRunner, StressTestConfig
        from unittest.mock import AsyncMock, patch
        
        with patch('src.multi_agent_service.patent.agents.coordinator.PatentCoordinatorAgent') as mock_coordinator:
            mock_coordinator.return_value.process_request = AsyncMock(return_value={
                'status': 'success',
                'analysis_id': 'regression-test',
                'results': {'patents_found': 75}
            })
            
            # åŸºå‡†æµ‹è¯•é…ç½®
            baseline_config = StressTestConfig(
                concurrent_users=8,
                requests_per_user=6,
                ramp_up_time=5,
                test_duration=30,
                max_response_time=8.0,
                min_success_rate=0.9
            )
            
            async def regression_test_function(user_id: int, request_id: int):
                from src.multi_agent_service.patent.agents.coordinator import PatentCoordinatorAgent
                from src.multi_agent_service.patent.models.requests import PatentAnalysisRequest
                
                agent_config = {'agent_id': f'regression-{user_id}', 'model_config': {}}
                coordinator = PatentCoordinatorAgent(agent_config)
                
                request = PatentAnalysisRequest(
                    keywords=[f'regression-{user_id}-{request_id}'],
                    analysis_type='basic',
                    limit=60
                )
                
                return await coordinator.process_request(request.dict())
            
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            baseline_runner = StressTestRunner(baseline_config)
            baseline_result = await baseline_runner.run_stress_test(regression_test_function)
            
            # è¿è¡Œå½“å‰æµ‹è¯•
            current_runner = StressTestRunner(baseline_config)
            current_result = await current_runner.run_stress_test(regression_test_function)
            
            # è®¡ç®—å›å½’æŒ‡æ ‡
            response_time_regression = (
                (current_result.avg_response_time - baseline_result.avg_response_time) 
                / baseline_result.avg_response_time
            ) if baseline_result.avg_response_time > 0 else 0
            
            success_rate_regression = baseline_result.success_rate - current_result.success_rate
            
            throughput_regression = (
                (baseline_result.requests_per_second - current_result.requests_per_second) 
                / baseline_result.requests_per_second
            ) if baseline_result.requests_per_second > 0 else 0
            
            return {
                'baseline_avg_response_time': baseline_result.avg_response_time,
                'current_avg_response_time': current_result.avg_response_time,
                'response_time_regression_pct': response_time_regression * 100,
                'baseline_success_rate': baseline_result.success_rate,
                'current_success_rate': current_result.success_rate,
                'success_rate_regression_pct': success_rate_regression * 100,
                'baseline_throughput': baseline_result.requests_per_second,
                'current_throughput': current_result.requests_per_second,
                'throughput_regression_pct': throughput_regression * 100,
                'regression_detected': (
                    abs(response_time_regression) > 0.15 or 
                    success_rate_regression > 0.05 or 
                    throughput_regression > 0.15
                )
            }
    
    async def _run_extreme_stress_test(self) -> Dict[str, Any]:
        """è¿è¡Œæé™å‹åŠ›æµ‹è¯•"""
        from tests.performance.test_patent_stress import StressTestRunner, StressTestConfig
        from unittest.mock import AsyncMock, patch
        import random
        
        with patch('src.multi_agent_service.patent.agents.coordinator.PatentCoordinatorAgent') as mock_coordinator:
            # æ¨¡æ‹Ÿæ›´çœŸå®çš„å¤„ç†å»¶è¿Ÿå’Œå¶å‘é”™è¯¯
            async def extreme_mock_process(*args, **kwargs):
                # éšæœºå»¶è¿Ÿ
                await asyncio.sleep(random.uniform(0.2, 3.0))
                
                # éšæœºå¤±è´¥
                if random.random() < 0.08:  # 8%å¤±è´¥ç‡
                    raise Exception("Extreme stress test simulated error")
                
                return {
                    'status': 'success',
                    'analysis_id': f'extreme-{random.randint(1000, 9999)}',
                    'results': {'patents_found': random.randint(30, 150)}
                }
            
            mock_coordinator.return_value.process_request = extreme_mock_process
            
            # æé™å‹åŠ›æµ‹è¯•é…ç½®
            extreme_config = StressTestConfig(
                concurrent_users=25,  # é«˜å¹¶å‘
                requests_per_user=8,
                ramp_up_time=15,
                test_duration=90,  # è¾ƒé•¿æ—¶é—´
                max_response_time=20.0,
                min_success_rate=0.75  # é™ä½æˆåŠŸç‡è¦æ±‚
            )
            
            async def extreme_test_function(user_id: int, request_id: int):
                from src.multi_agent_service.patent.agents.coordinator import PatentCoordinatorAgent
                from src.multi_agent_service.patent.models.requests import PatentAnalysisRequest
                
                agent_config = {'agent_id': f'extreme-{user_id}', 'model_config': {}}
                coordinator = PatentCoordinatorAgent(agent_config)
                
                # éšæœºå¤æ‚åº¦è¯·æ±‚
                complexity = random.choice(['basic', 'standard', 'comprehensive'])
                limit = {'basic': 40, 'standard': 80, 'comprehensive': 150}[complexity]
                
                request = PatentAnalysisRequest(
                    keywords=[f'extreme-{complexity}-{user_id}-{request_id}'],
                    analysis_type=complexity,
                    limit=limit
                )
                
                return await coordinator.process_request(request.dict())
            
            # è¿è¡Œæé™å‹åŠ›æµ‹è¯•
            extreme_runner = StressTestRunner(extreme_config)
            extreme_result = await extreme_runner.run_stress_test(extreme_test_function)
            
            return {
                'total_requests': extreme_result.total_requests,
                'successful_requests': extreme_result.successful_requests,
                'failed_requests': extreme_result.failed_requests,
                'success_rate': extreme_result.success_rate,
                'avg_response_time': extreme_result.avg_response_time,
                'max_response_time': extreme_result.max_response_time,
                'requests_per_second': extreme_result.requests_per_second,
                'peak_memory_usage': extreme_result.peak_memory_usage,
                'peak_cpu_usage': extreme_result.peak_cpu_usage,
                'test_duration': extreme_result.test_duration,
                'error_count': len(extreme_result.errors),
                'system_survived': extreme_result.success_rate > 0.5 and extreme_result.requests_per_second > 1.0
            }
    
    def _generate_comprehensive_report(self, benchmark_report: Dict[str, Any], specialized_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        return {
            'test_session': {
                'timestamp': self.timestamp,
                'session_dir': str(self.session_dir),
                'start_time': benchmark_report['summary']['test_start_time'],
                'end_time': benchmark_report['summary']['test_end_time'],
                'total_duration': benchmark_report['summary']['total_duration']
            },
            'benchmark_results': benchmark_report,
            'specialized_results': specialized_results,
            'overall_assessment': self._assess_overall_performance(benchmark_report, specialized_results),
            'recommendations': self._generate_recommendations(benchmark_report, specialized_results)
        }
    
    def _assess_overall_performance(self, benchmark_report: Dict[str, Any], specialized_results: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æ•´ä½“æ€§èƒ½"""
        assessment = {
            'overall_grade': 'A',  # A, B, C, D, F
            'performance_score': 0,  # 0-100
            'critical_issues': [],
            'warnings': [],
            'strengths': []
        }
        
        # åŸºå‡†æµ‹è¯•è¯„ä¼°
        benchmark_success_rate = benchmark_report['summary']['success_rate']
        if benchmark_success_rate >= 0.95:
            assessment['strengths'].append("åŸºå‡†æµ‹è¯•æˆåŠŸç‡ä¼˜ç§€")
            assessment['performance_score'] += 25
        elif benchmark_success_rate >= 0.85:
            assessment['performance_score'] += 20
        elif benchmark_success_rate >= 0.75:
            assessment['warnings'].append("åŸºå‡†æµ‹è¯•æˆåŠŸç‡åä½")
            assessment['performance_score'] += 15
        else:
            assessment['critical_issues'].append("åŸºå‡†æµ‹è¯•æˆåŠŸç‡è¿‡ä½")
            assessment['performance_score'] += 5
        
        # èšåˆæŒ‡æ ‡è¯„ä¼°
        if 'aggregate_metrics' in benchmark_report:
            metrics = benchmark_report['aggregate_metrics']
            
            # å“åº”æ—¶é—´è¯„ä¼°
            if 'avg_response_time' in metrics:
                avg_rt = metrics['avg_response_time']
                if avg_rt <= 5.0:
                    assessment['strengths'].append("å¹³å‡å“åº”æ—¶é—´ä¼˜ç§€")
                    assessment['performance_score'] += 25
                elif avg_rt <= 10.0:
                    assessment['performance_score'] += 20
                elif avg_rt <= 20.0:
                    assessment['warnings'].append("å¹³å‡å“åº”æ—¶é—´åé«˜")
                    assessment['performance_score'] += 15
                else:
                    assessment['critical_issues'].append("å¹³å‡å“åº”æ—¶é—´è¿‡é«˜")
                    assessment['performance_score'] += 5
            
            # ååé‡è¯„ä¼°
            if 'avg_throughput' in metrics:
                throughput = metrics['avg_throughput']
                if throughput >= 5.0:
                    assessment['strengths'].append("ç³»ç»Ÿååé‡ä¼˜ç§€")
                    assessment['performance_score'] += 25
                elif throughput >= 2.0:
                    assessment['performance_score'] += 20
                elif throughput >= 1.0:
                    assessment['warnings'].append("ç³»ç»Ÿååé‡åä½")
                    assessment['performance_score'] += 15
                else:
                    assessment['critical_issues'].append("ç³»ç»Ÿååé‡è¿‡ä½")
                    assessment['performance_score'] += 5
        
        # ä¸“é¡¹æµ‹è¯•è¯„ä¼°
        if 'memory_leak_test' in specialized_results:
            memory_result = specialized_results['memory_leak_test']
            if not memory_result.get('error'):
                if not memory_result.get('leak_detected', False):
                    assessment['strengths'].append("æœªæ£€æµ‹åˆ°å†…å­˜æ³„æ¼")
                    assessment['performance_score'] += 25
                else:
                    assessment['critical_issues'].append("æ£€æµ‹åˆ°æ½œåœ¨å†…å­˜æ³„æ¼")
        
        # ç¡®å®šæ€»ä½“ç­‰çº§
        if assessment['performance_score'] >= 90:
            assessment['overall_grade'] = 'A'
        elif assessment['performance_score'] >= 80:
            assessment['overall_grade'] = 'B'
        elif assessment['performance_score'] >= 70:
            assessment['overall_grade'] = 'C'
        elif assessment['performance_score'] >= 60:
            assessment['overall_grade'] = 'D'
        else:
            assessment['overall_grade'] = 'F'
        
        return assessment
    
    def _generate_recommendations(self, benchmark_report: Dict[str, Any], specialized_results: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºåŸºå‡†æµ‹è¯•ç»“æœçš„å»ºè®®
        if benchmark_report['summary']['success_rate'] < 0.9:
            recommendations.append("å»ºè®®ä¼˜åŒ–é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œæé«˜ç³»ç»Ÿç¨³å®šæ€§")
        
        if 'aggregate_metrics' in benchmark_report:
            metrics = benchmark_report['aggregate_metrics']
            
            if metrics.get('avg_response_time', 0) > 10.0:
                recommendations.append("å»ºè®®ä¼˜åŒ–Agentå¤„ç†é€»è¾‘ï¼Œå‡å°‘å“åº”æ—¶é—´")
                recommendations.append("è€ƒè™‘å¢åŠ ç¼“å­˜æœºåˆ¶ä»¥æé«˜å“åº”é€Ÿåº¦")
            
            if metrics.get('avg_throughput', 0) < 2.0:
                recommendations.append("å»ºè®®å¢åŠ å¹¶å‘å¤„ç†èƒ½åŠ›ï¼Œæé«˜ç³»ç»Ÿååé‡")
                recommendations.append("è€ƒè™‘ä½¿ç”¨è¿æ¥æ± å’Œå¼‚æ­¥å¤„ç†ä¼˜åŒ–")
        
        # åŸºäºä¸“é¡¹æµ‹è¯•ç»“æœçš„å»ºè®®
        if 'memory_leak_test' in specialized_results:
            memory_result = specialized_results['memory_leak_test']
            if not memory_result.get('error') and memory_result.get('leak_detected'):
                recommendations.append("æ£€æµ‹åˆ°å†…å­˜æ³„æ¼ï¼Œå»ºè®®æ£€æŸ¥Agentç”Ÿå‘½å‘¨æœŸç®¡ç†")
                recommendations.append("å»ºè®®æ·»åŠ å®šæœŸåƒåœ¾å›æ”¶æœºåˆ¶")
        
        if 'extreme_stress_test' in specialized_results:
            extreme_result = specialized_results['extreme_stress_test']
            if not extreme_result.get('error') and not extreme_result.get('system_survived', True):
                recommendations.append("ç³»ç»Ÿåœ¨æé™å‹åŠ›ä¸‹è¡¨ç°ä¸ä½³ï¼Œå»ºè®®å¢å¼ºå®¹é”™èƒ½åŠ›")
                recommendations.append("è€ƒè™‘å®ç°ç†”æ–­å™¨å’Œé™æµæœºåˆ¶")
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å»ºè®®å®šæœŸè¿è¡Œæ€§èƒ½æµ‹è¯•ä»¥ç›‘æ§ç³»ç»Ÿæ€§èƒ½è¶‹åŠ¿",
            "è€ƒè™‘å»ºç«‹æ€§èƒ½ç›‘æ§å‘Šè­¦æœºåˆ¶",
            "å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²APMå·¥å…·è¿›è¡Œå®æ—¶ç›‘æ§"
        ])
        
        return recommendations
    
    async def _save_comprehensive_report(self, report: Dict[str, Any]):
        """ä¿å­˜ç»¼åˆæŠ¥å‘Š"""
        # ä¿å­˜JSONæ ¼å¼
        json_file = self.session_dir / "comprehensive_performance_report.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜HTMLæ ¼å¼
        html_file = self.session_dir / "comprehensive_performance_report.html"
        html_content = self._generate_html_comprehensive_report(report)
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"\nğŸ“‹ ç»¼åˆæ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   JSON: {json_file}")
        print(f"   HTML: {html_file}")
    
    def _generate_html_comprehensive_report(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„ç»¼åˆæŠ¥å‘Š"""
        assessment = report['overall_assessment']
        
        grade_colors = {
            'A': '#28a745', 'B': '#6f42c1', 'C': '#fd7e14', 'D': '#dc3545', 'F': '#6c757d'
        }
        
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ä¸“åˆ©ç³»ç»Ÿç»¼åˆæ€§èƒ½æŠ¥å‘Š</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f8f9fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        .header {{ text-align: center; margin-bottom: 40px; }}
        .grade-badge {{ display: inline-block; padding: 15px 30px; border-radius: 50px; color: white; font-size: 24px; font-weight: bold; margin: 20px 0; }}
        .section {{ margin: 30px 0; padding: 20px; border: 1px solid #dee2e6; border-radius: 8px; }}
        .section h2 {{ color: #495057; border-bottom: 2px solid #007bff; padding-bottom: 10px; }}
        .metrics-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ background-color: #f8f9fa; padding: 15px; border-radius: 8px; text-align: center; }}
        .metric-value {{ font-size: 24px; font-weight: bold; color: #007bff; }}
        .metric-label {{ color: #6c757d; margin-top: 5px; }}
        .issue-list {{ list-style: none; padding: 0; }}
        .issue-item {{ padding: 10px; margin: 5px 0; border-radius: 5px; }}
        .critical {{ background-color: #f8d7da; color: #721c24; }}
        .warning {{ background-color: #fff3cd; color: #856404; }}
        .strength {{ background-color: #d4edda; color: #155724; }}
        .recommendation {{ background-color: #e2e3e5; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #007bff; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ border: 1px solid #dee2e6; padding: 12px; text-align: left; }}
        th {{ background-color: #e9ecef; font-weight: bold; }}
        .success {{ color: #28a745; }}
        .failure {{ color: #dc3545; }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ä¸“åˆ©ç³»ç»Ÿç»¼åˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
            <p>æµ‹è¯•æ—¶é—´: {report['test_session']['start_time']} - {report['test_session']['end_time']}</p>
            <div class="grade-badge" style="background-color: {grade_colors.get(assessment['overall_grade'], '#6c757d')}">
                æ€»ä½“è¯„çº§: {assessment['overall_grade']}
            </div>
            <p>æ€§èƒ½å¾—åˆ†: {assessment['performance_score']}/100</p>
        </div>
        
        <div class="section">
            <h2>ğŸ“Š æµ‹è¯•æ¦‚è§ˆ</h2>
            <div class="metrics-grid">
                <div class="metric-card">
                    <div class="metric-value">{report['benchmark_results']['summary']['total_tests']}</div>
                    <div class="metric-label">æ€»æµ‹è¯•æ•°</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{report['benchmark_results']['summary']['success_rate']:.1%}</div>
                    <div class="metric-label">æˆåŠŸç‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{report['benchmark_results']['summary']['total_duration']:.1f}s</div>
                    <div class="metric-label">æ€»è€—æ—¶</div>
                </div>
"""
        
        if 'aggregate_metrics' in report['benchmark_results']:
            metrics = report['benchmark_results']['aggregate_metrics']
            if 'avg_response_time' in metrics:
                html += f"""
                <div class="metric-card">
                    <div class="metric-value">{metrics['avg_response_time']:.2f}s</div>
                    <div class="metric-label">å¹³å‡å“åº”æ—¶é—´</div>
                </div>
"""
            if 'avg_throughput' in metrics:
                html += f"""
                <div class="metric-card">
                    <div class="metric-value">{metrics['avg_throughput']:.2f}</div>
                    <div class="metric-label">å¹³å‡ååé‡ (RPS)</div>
                </div>
"""
        
        html += """
            </div>
        </div>
        
        <div class="section">
            <h2>ğŸ¯ æ€§èƒ½è¯„ä¼°</h2>
"""
        
        if assessment['critical_issues']:
            html += """
            <h3>ğŸš¨ ä¸¥é‡é—®é¢˜</h3>
            <ul class="issue-list">
"""
            for issue in assessment['critical_issues']:
                html += f'<li class="issue-item critical">âŒ {issue}</li>'
            html += "</ul>"
        
        if assessment['warnings']:
            html += """
            <h3>âš ï¸ è­¦å‘Š</h3>
            <ul class="issue-list">
"""
            for warning in assessment['warnings']:
                html += f'<li class="issue-item warning">âš ï¸ {warning}</li>'
            html += "</ul>"
        
        if assessment['strengths']:
            html += """
            <h3>âœ… ä¼˜åŠ¿</h3>
            <ul class="issue-list">
"""
            for strength in assessment['strengths']:
                html += f'<li class="issue-item strength">âœ… {strength}</li>'
            html += "</ul>"
        
        html += """
        </div>
        
        <div class="section">
            <h2>ğŸ”§ ä¼˜åŒ–å»ºè®®</h2>
"""
        
        for i, recommendation in enumerate(report['recommendations'], 1):
            html += f'<div class="recommendation">{i}. {recommendation}</div>'
        
        html += """
        </div>
        
        <div class="section">
            <h2>ğŸ“ˆ åŸºå‡†æµ‹è¯•è¯¦æƒ…</h2>
            <table>
                <thead>
                    <tr>
                        <th>æµ‹è¯•åç§°</th>
                        <th>çŠ¶æ€</th>
                        <th>è€—æ—¶(s)</th>
                        <th>æˆåŠŸç‡</th>
                        <th>å¹³å‡å“åº”æ—¶é—´(s)</th>
                    </tr>
                </thead>
                <tbody>
"""
        
        for result in report['benchmark_results']['test_results']:
            status_class = 'success' if result['success'] else 'failure'
            status_text = 'âœ… é€šè¿‡' if result['success'] else 'âŒ å¤±è´¥'
            
            success_rate = result['metrics'].get('success_rate', 0)
            avg_response_time = result['metrics'].get('avg_response_time', 0)
            
            html += f"""
                    <tr>
                        <td>{result['test_name']}</td>
                        <td class="{status_class}">{status_text}</td>
                        <td>{result['duration']:.2f}</td>
                        <td>{success_rate:.1%}</td>
                        <td>{avg_response_time:.3f}</td>
                    </tr>
"""
        
        html += """
                </tbody>
            </table>
        </div>
"""
        
        # ä¸“é¡¹æµ‹è¯•ç»“æœ
        if report['specialized_results']:
            html += """
        <div class="section">
            <h2>ğŸ”¬ ä¸“é¡¹æµ‹è¯•ç»“æœ</h2>
"""
            
            for test_name, result in report['specialized_results'].items():
                if 'error' not in result:
                    html += f"<h3>{test_name}</h3>"
                    html += "<div class='metrics-grid'>"
                    
                    # æ ¹æ®æµ‹è¯•ç±»å‹æ˜¾ç¤ºä¸åŒæŒ‡æ ‡
                    if test_name == 'memory_leak_test':
                        html += f"""
                        <div class="metric-card">
                            <div class="metric-value">{'âŒ' if result.get('leak_detected') else 'âœ…'}</div>
                            <div class="metric-label">å†…å­˜æ³„æ¼æ£€æµ‹</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{result.get('memory_increase_mb', 0):.2f}MB</div>
                            <div class="metric-label">å†…å­˜å¢é•¿</div>
                        </div>
"""
                    elif test_name == 'extreme_stress_test':
                        html += f"""
                        <div class="metric-card">
                            <div class="metric-value">{result.get('success_rate', 0):.1%}</div>
                            <div class="metric-label">æé™å‹åŠ›æˆåŠŸç‡</div>
                        </div>
                        <div class="metric-card">
                            <div class="metric-value">{result.get('requests_per_second', 0):.2f}</div>
                            <div class="metric-label">æé™å‹åŠ›ååé‡</div>
                        </div>
"""
                    
                    html += "</div>"
        
        html += """
        </div>
        
        <div class="section">
            <h2>â„¹ï¸ ç³»ç»Ÿä¿¡æ¯</h2>
            <div class="metrics-grid">
"""
        
        system_info = report['benchmark_results']['system_info']
        html += f"""
                <div class="metric-card">
                    <div class="metric-value">{system_info['cpu_count']}</div>
                    <div class="metric-label">CPUæ ¸å¿ƒæ•°</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{system_info['memory_total_gb']:.1f}GB</div>
                    <div class="metric-label">å†…å­˜æ€»é‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{system_info['python_version']}</div>
                    <div class="metric-label">Pythonç‰ˆæœ¬</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{system_info['platform']}</div>
                    <div class="metric-label">å¹³å°</div>
                </div>
"""
        
        html += """
            </div>
        </div>
        
        <div style="text-align: center; margin-top: 40px; color: #6c757d;">
            <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: """ + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + """</p>
        </div>
    </div>
</body>
</html>
"""
        return html
    
    def _print_test_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        assessment = report['overall_assessment']
        
        print(f"\n{'='*80}")
        print("ğŸ¯ ä¸“åˆ©ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æ€»ç»“")
        print(f"{'='*80}")
        print(f"æ€»ä½“è¯„çº§: {assessment['overall_grade']}")
        print(f"æ€§èƒ½å¾—åˆ†: {assessment['performance_score']}/100")
        print(f"æµ‹è¯•æˆåŠŸç‡: {report['benchmark_results']['summary']['success_rate']:.1%}")
        print(f"æ€»æµ‹è¯•æ—¶é—´: {report['benchmark_results']['summary']['total_duration']:.1f} ç§’")
        
        if 'aggregate_metrics' in report['benchmark_results']:
            metrics = report['benchmark_results']['aggregate_metrics']
            if 'avg_response_time' in metrics:
                print(f"å¹³å‡å“åº”æ—¶é—´: {metrics['avg_response_time']:.3f} ç§’")
            if 'avg_throughput' in metrics:
                print(f"å¹³å‡ååé‡: {metrics['avg_throughput']:.2f} RPS")
        
        if assessment['critical_issues']:
            print(f"\nğŸš¨ ä¸¥é‡é—®é¢˜ ({len(assessment['critical_issues'])}ä¸ª):")
            for issue in assessment['critical_issues']:
                print(f"   âŒ {issue}")
        
        if assessment['warnings']:
            print(f"\nâš ï¸  è­¦å‘Š ({len(assessment['warnings'])}ä¸ª):")
            for warning in assessment['warnings']:
                print(f"   âš ï¸  {warning}")
        
        if assessment['strengths']:
            print(f"\nâœ… ä¼˜åŠ¿ ({len(assessment['strengths'])}ä¸ª):")
            for strength in assessment['strengths']:
                print(f"   âœ… {strength}")
        
        print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {self.session_dir}")
        print(f"{'='*80}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸“åˆ©ç³»ç»Ÿæ€§èƒ½æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--config", help="æµ‹è¯•é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--tests", nargs="+", help="æŒ‡å®šè¦è¿è¡Œçš„æµ‹è¯•åç§°")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå‡å°‘æµ‹è¯•æ•°é‡ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = PerformanceTestSuite(args.output)
    
    # å¦‚æœæ˜¯å¿«é€Ÿæ¨¡å¼ï¼Œè¿‡æ»¤æµ‹è¯•
    test_filter = args.tests
    if args.quick and not test_filter:
        test_filter = [
            "single_agent_baseline",
            "coordinator_agent_baseline", 
            "concurrent_analysis_light",
            "memory_usage_monitoring"
        ]
    
    try:
        # è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
        report = await suite.run_full_performance_suite(args.config, test_filter)
        
        # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
        if report['overall_assessment']['overall_grade'] in ['A', 'B']:
            sys.exit(0)  # æˆåŠŸ
        elif report['overall_assessment']['overall_grade'] in ['C', 'D']:
            sys.exit(1)  # è­¦å‘Š
        else:
            sys.exit(2)  # å¤±è´¥
            
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())