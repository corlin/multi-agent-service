#!/usr/bin/env python3
"""Google Patents Browser-Use æœ€ç»ˆæ¼”ç¤º."""

import asyncio
import logging
import json
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def get_chinese_keyword_mapping():
    """è·å–ä¸­æ–‡å…³é”®è¯åˆ°è‹±æ–‡å…³é”®è¯çš„æ˜ å°„."""
    # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
    try:
        config_file = "chinese_keywords_config.json"
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("å…³é”®è¯æ˜ å°„", {})
    except Exception as e:
        logger.warning(f"æ— æ³•åŠ è½½å…³é”®è¯é…ç½®æ–‡ä»¶: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    
    # é»˜è®¤æ˜ å°„ï¼ˆå¤‡ç”¨ï¼‰
    return {
        "å…·èº«æ™ºèƒ½": ["embodied intelligence", "embodied AI", "physical AI", "robotics intelligence"],
        "å¤§è¯­è¨€æ¨¡å‹": ["large language model", "LLM", "transformer", "GPT", "BERT", "language model"],
        "å®¢æˆ·ç»†åˆ†": ["customer segmentation", "user profiling", "market segmentation", "customer analytics"],
        "å¤šæ¨¡æ€": ["multimodal", "multi-modal", "cross-modal", "vision language"],
        "æ¨èç³»ç»Ÿ": ["recommendation system", "collaborative filtering", "personalization"],
        "è®¡ç®—æœºè§†è§‰": ["computer vision", "image recognition", "object detection", "visual AI"],
        "è‡ªç„¶è¯­è¨€å¤„ç†": ["natural language processing", "NLP", "text analysis", "language understanding"],
        "æ·±åº¦å­¦ä¹ ": ["deep learning", "neural network", "artificial neural network"],
        "æœºå™¨å­¦ä¹ ": ["machine learning", "ML", "supervised learning", "unsupervised learning"],
        "äººå·¥æ™ºèƒ½": ["artificial intelligence", "AI", "intelligent system"],
        "çŸ¥è¯†å›¾è°±": ["knowledge graph", "knowledge base", "semantic network"],
        "å¼ºåŒ–å­¦ä¹ ": ["reinforcement learning", "RL", "Q-learning", "policy gradient"],
        "è”é‚¦å­¦ä¹ ": ["federated learning", "distributed learning", "privacy-preserving learning"],
        "è¾¹ç¼˜è®¡ç®—": ["edge computing", "edge AI", "mobile computing", "distributed computing"],
        "åŒºå—é“¾": ["blockchain", "distributed ledger", "cryptocurrency", "smart contract"],
        "ç‰©è”ç½‘": ["Internet of Things", "IoT", "connected devices", "smart devices"],
        "äº‘è®¡ç®—": ["cloud computing", "distributed computing", "virtualization"],
        "æ•°æ®æŒ–æ˜": ["data mining", "data analytics", "pattern recognition", "knowledge discovery"],
        "å›¾åƒå¤„ç†": ["image processing", "digital image processing", "image analysis"],
        "è¯­éŸ³è¯†åˆ«": ["speech recognition", "voice recognition", "automatic speech recognition", "ASR"],
        "æƒ…æ„Ÿåˆ†æ": ["sentiment analysis", "emotion recognition", "affective computing"],
        "é¢„æµ‹åˆ†æ": ["predictive analytics", "forecasting", "predictive modeling"],
        "å¼‚å¸¸æ£€æµ‹": ["anomaly detection", "outlier detection", "fraud detection"],
        "èšç±»åˆ†æ": ["clustering", "cluster analysis", "unsupervised classification"],
        "åˆ†ç±»ç®—æ³•": ["classification", "supervised learning", "pattern classification"],
        "å›å½’åˆ†æ": ["regression analysis", "linear regression", "predictive regression"],
        "æ—¶é—´åºåˆ—": ["time series", "temporal analysis", "sequential data"],
        "ä¼˜åŒ–ç®—æ³•": ["optimization algorithm", "mathematical optimization", "algorithmic optimization"],
        "æœç´¢ç®—æ³•": ["search algorithm", "information retrieval", "search engine"],
        "æ’åºç®—æ³•": ["sorting algorithm", "ranking algorithm", "ordering algorithm"],
        "åŠ å¯†æŠ€æœ¯": ["encryption", "cryptography", "data security", "cybersecurity"],
        "éšç§ä¿æŠ¤": ["privacy protection", "data privacy", "privacy-preserving", "differential privacy"]
    }


def expand_keywords_with_chinese(keywords, chinese_mapping=None):
    """æ‰©å±•å…³é”®è¯åˆ—è¡¨ï¼Œæ”¯æŒä¸­æ–‡å…³é”®è¯æ˜ å°„."""
    if chinese_mapping is None:
        chinese_mapping = get_chinese_keyword_mapping()
    
    expanded_keywords = []
    
    for keyword in keywords:
        # æ·»åŠ åŸå§‹å…³é”®è¯
        expanded_keywords.append(keyword)
        
        # å¦‚æœæ˜¯ä¸­æ–‡å…³é”®è¯ï¼Œæ·»åŠ å¯¹åº”çš„è‹±æ–‡å…³é”®è¯
        if keyword in chinese_mapping:
            expanded_keywords.extend(chinese_mapping[keyword])
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å…³é”®è¯çš„éƒ¨åˆ†åŒ¹é…
        for chinese_key, english_keywords in chinese_mapping.items():
            if chinese_key in keyword or keyword in chinese_key:
                expanded_keywords.extend(english_keywords)
    
    # å»é‡å¹¶è¿”å›
    return list(set(expanded_keywords))


async def demo_google_patents_complete():
    """å®Œæ•´çš„ Google Patents åŠŸèƒ½æ¼”ç¤º."""
    try:
        from multi_agent_service.patent.services.google_patents_browser import GooglePatentsBrowserService
        
        logger.info("=== Google Patents Browser-Use å®Œæ•´åŠŸèƒ½æ¼”ç¤º ===")
        logger.info(f"æ¼”ç¤ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        service = GooglePatentsBrowserService(headless=True, timeout=60)
        
        try:
            # åˆå§‹åŒ–æœåŠ¡
            logger.info("\n1. åˆå§‹åŒ– Google Patents æœåŠ¡...")
            await service.initialize()
            logger.info("âœ“ æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            
            # æ¼”ç¤ºä¸åŒç±»å‹çš„æœç´¢ - ä¼˜åŒ–ä¸ºä¸­æ–‡å…³é”®è¯
            demo_searches = [
                {
                    "name": "å…·èº«æ™ºèƒ½ä¸“åˆ©æœç´¢",
                    "keywords": ["embodied intelligence", "embodied AI", "physical AI", "robotics intelligence"],
                    "limit": 5,
                    "description": "æœç´¢å…·èº«æ™ºèƒ½ç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬æœºå™¨äººæ™ºèƒ½ã€ç‰©ç†AIç­‰"
                },
                {
                    "name": "å¤§è¯­è¨€æ¨¡å‹ä¸“åˆ©æœç´¢", 
                    "keywords": ["large language model", "LLM", "transformer", "GPT", "BERT"],
                    "limit": 5,
                    "description": "æœç´¢å¤§è¯­è¨€æ¨¡å‹ç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬Transformerã€GPTã€BERTç­‰"
                },
                {
                    "name": "å®¢æˆ·ç»†åˆ†ä¸“åˆ©æœç´¢",
                    "keywords": ["customer segmentation", "user profiling", "market segmentation", "customer analytics"],
                    "limit": 4,
                    "description": "æœç´¢å®¢æˆ·ç»†åˆ†ç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬ç”¨æˆ·ç”»åƒã€å¸‚åœºç»†åˆ†ç­‰"
                },
                {
                    "name": "å¤šæ¨¡æ€AIä¸“åˆ©æœç´¢",
                    "keywords": ["multimodal AI", "vision language model", "cross-modal", "multi-modal learning"],
                    "limit": 4,
                    "description": "æœç´¢å¤šæ¨¡æ€AIç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬è§†è§‰è¯­è¨€æ¨¡å‹ã€è·¨æ¨¡æ€å­¦ä¹ ç­‰"
                },
                {
                    "name": "æ™ºèƒ½æ¨èç³»ç»Ÿä¸“åˆ©æœç´¢",
                    "keywords": ["recommendation system", "collaborative filtering", "content-based filtering", "personalization"],
                    "limit": 3,
                    "description": "æœç´¢æ™ºèƒ½æ¨èç³»ç»Ÿç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬ååŒè¿‡æ»¤ã€ä¸ªæ€§åŒ–æ¨èç­‰"
                },
                {
                    "name": "è®¡ç®—æœºè§†è§‰ä¸“åˆ©æœç´¢",
                    "keywords": ["computer vision", "image recognition", "object detection", "deep learning vision"],
                    "limit": 3,
                    "description": "æœç´¢è®¡ç®—æœºè§†è§‰ç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç›®æ ‡æ£€æµ‹ç­‰"
                },
                {
                    "name": "è‡ªç„¶è¯­è¨€å¤„ç†ä¸“åˆ©æœç´¢",
                    "keywords": ["natural language processing", "NLP", "text analysis", "language understanding"],
                    "limit": 3,
                    "description": "æœç´¢è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³ä¸“åˆ©ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†æã€è¯­è¨€ç†è§£ç­‰"
                },
                {
                    "name": "é«˜çº§æœç´¢æ¼”ç¤º - è¿‘æœŸAIä¸“åˆ©",
                    "keywords": ["artificial intelligence", "machine learning", "deep learning"],
                    "limit": 3,
                    "date_range": {"start_year": "2023", "end_year": "2024"},
                    "assignee": "Google",
                    "description": "æœç´¢Googleè¿‘æœŸAIç›¸å…³ä¸“åˆ©ï¼Œå¸¦æ—¥æœŸèŒƒå›´å’Œç”³è¯·äººè¿‡æ»¤"
                }
            ]
            
            all_results = {}
            
            for i, search in enumerate(demo_searches, 1):
                logger.info(f"\n{i}. {search['name']}")
                logger.info(f"   æè¿°: {search['description']}")
                logger.info(f"   å…³é”®è¯: {search['keywords']}")
                if search.get('date_range'):
                    logger.info(f"   æ—¥æœŸèŒƒå›´: {search['date_range']}")
                if search.get('assignee'):
                    logger.info(f"   ç”³è¯·äºº: {search['assignee']}")
                
                # æ‰§è¡Œæœç´¢
                patents = await service.search_patents(
                    keywords=search["keywords"],
                    limit=search["limit"],
                    date_range=search.get("date_range"),
                    assignee=search.get("assignee")
                )
                
                all_results[search["name"]] = {
                    "search_params": search,
                    "results": patents,
                    "count": len(patents)
                }
                
                logger.info(f"   æœç´¢ç»“æœ: æ‰¾åˆ° {len(patents)} ä¸ªä¸“åˆ©")
                
                if patents:
                    for j, patent in enumerate(patents, 1):
                        logger.info(f"\n   ä¸“åˆ© {j}:")
                        logger.info(f"     æ ‡é¢˜: {patent.get('title', 'N/A')}")
                        logger.info(f"     ä¸“åˆ©å·: {patent.get('patent_number', 'N/A')}")
                        logger.info(f"     ç”³è¯·äºº: {', '.join(patent.get('applicants', []))}")
                        logger.info(f"     å‘å¸ƒæ—¥æœŸ: {patent.get('publication_date', 'N/A')}")
                        logger.info(f"     æ¥æº: {patent.get('source', 'N/A')}")
                        logger.info(f"     URL: {patent.get('url', 'N/A')}")
                        if patent.get('abstract'):
                            logger.info(f"     æ‘˜è¦: {patent['abstract'][:100]}...")
                else:
                    logger.warning(f"   æœªæ‰¾åˆ°ç›¸å…³ä¸“åˆ©")
                
                # æœç´¢é—´éš”
                await asyncio.sleep(1)
            
            # æ¼”ç¤ºæ•°æ®è½¬æ¢åŠŸèƒ½
            logger.info(f"\n{len(demo_searches) + 1}. æ•°æ®è½¬æ¢åŠŸèƒ½æ¼”ç¤º")
            
            # æ”¶é›†æ‰€æœ‰ä¸“åˆ©è¿›è¡Œè½¬æ¢
            all_patents = []
            for result in all_results.values():
                all_patents.extend(result["results"])
            
            if all_patents:
                # è½¬æ¢ä¸º PatentRecord æ ¼å¼
                patent_records = service.convert_to_patent_records(all_patents)
                
                logger.info(f"   è½¬æ¢äº† {len(patent_records)} ä¸ªä¸“åˆ©è®°å½•")
                
                if patent_records:
                    logger.info(f"\n   è½¬æ¢åçš„ä¸“åˆ©è®°å½•ç¤ºä¾‹:")
                    record = patent_records[0]
                    logger.info(f"     ä¸“åˆ©ID: {record.patent_id}")
                    logger.info(f"     æ ‡é¢˜: {record.patent_title}")
                    logger.info(f"     ç”³è¯·äºº: {record.assignee_organization}")
                    logger.info(f"     å‘æ˜äºº: {record.inventor_name_first} {record.inventor_name_last}")
                    logger.info(f"     æ—¥æœŸ: {record.patent_date}")
                    logger.info(f"     IPCåˆ†ç±»: {record.ipc_class}")
                    logger.info(f"     CPCåˆ†ç±»: {record.cpc_class}")
            
            # ä¿å­˜æ¼”ç¤ºç»“æœ
            result_file = f"google_patents_demo_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            logger.info(f"\næ¼”ç¤ºç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            
            # ç»Ÿè®¡æ€»ç»“
            total_searches = len(demo_searches)
            successful_searches = sum(1 for result in all_results.values() if result['count'] > 0)
            total_patents = sum(result['count'] for result in all_results.values())
            
            logger.info(f"\n=== æ¼”ç¤ºç»Ÿè®¡æ€»ç»“ ===")
            logger.info(f"æ€»æœç´¢æ¬¡æ•°: {total_searches}")
            logger.info(f"æˆåŠŸæœç´¢: {successful_searches}")
            logger.info(f"æ€»ä¸“åˆ©æ•°: {total_patents}")
            logger.info(f"æˆåŠŸç‡: {successful_searches/total_searches*100:.1f}%")
            logger.info(f"å¹³å‡æ¯æ¬¡æœç´¢ä¸“åˆ©æ•°: {total_patents/total_searches:.1f}")
            
            return successful_searches > 0
            
        finally:
            # å…³é—­æœåŠ¡
            await service.close()
            logger.info("\nGoogle Patents æœåŠ¡å·²å…³é—­")
            
    except Exception as e:
        logger.error(f"æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False


async def demo_chinese_keywords():
    """æ¼”ç¤ºä¸­æ–‡å…³é”®è¯æœç´¢åŠŸèƒ½."""
    logger.info("\n=== ä¸­æ–‡å…³é”®è¯ä¸“åˆ©æœç´¢æ¼”ç¤º ===")
    
    try:
        from multi_agent_service.patent.services.google_patents_browser import GooglePatentsBrowserService
        
        # ä¸­æ–‡å…³é”®è¯æœç´¢æ¼”ç¤º
        chinese_searches = [
            {
                "name": "å…·èº«æ™ºèƒ½æŠ€æœ¯ä¸“åˆ©",
                "chinese_keywords": ["å…·èº«æ™ºèƒ½"],
                "limit": 3,
                "description": "æœç´¢å…·èº«æ™ºèƒ½ç›¸å…³çš„æœ€æ–°ä¸“åˆ©æŠ€æœ¯"
            },
            {
                "name": "å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ä¸“åˆ©",
                "chinese_keywords": ["å¤§è¯­è¨€æ¨¡å‹"],
                "limit": 3,
                "description": "æœç´¢å¤§è¯­è¨€æ¨¡å‹åœ¨å„é¢†åŸŸçš„åº”ç”¨ä¸“åˆ©"
            },
            {
                "name": "å®¢æˆ·ç»†åˆ†ç®—æ³•ä¸“åˆ©",
                "chinese_keywords": ["å®¢æˆ·ç»†åˆ†"],
                "limit": 3,
                "description": "æœç´¢å®¢æˆ·ç»†åˆ†å’Œç”¨æˆ·ç”»åƒç›¸å…³ä¸“åˆ©"
            },
            {
                "name": "å¤šæ¨¡æ€AIä¸“åˆ©",
                "chinese_keywords": ["å¤šæ¨¡æ€"],
                "limit": 2,
                "description": "æœç´¢å¤šæ¨¡æ€äººå·¥æ™ºèƒ½ç›¸å…³ä¸“åˆ©"
            },
            {
                "name": "è”é‚¦å­¦ä¹ ä¸“åˆ©",
                "chinese_keywords": ["è”é‚¦å­¦ä¹ "],
                "limit": 2,
                "description": "æœç´¢è”é‚¦å­¦ä¹ å’Œéšç§ä¿æŠ¤æœºå™¨å­¦ä¹ ä¸“åˆ©"
            }
        ]
        
        service = GooglePatentsBrowserService(headless=True, timeout=60)
        await service.initialize()
        
        try:
            chinese_mapping = get_chinese_keyword_mapping()
            all_chinese_results = {}
            
            for i, search in enumerate(chinese_searches, 1):
                logger.info(f"\n{i}. {search['name']}")
                logger.info(f"   ä¸­æ–‡å…³é”®è¯: {search['chinese_keywords']}")
                
                # æ‰©å±•ä¸­æ–‡å…³é”®è¯ä¸ºè‹±æ–‡å…³é”®è¯
                expanded_keywords = expand_keywords_with_chinese(
                    search['chinese_keywords'], 
                    chinese_mapping
                )
                
                logger.info(f"   æ‰©å±•åçš„è‹±æ–‡å…³é”®è¯: {expanded_keywords[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"   æè¿°: {search['description']}")
                
                # æ‰§è¡Œæœç´¢
                patents = await service.search_patents(
                    keywords=expanded_keywords,
                    limit=search["limit"]
                )
                
                all_chinese_results[search["name"]] = {
                    "chinese_keywords": search['chinese_keywords'],
                    "expanded_keywords": expanded_keywords,
                    "results": patents,
                    "count": len(patents)
                }
                
                logger.info(f"   æœç´¢ç»“æœ: æ‰¾åˆ° {len(patents)} ä¸ªä¸“åˆ©")
                
                if patents:
                    for j, patent in enumerate(patents, 1):
                        logger.info(f"\n   ä¸“åˆ© {j}:")
                        logger.info(f"     æ ‡é¢˜: {patent.get('title', 'N/A')}")
                        logger.info(f"     ä¸“åˆ©å·: {patent.get('patent_number', 'N/A')}")
                        logger.info(f"     ç”³è¯·äºº: {', '.join(patent.get('applicants', []))}")
                        logger.info(f"     å‘å¸ƒæ—¥æœŸ: {patent.get('publication_date', 'N/A')}")
                        if patent.get('abstract'):
                            logger.info(f"     æ‘˜è¦: {patent['abstract'][:150]}...")
                else:
                    logger.warning(f"   æœªæ‰¾åˆ°ç›¸å…³ä¸“åˆ©")
                
                # æœç´¢é—´éš”
                await asyncio.sleep(2)
            
            # ä¿å­˜ä¸­æ–‡å…³é”®è¯æœç´¢ç»“æœ
            chinese_result_file = f"chinese_keywords_patents_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(chinese_result_file, 'w', encoding='utf-8') as f:
                json.dump(all_chinese_results, f, ensure_ascii=False, indent=2)
            logger.info(f"\nä¸­æ–‡å…³é”®è¯æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {chinese_result_file}")
            
            # ç»Ÿè®¡æ€»ç»“
            total_chinese_searches = len(chinese_searches)
            successful_chinese_searches = sum(1 for result in all_chinese_results.values() if result['count'] > 0)
            total_chinese_patents = sum(result['count'] for result in all_chinese_results.values())
            
            logger.info(f"\n=== ä¸­æ–‡å…³é”®è¯æœç´¢ç»Ÿè®¡ ===")
            logger.info(f"æ€»æœç´¢æ¬¡æ•°: {total_chinese_searches}")
            logger.info(f"æˆåŠŸæœç´¢: {successful_chinese_searches}")
            logger.info(f"æ€»ä¸“åˆ©æ•°: {total_chinese_patents}")
            logger.info(f"æˆåŠŸç‡: {successful_chinese_searches/total_chinese_searches*100:.1f}%")
            
            return successful_chinese_searches > 0
            
        finally:
            await service.close()
            
    except Exception as e:
        logger.error(f"ä¸­æ–‡å…³é”®è¯æ¼”ç¤ºå¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False


async def demo_service_features():
    """æ¼”ç¤ºæœåŠ¡ç‰¹æ€§."""
    logger.info("\n=== Google Patents æœåŠ¡ç‰¹æ€§æ¼”ç¤º ===")
    
    try:
        from multi_agent_service.patent.services.google_patents_browser import GooglePatentsBrowserService
        
        # ç‰¹æ€§1: ä¸åŒçš„åˆå§‹åŒ–é€‰é¡¹
        logger.info("\n1. æœåŠ¡åˆå§‹åŒ–é€‰é¡¹æ¼”ç¤º")
        
        configs = [
            {"headless": True, "timeout": 30, "description": "æ ‡å‡†é…ç½®"},
            {"headless": True, "timeout": 60, "description": "é•¿è¶…æ—¶é…ç½®"}
        ]
        
        for config in configs:
            try:
                service = GooglePatentsBrowserService(**{k: v for k, v in config.items() if k != 'description'})
                await service.initialize()
                logger.info(f"   âœ“ {config['description']} - åˆå§‹åŒ–æˆåŠŸ")
                await service.close()
            except Exception as e:
                logger.warning(f"   âœ— {config['description']} - åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # ç‰¹æ€§2: æœç´¢é…ç½®é€‰é¡¹
        logger.info("\n2. æœç´¢é…ç½®é€‰é¡¹æ¼”ç¤º")
        
        service = GooglePatentsBrowserService(headless=True)
        await service.initialize()
        
        try:
            # å±•ç¤ºæœç´¢é…ç½®
            logger.info(f"   æœç´¢é…ç½®:")
            logger.info(f"     æ¯é¡µæœ€å¤§ç»“æœæ•°: {service.search_config['max_results_per_page']}")
            logger.info(f"     æœ€å¤§é¡µæ•°: {service.search_config['max_pages']}")
            logger.info(f"     æ»šåŠ¨å»¶è¿Ÿ: {service.search_config['scroll_delay']}s")
            logger.info(f"     ç‚¹å‡»å»¶è¿Ÿ: {service.search_config['click_delay']}s")
            
            # å±•ç¤ºCSSé€‰æ‹©å™¨é…ç½®
            logger.info(f"   CSSé€‰æ‹©å™¨é…ç½®:")
            for key, selector in list(service.selectors.items())[:3]:
                logger.info(f"     {key}: {selector}")
            
        finally:
            await service.close()
        
        # ç‰¹æ€§3: é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶
        logger.info("\n3. é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶æ¼”ç¤º")
        
        service = GooglePatentsBrowserService(headless=True)
        await service.initialize()
        
        try:
            # æµ‹è¯•æ— æ•ˆæœç´¢
            logger.info("   æµ‹è¯•ç©ºå…³é”®è¯æœç´¢...")
            patents = await service.search_patents(keywords=[], limit=1)
            logger.info(f"   ç©ºå…³é”®è¯æœç´¢ç»“æœ: {len(patents)} ä¸ªä¸“åˆ©")
            
            # æµ‹è¯•è¶…å¤§é™åˆ¶
            logger.info("   æµ‹è¯•è¶…å¤§ç»“æœé™åˆ¶...")
            patents = await service.search_patents(keywords=["test"], limit=1000)
            logger.info(f"   è¶…å¤§é™åˆ¶æœç´¢ç»“æœ: {len(patents)} ä¸ªä¸“åˆ©")
            
        finally:
            await service.close()
        
        logger.info("   âœ“ é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶æ­£å¸¸å·¥ä½œ")
        
        return True
        
    except Exception as e:
        logger.error(f"ç‰¹æ€§æ¼”ç¤ºå¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°."""
    logger.info("ğŸš€ Google Patents Browser-Use ä¸­æ–‡å…³é”®è¯ä¼˜åŒ–æ¼”ç¤ºå¼€å§‹")
    logger.info("=" * 70)
    
    # æ¼”ç¤º1: å®Œæ•´åŠŸèƒ½ï¼ˆåŒ…å«ä¼˜åŒ–çš„å…³é”®è¯ï¼‰
    demo1_success = await demo_google_patents_complete()
    
    # æ¼”ç¤º2: ä¸­æ–‡å…³é”®è¯ä¸“é—¨æ¼”ç¤º
    demo2_success = await demo_chinese_keywords()
    
    # æ¼”ç¤º3: æœåŠ¡ç‰¹æ€§
    demo3_success = await demo_service_features()
    
    # æ€»ç»“
    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š æœ€ç»ˆæ¼”ç¤ºæ€»ç»“:")
    logger.info(f"  å®Œæ•´åŠŸèƒ½æ¼”ç¤º: {'âœ“ æˆåŠŸ' if demo1_success else 'âœ— å¤±è´¥'}")
    logger.info(f"  ä¸­æ–‡å…³é”®è¯æ¼”ç¤º: {'âœ“ æˆåŠŸ' if demo2_success else 'âœ— å¤±è´¥'}")
    logger.info(f"  æœåŠ¡ç‰¹æ€§æ¼”ç¤º: {'âœ“ æˆåŠŸ' if demo3_success else 'âœ— å¤±è´¥'}")
    
    if demo1_success and demo2_success and demo3_success:
        logger.info("\nğŸ‰ Google Patents ä¸­æ–‡å…³é”®è¯ä¼˜åŒ–æ¼”ç¤ºå®Œå…¨æˆåŠŸ!")
        logger.info("âœ¨ ä¸»è¦æˆå°±:")
        logger.info("   â€¢ æˆåŠŸå®ç°äº† Google Patents è®¿é—®")
        logger.info("   â€¢ æ™ºèƒ½çˆ¬è™«æŠ€æœ¯ç»•è¿‡äº† JavaScript æ¸²æŸ“é™åˆ¶")
        logger.info("   â€¢ æä¾›äº†å®Œæ•´çš„ä¸“åˆ©æœç´¢å’Œæ•°æ®è½¬æ¢åŠŸèƒ½")
        logger.info("   â€¢ å®ç°äº†å¯é çš„é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶")
        logger.info("   â€¢ æ”¯æŒé«˜çº§æœç´¢å‚æ•°ï¼ˆæ—¥æœŸèŒƒå›´ã€ç”³è¯·äººè¿‡æ»¤ç­‰ï¼‰")
        logger.info("   â€¢ ğŸ†• æ”¯æŒä¸­æ–‡å…³é”®è¯è‡ªåŠ¨æ˜ å°„åˆ°è‹±æ–‡å…³é”®è¯")
        logger.info("   â€¢ ğŸ†• ä¼˜åŒ–äº†å…·èº«æ™ºèƒ½ã€å¤§è¯­è¨€æ¨¡å‹ã€å®¢æˆ·ç»†åˆ†ç­‰çƒ­é—¨é¢†åŸŸæœç´¢")
        logger.info("   â€¢ ğŸ†• æ‰©å±•äº†30+ä¸ªä¸­æ–‡æŠ€æœ¯æœ¯è¯­çš„è‹±æ–‡æ˜ å°„")
        return 0
    elif demo1_success or demo2_success:
        logger.warning("\nâš ï¸  éƒ¨åˆ†æ¼”ç¤ºæˆåŠŸï¼Œç³»ç»ŸåŸºæœ¬å¯ç”¨")
        logger.info("ğŸ’¡ å»ºè®®:")
        if not demo2_success:
            logger.info("   â€¢ ä¸­æ–‡å…³é”®è¯åŠŸèƒ½å¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        if not demo3_success:
            logger.info("   â€¢ æœåŠ¡ç‰¹æ€§æ¼”ç¤ºå¤±è´¥ï¼Œä½†æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸")
        return 1
    else:
        logger.error("\nâŒ æ¼”ç¤ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")
        return 2


async def quick_chinese_test():
    """å¿«é€Ÿæµ‹è¯•ä¸­æ–‡å…³é”®è¯åŠŸèƒ½."""
    logger.info("ğŸ” å¿«é€Ÿä¸­æ–‡å…³é”®è¯æµ‹è¯•")
    logger.info("=" * 40)
    
    # æµ‹è¯•å…³é”®è¯æ˜ å°„åŠŸèƒ½
    chinese_mapping = get_chinese_keyword_mapping()
    test_keywords = ["å…·èº«æ™ºèƒ½", "å¤§è¯­è¨€æ¨¡å‹", "å®¢æˆ·ç»†åˆ†"]
    
    logger.info("æµ‹è¯•å…³é”®è¯æ˜ å°„:")
    for keyword in test_keywords:
        expanded = expand_keywords_with_chinese([keyword], chinese_mapping)
        logger.info(f"  {keyword} -> {expanded[:3]}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
    
    # å¿«é€Ÿæœç´¢æµ‹è¯•
    try:
        from multi_agent_service.patent.services.google_patents_browser import GooglePatentsBrowserService
        
        service = GooglePatentsBrowserService(headless=True, timeout=30)
        await service.initialize()
        
        try:
            # æµ‹è¯•ä¸€ä¸ªå…³é”®è¯
            test_keyword = "å…·èº«æ™ºèƒ½"
            expanded_keywords = expand_keywords_with_chinese([test_keyword], chinese_mapping)
            
            logger.info(f"\nå¿«é€Ÿæœç´¢æµ‹è¯•: {test_keyword}")
            patents = await service.search_patents(keywords=expanded_keywords, limit=1)
            
            if patents:
                logger.info(f"âœ“ æˆåŠŸæ‰¾åˆ° {len(patents)} ä¸ªä¸“åˆ©")
                patent = patents[0]
                logger.info(f"  ç¤ºä¾‹ä¸“åˆ©: {patent.get('title', 'N/A')}")
            else:
                logger.info("âœ— æœªæ‰¾åˆ°ç›¸å…³ä¸“åˆ©")
                
        finally:
            await service.close()
            
        return len(patents) > 0 if 'patents' in locals() else False
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] == "--quick":
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
        success = asyncio.run(quick_chinese_test())
        sys.exit(0 if success else 1)
    else:
        # å®Œæ•´æ¼”ç¤ºæ¨¡å¼
        exit_code = asyncio.run(main())
        sys.exit(exit_code)