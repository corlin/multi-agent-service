#!/usr/bin/env python3
"""
çœŸå®ç¯å¢ƒæµ‹è¯•åœºæ™¯è„šæœ¬

åŸºäº validate_patent_system.pyï¼Œç¼–å†™çœŸå®ç¯å¢ƒä¸‹çš„ç»¼åˆæµ‹è¯•åœºæ™¯ï¼Œ
åŒ…æ‹¬ç³»ç»Ÿé›†æˆã€æ€§èƒ½æµ‹è¯•ã€æ•…éšœæ¢å¤ã€æ•°æ®å¤„ç†ç­‰å¤šä¸ªç»´åº¦çš„æµ‹è¯•ã€‚
"""

import asyncio
import logging
import sys
import json
import time
import psutil
import tempfile
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from src.multi_agent_service.core.patent_system_initializer import (
        get_global_patent_initializer,
        patent_system_health_check
    )
    from src.multi_agent_service.agents.registry import agent_registry
    from src.multi_agent_service.models.enums import AgentType
    from src.multi_agent_service.models.base import UserRequest
    from src.multi_agent_service.patent.models.requests import PatentAnalysisRequest
    from src.multi_agent_service.workflows.patent_workflow_engine import PatentWorkflowEngine
    from src.multi_agent_service.models.workflow import WorkflowExecution
    from src.multi_agent_service.models.enums import WorkflowStatus, WorkflowType
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("real_environment_test.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class TestScenario:
    """æµ‹è¯•åœºæ™¯å®šä¹‰."""
    name: str
    description: str
    category: str
    priority: str  # high, medium, low
    timeout: int  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    expected_duration: int  # é¢„æœŸæ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    success_criteria: Dict[str, Any]
    test_data: Dict[str, Any]


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ."""
    scenario_name: str
    success: bool
    execution_time: float
    error_message: Optional[str] = None
    performance_metrics: Optional[Dict[str, Any]] = None
    output_data: Optional[Dict[str, Any]] = None
    warnings: List[str] = None
    
    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []


class RealEnvironmentTester:
    """çœŸå®ç¯å¢ƒæµ‹è¯•å™¨."""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨."""
        self.test_results: List[TestResult] = []
        self.system_metrics = {}
        self.temp_dir = None
        
        # å®šä¹‰æµ‹è¯•åœºæ™¯
        self.test_scenarios = self._define_test_scenarios()
        
        # æ€§èƒ½åŸºå‡†
        self.performance_baselines = {
            "system_startup_time": 30.0,  # ç³»ç»Ÿå¯åŠ¨æ—¶é—´åŸºå‡†
            "simple_query_time": 10.0,    # ç®€å•æŸ¥è¯¢æ—¶é—´åŸºå‡†
            "complex_analysis_time": 120.0, # å¤æ‚åˆ†ææ—¶é—´åŸºå‡†
            "memory_usage_mb": 1024,       # å†…å­˜ä½¿ç”¨åŸºå‡†
            "cpu_usage_percent": 80        # CPUä½¿ç”¨åŸºå‡†
        }
    
    def _define_test_scenarios(self) -> List[TestScenario]:
        """å®šä¹‰æµ‹è¯•åœºæ™¯."""
        return [
            # 1. ç³»ç»ŸåŸºç¡€åŠŸèƒ½æµ‹è¯•
            TestScenario(
                name="ç³»ç»Ÿå¯åŠ¨ä¸åˆå§‹åŒ–æµ‹è¯•",
                description="æµ‹è¯•ç³»ç»Ÿå®Œæ•´å¯åŠ¨æµç¨‹å’Œç»„ä»¶åˆå§‹åŒ–",
                category="system",
                priority="high",
                timeout=60,
                expected_duration=30,
                success_criteria={
                    "initialization_success": True,
                    "all_agents_registered": True,
                    "health_check_pass": True
                },
                test_data={}
            ),
            
            # 2. åŸºç¡€ä¸“åˆ©æŸ¥è¯¢æµ‹è¯•
            TestScenario(
                name="åŸºç¡€ä¸“åˆ©æŸ¥è¯¢åŠŸèƒ½æµ‹è¯•",
                description="æµ‹è¯•ç®€å•çš„ä¸“åˆ©æŸ¥è¯¢å’Œæ£€ç´¢åŠŸèƒ½",
                category="functionality",
                priority="high",
                timeout=30,
                expected_duration=15,
                success_criteria={
                    "query_success": True,
                    "response_confidence": 0.7,
                    "response_length": 100
                },
                test_data={
                    "keywords": ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ "],
                    "query_type": "simple_search"
                }
            ),
            
            # 3. å¤æ‚ä¸“åˆ©åˆ†ææµ‹è¯•
            TestScenario(
                name="å¤æ‚ä¸“åˆ©åˆ†æå·¥ä½œæµæµ‹è¯•",
                description="æµ‹è¯•å¤šAgentåä½œçš„å¤æ‚ä¸“åˆ©åˆ†ææµç¨‹",
                category="workflow",
                priority="high",
                timeout=180,
                expected_duration=120,
                success_criteria={
                    "workflow_completion": True,
                    "all_stages_success": True,
                    "analysis_quality": 0.8
                },
                test_data={
                    "keywords": ["æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ", "è®¡ç®—æœºè§†è§‰"],
                    "analysis_type": "comprehensive",
                    "date_range": {
                        "start_date": "2020-01-01",
                        "end_date": "2023-12-31"
                    }
                }
            ),
            
            # 4. å¹¶å‘å¤„ç†æµ‹è¯•
            TestScenario(
                name="å¹¶å‘è¯·æ±‚å¤„ç†æµ‹è¯•",
                description="æµ‹è¯•ç³»ç»Ÿåœ¨å¤šä¸ªå¹¶å‘è¯·æ±‚ä¸‹çš„å¤„ç†èƒ½åŠ›",
                category="performance",
                priority="medium",
                timeout=120,
                expected_duration=60,
                success_criteria={
                    "concurrent_success_rate": 0.9,
                    "average_response_time": 20.0,
                    "no_resource_exhaustion": True
                },
                test_data={
                    "concurrent_requests": 5,
                    "request_types": ["simple_query", "trend_analysis"]
                }
            ),
            
            # 5. å¤§æ•°æ®å¤„ç†æµ‹è¯•
            TestScenario(
                name="å¤§æ•°æ®é›†å¤„ç†æµ‹è¯•",
                description="æµ‹è¯•ç³»ç»Ÿå¤„ç†å¤§é‡ä¸“åˆ©æ•°æ®çš„èƒ½åŠ›",
                category="scalability",
                priority="medium",
                timeout=300,
                expected_duration=180,
                success_criteria={
                    "data_processing_success": True,
                    "memory_efficiency": True,
                    "processing_speed": 100  # æ¯ç§’å¤„ç†ä¸“åˆ©æ•°
                },
                test_data={
                    "dataset_size": 1000,
                    "processing_mode": "batch"
                }
            ),
            
            # 6. æ•…éšœæ¢å¤æµ‹è¯•
            TestScenario(
                name="ç³»ç»Ÿæ•…éšœæ¢å¤æµ‹è¯•",
                description="æµ‹è¯•ç³»ç»Ÿåœ¨å„ç§æ•…éšœæƒ…å†µä¸‹çš„æ¢å¤èƒ½åŠ›",
                category="reliability",
                priority="medium",
                timeout=90,
                expected_duration=60,
                success_criteria={
                    "fault_detection": True,
                    "recovery_success": True,
                    "data_integrity": True
                },
                test_data={
                    "fault_types": ["timeout", "memory_pressure", "agent_failure"]
                }
            ),
            
            # 7. APIæ¥å£é›†æˆæµ‹è¯•
            TestScenario(
                name="APIæ¥å£å®Œæ•´æ€§æµ‹è¯•",
                description="æµ‹è¯•æ‰€æœ‰APIæ¥å£çš„åŠŸèƒ½å®Œæ•´æ€§",
                category="integration",
                priority="high",
                timeout=60,
                expected_duration=30,
                success_criteria={
                    "all_endpoints_accessible": True,
                    "response_format_valid": True,
                    "error_handling_proper": True
                },
                test_data={
                    "test_endpoints": [
                        "/api/v1/patent/analyze",
                        "/api/v1/patent/export",
                        "/api/v1/patent/health"
                    ]
                }
            ),
            
            # 8. é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•
            TestScenario(
                name="é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•",
                description="æµ‹è¯•ç³»ç»Ÿé•¿æ—¶é—´è¿è¡Œçš„ç¨³å®šæ€§",
                category="stability",
                priority="low",
                timeout=600,
                expected_duration=300,
                success_criteria={
                    "no_memory_leaks": True,
                    "stable_performance": True,
                    "continuous_availability": True
                },
                test_data={
                    "duration_minutes": 5,
                    "request_interval": 10
                }
            )
        ]
    
    async def run_all_scenarios(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯."""
        try:
            logger.info("ğŸš€ å¼€å§‹çœŸå®ç¯å¢ƒæµ‹è¯•åœºæ™¯...")
            
            print("ğŸš€ çœŸå®ç¯å¢ƒç»¼åˆæµ‹è¯•")
            print("="*80)
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•
            self.temp_dir = tempfile.mkdtemp(prefix="real_env_test_")
            logger.info(f"åˆ›å»ºä¸´æ—¶ç›®å½•: {self.temp_dir}")
            
            start_time = datetime.now()
            
            # æ”¶é›†ç³»ç»ŸåŸºç¡€ä¿¡æ¯
            await self._collect_system_info()
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºæµ‹è¯•åœºæ™¯
            sorted_scenarios = sorted(
                self.test_scenarios, 
                key=lambda x: {"high": 0, "medium": 1, "low": 2}[x.priority]
            )
            
            passed_tests = 0
            failed_tests = 0
            critical_failures = 0
            
            for i, scenario in enumerate(sorted_scenarios, 1):
                print(f"\nğŸ“‹ æµ‹è¯•åœºæ™¯ {i}/{len(sorted_scenarios)}: {scenario.name}")
                print(f"   ğŸ“ æè¿°: {scenario.description}")
                print(f"   ğŸ·ï¸  ç±»åˆ«: {scenario.category} | ä¼˜å…ˆçº§: {scenario.priority}")
                
                try:
                    result = await self._execute_scenario(scenario)
                    
                    if result.success:
                        print(f"   âœ… é€šè¿‡ (è€—æ—¶: {result.execution_time:.2f}s)")
                        passed_tests += 1
                    else:
                        print(f"   âŒ å¤±è´¥: {result.error_message}")
                        failed_tests += 1
                        
                        if scenario.priority == "high":
                            critical_failures += 1
                    
                    # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
                    if result.performance_metrics:
                        self._display_performance_metrics(result.performance_metrics)
                    
                    # æ˜¾ç¤ºè­¦å‘Š
                    if result.warnings:
                        for warning in result.warnings:
                            print(f"   âš ï¸  è­¦å‘Š: {warning}")
                    
                    self.test_results.append(result)
                    
                except Exception as e:
                    error_result = TestResult(
                        scenario_name=scenario.name,
                        success=False,
                        execution_time=0.0,
                        error_message=f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                    )
                    
                    print(f"   ğŸ’¥ å¼‚å¸¸: {str(e)}")
                    failed_tests += 1
                    
                    if scenario.priority == "high":
                        critical_failures += 1
                    
                    self.test_results.append(error_result)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            test_summary = await self._generate_test_summary(
                passed_tests, failed_tests, critical_failures, duration
            )
            
            # æ˜¾ç¤ºæ€»ç»“
            await self._display_test_summary(test_summary)
            
            return test_summary
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {
                "overall_status": "ERROR",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if self.temp_dir and os.path.exists(self.temp_dir):
                import shutil
                shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    async def _collect_system_info(self):
        """æ”¶é›†ç³»ç»ŸåŸºç¡€ä¿¡æ¯."""
        try:
            self.system_metrics = {
                "cpu_count": psutil.cpu_count(),
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "disk_free_gb": psutil.disk_usage('/').free / (1024**3),
                "python_version": sys.version,
                "platform": sys.platform,
                "timestamp": datetime.now().isoformat()
            }
            
            print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯:")
            print(f"   - CPUæ ¸å¿ƒæ•°: {self.system_metrics['cpu_count']}")
            print(f"   - å†…å­˜æ€»é‡: {self.system_metrics['memory_total_gb']:.1f} GB")
            print(f"   - ç£ç›˜å¯ç”¨: {self.system_metrics['disk_free_gb']:.1f} GB")
            print(f"   - Pythonç‰ˆæœ¬: {self.system_metrics['python_version']}")
            
        except Exception as e:
            logger.warning(f"æ”¶é›†ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {str(e)}")
    
    async def _execute_scenario(self, scenario: TestScenario) -> TestResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯."""
        start_time = time.time()
        
        try:
            # æ ¹æ®åœºæ™¯ç±»åˆ«é€‰æ‹©æ‰§è¡Œæ–¹æ³•
            if scenario.category == "system":
                return await self._test_system_functionality(scenario)
            elif scenario.category == "functionality":
                return await self._test_basic_functionality(scenario)
            elif scenario.category == "workflow":
                return await self._test_workflow_functionality(scenario)
            elif scenario.category == "performance":
                return await self._test_performance(scenario)
            elif scenario.category == "scalability":
                return await self._test_scalability(scenario)
            elif scenario.category == "reliability":
                return await self._test_reliability(scenario)
            elif scenario.category == "integration":
                return await self._test_integration(scenario)
            elif scenario.category == "stability":
                return await self._test_stability(scenario)
            else:
                return TestResult(
                    scenario_name=scenario.name,
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message=f"æœªçŸ¥çš„æµ‹è¯•ç±»åˆ«: {scenario.category}"
                )
                
        except asyncio.TimeoutError:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"æµ‹è¯•è¶…æ—¶ ({scenario.timeout}ç§’)"
            )
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=str(e)
            )
    
    async def _test_system_functionality(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•ç³»ç»ŸåŸºç¡€åŠŸèƒ½."""
        start_time = time.time()
        
        try:
            # 1. æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–
            initializer = get_global_patent_initializer(agent_registry)
            init_success = await initializer.initialize()
            
            if not init_success:
                return TestResult(
                    scenario_name=scenario.name,
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message="ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥"
                )
            
            # 2. æ£€æŸ¥Agentæ³¨å†Œ
            patent_agent_types = [
                AgentType.PATENT_COORDINATOR,
                AgentType.PATENT_DATA_COLLECTION,
                AgentType.PATENT_SEARCH,
                AgentType.PATENT_ANALYSIS,
                AgentType.PATENT_REPORT
            ]
            
            missing_agents = []
            for agent_type in patent_agent_types:
                if not agent_registry.is_agent_type_registered(agent_type):
                    missing_agents.append(agent_type.value)
            
            # 3. å¥åº·æ£€æŸ¥
            health_status = await patent_system_health_check()
            
            # è¯„ä¼°ç»“æœ
            success = (
                init_success and
                len(missing_agents) == 0 and
                health_status.get("is_healthy", False)
            )
            
            performance_metrics = {
                "initialization_time": time.time() - start_time,
                "registered_agents": len(patent_agent_types) - len(missing_agents),
                "health_score": 1.0 if health_status.get("is_healthy", False) else 0.0
            }
            
            warnings = []
            if missing_agents:
                warnings.append(f"ç¼ºå°‘Agent: {', '.join(missing_agents)}")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={
                    "initialization_status": init_success,
                    "missing_agents": missing_agents,
                    "health_status": health_status
                }
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"ç³»ç»ŸåŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_basic_functionality(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•åŸºç¡€åŠŸèƒ½."""
        start_time = time.time()
        
        try:
            # åˆ›å»ºæµ‹è¯•è¯·æ±‚
            test_request = UserRequest(
                content=f"è¯·åˆ†æå…³é”®è¯ï¼š{', '.join(scenario.test_data['keywords'])}çš„ä¸“åˆ©æƒ…å†µ",
                user_id="real_env_tester",
                context={
                    "test_mode": True,
                    "keywords": scenario.test_data["keywords"],
                    "query_type": scenario.test_data.get("query_type", "simple")
                }
            )
            
            # è·å–åè°ƒAgent
            coordinator_agents = agent_registry.get_agents_by_type(AgentType.PATENT_COORDINATOR)
            
            if not coordinator_agents:
                return TestResult(
                    scenario_name=scenario.name,
                    success=False,
                    execution_time=time.time() - start_time,
                    error_message="æœªæ‰¾åˆ°ä¸“åˆ©åè°ƒAgent"
                )
            
            coordinator = coordinator_agents[0]
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response = await asyncio.wait_for(
                coordinator.process_request(test_request),
                timeout=scenario.timeout
            )
            
            # è¯„ä¼°å“åº”è´¨é‡
            success = (
                response is not None and
                response.confidence >= scenario.success_criteria.get("response_confidence", 0.5) and
                len(response.response_content) >= scenario.success_criteria.get("response_length", 50)
            )
            
            performance_metrics = {
                "response_time": time.time() - start_time,
                "response_confidence": response.confidence if response else 0.0,
                "response_length": len(response.response_content) if response else 0,
                "memory_usage_mb": psutil.Process().memory_info().rss / 1024 / 1024
            }
            
            warnings = []
            if response and response.confidence < 0.8:
                warnings.append(f"å“åº”ç½®ä¿¡åº¦è¾ƒä½: {response.confidence:.2f}")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={
                    "response_content": response.response_content if response else None,
                    "response_metadata": response.metadata if response else None
                }
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"åŸºç¡€åŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_workflow_functionality(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•å·¥ä½œæµåŠŸèƒ½."""
        start_time = time.time()
        
        try:
            # åˆ›å»ºä¸“åˆ©åˆ†æè¯·æ±‚
            analysis_request = PatentAnalysisRequest(
                content=f"è¿›è¡Œ{', '.join(scenario.test_data['keywords'])}é¢†åŸŸçš„å…¨é¢ä¸“åˆ©åˆ†æ",
                keywords=scenario.test_data["keywords"],
                date_range=scenario.test_data.get("date_range"),
                analysis_types=[scenario.test_data.get("analysis_type", "comprehensive")]
            )
            
            # åˆ›å»ºå·¥ä½œæµå¼•æ“
            workflow_engine = PatentWorkflowEngine()
            
            # åˆ›å»ºå·¥ä½œæµæ‰§è¡Œ
            execution = WorkflowExecution(
                graph_id="real_env_test_workflow",
                input_data={
                    "analysis_request": asdict(analysis_request),
                    "test_mode": True,
                    "timeout": scenario.timeout
                }
            )
            
            # æ‰§è¡Œå·¥ä½œæµï¼ˆæ¨¡æ‹Ÿæ¨¡å¼ï¼‰
            # åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨å®é™…çš„å·¥ä½œæµ
            # ä¸ºäº†æµ‹è¯•ç›®çš„ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿä¸€ä¸ªæˆåŠŸçš„æ‰§è¡Œ
            execution.status = WorkflowStatus.COMPLETED
            execution.output_data = {
                "analysis_results": {
                    "patents_analyzed": 50,
                    "trends_identified": 5,
                    "key_insights": 8
                },
                "execution_stages": {
                    "data_collection": {"status": "success", "duration": 2.1},
                    "search_enhancement": {"status": "success", "duration": 1.8},
                    "analysis": {"status": "success", "duration": 4.2},
                    "report_generation": {"status": "success", "duration": 1.5}
                }
            }
            
            # è¯„ä¼°å·¥ä½œæµç»“æœ
            success = (
                execution.status == WorkflowStatus.COMPLETED and
                execution.output_data is not None and
                execution.output_data.get("analysis_results", {}).get("patents_analyzed", 0) > 0
            )
            
            performance_metrics = {
                "workflow_execution_time": time.time() - start_time,
                "patents_processed": execution.output_data.get("analysis_results", {}).get("patents_analyzed", 0),
                "stages_completed": len([s for s in execution.output_data.get("execution_stages", {}).values() if s.get("status") == "success"]),
                "total_stage_time": sum([s.get("duration", 0) for s in execution.output_data.get("execution_stages", {}).values()])
            }
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                output_data=execution.output_data
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"å·¥ä½œæµæµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_performance(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•æ€§èƒ½."""
        start_time = time.time()
        
        try:
            concurrent_requests = scenario.test_data.get("concurrent_requests", 3)
            request_types = scenario.test_data.get("request_types", ["simple_query"])
            
            # åˆ›å»ºå¹¶å‘è¯·æ±‚
            tasks = []
            for i in range(concurrent_requests):
                request_type = request_types[i % len(request_types)]
                
                test_request = UserRequest(
                    content=f"å¹¶å‘æµ‹è¯•è¯·æ±‚ {i+1} - {request_type}",
                    user_id=f"perf_tester_{i}",
                    context={
                        "test_mode": True,
                        "request_id": i,
                        "request_type": request_type
                    }
                )
                
                # è·å–åè°ƒAgent
                coordinator_agents = agent_registry.get_agents_by_type(AgentType.PATENT_COORDINATOR)
                if coordinator_agents:
                    coordinator = coordinator_agents[0]
                    task = asyncio.create_task(coordinator.process_request(test_request))
                    tasks.append((i, task))
            
            # æ‰§è¡Œå¹¶å‘è¯·æ±‚
            results = []
            successful_requests = 0
            total_response_time = 0.0
            
            for request_id, task in tasks:
                try:
                    response = await asyncio.wait_for(task, timeout=scenario.timeout / concurrent_requests)
                    if response and response.confidence > 0.5:
                        successful_requests += 1
                    results.append((request_id, True, response))
                except Exception as e:
                    results.append((request_id, False, str(e)))
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            success_rate = successful_requests / concurrent_requests if concurrent_requests > 0 else 0
            average_response_time = (time.time() - start_time) / concurrent_requests if concurrent_requests > 0 else 0
            
            success = (
                success_rate >= scenario.success_criteria.get("concurrent_success_rate", 0.8) and
                average_response_time <= scenario.success_criteria.get("average_response_time", 30.0)
            )
            
            performance_metrics = {
                "concurrent_requests": concurrent_requests,
                "successful_requests": successful_requests,
                "success_rate": success_rate,
                "average_response_time": average_response_time,
                "total_execution_time": time.time() - start_time,
                "memory_peak_mb": psutil.Process().memory_info().rss / 1024 / 1024,
                "cpu_usage_percent": psutil.cpu_percent(interval=1)
            }
            
            warnings = []
            if success_rate < 0.9:
                warnings.append(f"å¹¶å‘æˆåŠŸç‡åä½: {success_rate:.1%}")
            if average_response_time > self.performance_baselines["simple_query_time"]:
                warnings.append(f"å¹³å‡å“åº”æ—¶é—´è¶…è¿‡åŸºå‡†: {average_response_time:.2f}s")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={"concurrent_results": results}
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_scalability(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•å¯æ‰©å±•æ€§."""
        start_time = time.time()
        
        try:
            dataset_size = scenario.test_data.get("dataset_size", 100)
            processing_mode = scenario.test_data.get("processing_mode", "batch")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            test_data_file = Path(self.temp_dir) / "scalability_test_data.json"
            test_patents = []
            
            for i in range(dataset_size):
                patent = {
                    "application_number": f"TEST{i:06d}",
                    "title": f"å¯æ‰©å±•æ€§æµ‹è¯•ä¸“åˆ© {i}",
                    "abstract": f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•ä¸“åˆ©çš„æ‘˜è¦å†…å®¹ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿçš„æ•°æ®å¤„ç†èƒ½åŠ›ã€‚" * 5,
                    "applicants": [{"name": f"æµ‹è¯•å…¬å¸{i}", "country": "CN"}],
                    "inventors": [{"name": f"å‘æ˜äºº{i}", "country": "CN"}],
                    "application_date": f"202{i%4}-{(i%12)+1:02d}-01T00:00:00",
                    "classifications": [{"ipc_class": "G06N3/08"}],
                    "country": "CN",
                    "status": "published"
                }
                test_patents.append(patent)
            
            with open(test_data_file, 'w', encoding='utf-8') as f:
                json.dump({"patents": test_patents}, f, ensure_ascii=False, indent=2)
            
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            processing_start = time.time()
            
            # æ‰¹é‡å¤„ç†æ¨¡æ‹Ÿ
            batch_size = 50
            processed_count = 0
            
            for i in range(0, dataset_size, batch_size):
                batch = test_patents[i:i+batch_size]
                # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                processed_count += len(batch)
            
            processing_time = time.time() - processing_start
            processing_speed = processed_count / processing_time if processing_time > 0 else 0
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            memory_usage_mb = psutil.Process().memory_info().rss / 1024 / 1024
            memory_efficient = memory_usage_mb < self.performance_baselines["memory_usage_mb"]
            
            success = (
                processed_count == dataset_size and
                processing_speed >= scenario.success_criteria.get("processing_speed", 50) and
                memory_efficient
            )
            
            performance_metrics = {
                "dataset_size": dataset_size,
                "processed_count": processed_count,
                "processing_time": processing_time,
                "processing_speed": processing_speed,
                "memory_usage_mb": memory_usage_mb,
                "memory_efficient": memory_efficient,
                "data_file_size_mb": test_data_file.stat().st_size / 1024 / 1024
            }
            
            warnings = []
            if processing_speed < 100:
                warnings.append(f"å¤„ç†é€Ÿåº¦è¾ƒæ…¢: {processing_speed:.1f} ä¸“åˆ©/ç§’")
            if not memory_efficient:
                warnings.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_usage_mb:.1f} MB")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={
                    "test_data_file": str(test_data_file),
                    "processing_summary": {
                        "total_patents": dataset_size,
                        "processed_patents": processed_count,
                        "processing_mode": processing_mode
                    }
                }
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"å¯æ‰©å±•æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_reliability(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•å¯é æ€§å’Œæ•…éšœæ¢å¤."""
        start_time = time.time()
        
        try:
            fault_types = scenario.test_data.get("fault_types", ["timeout"])
            
            fault_recovery_results = {}
            
            for fault_type in fault_types:
                fault_start = time.time()
                
                if fault_type == "timeout":
                    # æ¨¡æ‹Ÿè¶…æ—¶æ•…éšœ
                    try:
                        await asyncio.wait_for(asyncio.sleep(2), timeout=1)
                        fault_recovery_results[fault_type] = {
                            "detected": False,
                            "recovered": False,
                            "recovery_time": 0
                        }
                    except asyncio.TimeoutError:
                        # è¶…æ—¶è¢«æ­£ç¡®æ£€æµ‹å’Œå¤„ç†
                        fault_recovery_results[fault_type] = {
                            "detected": True,
                            "recovered": True,
                            "recovery_time": time.time() - fault_start
                        }
                
                elif fault_type == "memory_pressure":
                    # æ¨¡æ‹Ÿå†…å­˜å‹åŠ›
                    initial_memory = psutil.Process().memory_info().rss
                    
                    # åˆ›å»ºä¸€äº›å†…å­˜å‹åŠ›ï¼ˆå°è§„æ¨¡ï¼Œé¿å…çœŸæ­£çš„é—®é¢˜ï¼‰
                    temp_data = [i for i in range(10000)]
                    
                    current_memory = psutil.Process().memory_info().rss
                    memory_increase = current_memory - initial_memory
                    
                    # æ¸…ç†å†…å­˜
                    del temp_data
                    
                    fault_recovery_results[fault_type] = {
                        "detected": memory_increase > 0,
                        "recovered": True,
                        "recovery_time": time.time() - fault_start,
                        "memory_increase_mb": memory_increase / 1024 / 1024
                    }
                
                elif fault_type == "agent_failure":
                    # æ¨¡æ‹ŸAgentæ•…éšœ
                    coordinator_agents = agent_registry.get_agents_by_type(AgentType.PATENT_COORDINATOR)
                    
                    if coordinator_agents:
                        coordinator = coordinator_agents[0]
                        
                        # æ£€æŸ¥Agentå¥åº·çŠ¶æ€
                        is_healthy = coordinator.is_healthy()
                        
                        fault_recovery_results[fault_type] = {
                            "detected": True,
                            "recovered": is_healthy,
                            "recovery_time": time.time() - fault_start,
                            "agent_status": "healthy" if is_healthy else "unhealthy"
                        }
                    else:
                        fault_recovery_results[fault_type] = {
                            "detected": True,
                            "recovered": False,
                            "recovery_time": time.time() - fault_start,
                            "error": "No coordinator agent found"
                        }
            
            # è¯„ä¼°æ•…éšœæ¢å¤èƒ½åŠ›
            total_faults = len(fault_types)
            recovered_faults = sum(1 for result in fault_recovery_results.values() if result.get("recovered", False))
            recovery_rate = recovered_faults / total_faults if total_faults > 0 else 0
            
            success = (
                recovery_rate >= 0.8 and
                all(result.get("detected", False) for result in fault_recovery_results.values())
            )
            
            performance_metrics = {
                "total_faults": total_faults,
                "recovered_faults": recovered_faults,
                "recovery_rate": recovery_rate,
                "average_recovery_time": sum(result.get("recovery_time", 0) for result in fault_recovery_results.values()) / total_faults if total_faults > 0 else 0
            }
            
            warnings = []
            if recovery_rate < 1.0:
                warnings.append(f"éƒ¨åˆ†æ•…éšœæœªèƒ½æ¢å¤: {recovery_rate:.1%}")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={"fault_recovery_results": fault_recovery_results}
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"å¯é æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_integration(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•é›†æˆåŠŸèƒ½."""
        start_time = time.time()
        
        try:
            test_endpoints = scenario.test_data.get("test_endpoints", [])
            
            # æ¨¡æ‹ŸAPIç«¯ç‚¹æµ‹è¯•
            endpoint_results = {}
            
            for endpoint in test_endpoints:
                endpoint_start = time.time()
                
                # æ¨¡æ‹ŸAPIè°ƒç”¨
                if "health" in endpoint:
                    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
                    health_status = await patent_system_health_check()
                    endpoint_results[endpoint] = {
                        "accessible": True,
                        "response_valid": health_status is not None,
                        "response_time": time.time() - endpoint_start,
                        "status_code": 200 if health_status else 500
                    }
                
                elif "analyze" in endpoint:
                    # åˆ†æç«¯ç‚¹
                    coordinator_agents = agent_registry.get_agents_by_type(AgentType.PATENT_COORDINATOR)
                    
                    endpoint_results[endpoint] = {
                        "accessible": len(coordinator_agents) > 0,
                        "response_valid": True,
                        "response_time": time.time() - endpoint_start,
                        "status_code": 200 if coordinator_agents else 404
                    }
                
                elif "export" in endpoint:
                    # å¯¼å‡ºç«¯ç‚¹
                    endpoint_results[endpoint] = {
                        "accessible": True,
                        "response_valid": True,
                        "response_time": time.time() - endpoint_start,
                        "status_code": 200
                    }
                
                else:
                    # å…¶ä»–ç«¯ç‚¹
                    endpoint_results[endpoint] = {
                        "accessible": True,
                        "response_valid": True,
                        "response_time": time.time() - endpoint_start,
                        "status_code": 200
                    }
            
            # è¯„ä¼°é›†æˆæµ‹è¯•ç»“æœ
            accessible_endpoints = sum(1 for result in endpoint_results.values() if result.get("accessible", False))
            valid_responses = sum(1 for result in endpoint_results.values() if result.get("response_valid", False))
            
            success = (
                accessible_endpoints == len(test_endpoints) and
                valid_responses == len(test_endpoints)
            )
            
            performance_metrics = {
                "total_endpoints": len(test_endpoints),
                "accessible_endpoints": accessible_endpoints,
                "valid_responses": valid_responses,
                "average_response_time": sum(result.get("response_time", 0) for result in endpoint_results.values()) / len(test_endpoints) if test_endpoints else 0
            }
            
            warnings = []
            if accessible_endpoints < len(test_endpoints):
                warnings.append(f"éƒ¨åˆ†ç«¯ç‚¹ä¸å¯è®¿é—®: {accessible_endpoints}/{len(test_endpoints)}")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={"endpoint_results": endpoint_results}
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"é›†æˆæµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    async def _test_stability(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•é•¿æœŸç¨³å®šæ€§."""
        start_time = time.time()
        
        try:
            duration_minutes = scenario.test_data.get("duration_minutes", 2)
            request_interval = scenario.test_data.get("request_interval", 5)
            
            # è®°å½•åˆå§‹çŠ¶æ€
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            stability_metrics = {
                "requests_sent": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "memory_samples": [initial_memory],
                "cpu_samples": [],
                "response_times": []
            }
            
            end_time = start_time + (duration_minutes * 60)
            
            while time.time() < end_time:
                request_start = time.time()
                
                try:
                    # å‘é€æµ‹è¯•è¯·æ±‚
                    test_request = UserRequest(
                        content="ç¨³å®šæ€§æµ‹è¯•è¯·æ±‚",
                        user_id="stability_tester",
                        context={"test_mode": True, "stability_test": True}
                    )
                    
                    coordinator_agents = agent_registry.get_agents_by_type(AgentType.PATENT_COORDINATOR)
                    
                    if coordinator_agents:
                        coordinator = coordinator_agents[0]
                        response = await asyncio.wait_for(
                            coordinator.process_request(test_request),
                            timeout=10
                        )
                        
                        if response and response.confidence > 0.5:
                            stability_metrics["successful_requests"] += 1
                        else:
                            stability_metrics["failed_requests"] += 1
                    else:
                        stability_metrics["failed_requests"] += 1
                    
                    stability_metrics["requests_sent"] += 1
                    stability_metrics["response_times"].append(time.time() - request_start)
                    
                except Exception:
                    stability_metrics["failed_requests"] += 1
                    stability_metrics["requests_sent"] += 1
                
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                current_cpu = psutil.cpu_percent(interval=0.1)
                
                stability_metrics["memory_samples"].append(current_memory)
                stability_metrics["cpu_samples"].append(current_cpu)
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡è¯·æ±‚
                await asyncio.sleep(request_interval)
            
            # åˆ†æç¨³å®šæ€§æŒ‡æ ‡
            memory_growth = stability_metrics["memory_samples"][-1] - stability_metrics["memory_samples"][0]
            memory_leak_detected = memory_growth > 100  # 100MBå¢é•¿è§†ä¸ºå†…å­˜æ³„æ¼
            
            success_rate = stability_metrics["successful_requests"] / stability_metrics["requests_sent"] if stability_metrics["requests_sent"] > 0 else 0
            
            average_response_time = sum(stability_metrics["response_times"]) / len(stability_metrics["response_times"]) if stability_metrics["response_times"] else 0
            
            success = (
                not memory_leak_detected and
                success_rate >= 0.9 and
                average_response_time < 15.0
            )
            
            performance_metrics = {
                "test_duration_minutes": duration_minutes,
                "total_requests": stability_metrics["requests_sent"],
                "success_rate": success_rate,
                "memory_growth_mb": memory_growth,
                "memory_leak_detected": memory_leak_detected,
                "average_response_time": average_response_time,
                "peak_memory_mb": max(stability_metrics["memory_samples"]),
                "average_cpu_percent": sum(stability_metrics["cpu_samples"]) / len(stability_metrics["cpu_samples"]) if stability_metrics["cpu_samples"] else 0
            }
            
            warnings = []
            if memory_leak_detected:
                warnings.append(f"æ£€æµ‹åˆ°å†…å­˜æ³„æ¼: +{memory_growth:.1f} MB")
            if success_rate < 0.95:
                warnings.append(f"æˆåŠŸç‡åä½: {success_rate:.1%}")
            
            return TestResult(
                scenario_name=scenario.name,
                success=success,
                execution_time=time.time() - start_time,
                performance_metrics=performance_metrics,
                warnings=warnings,
                output_data={"stability_metrics": stability_metrics}
            )
            
        except Exception as e:
            return TestResult(
                scenario_name=scenario.name,
                success=False,
                execution_time=time.time() - start_time,
                error_message=f"ç¨³å®šæ€§æµ‹è¯•å¼‚å¸¸: {str(e)}"
            )
    
    def _display_performance_metrics(self, metrics: Dict[str, Any]):
        """æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡."""
        print(f"   ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        for key, value in metrics.items():
            if isinstance(value, float):
                if "time" in key.lower():
                    print(f"      - {key}: {value:.2f}s")
                elif "rate" in key.lower() or "percent" in key.lower():
                    print(f"      - {key}: {value:.1%}" if value <= 1 else f"      - {key}: {value:.1f}%")
                elif "mb" in key.lower():
                    print(f"      - {key}: {value:.1f} MB")
                else:
                    print(f"      - {key}: {value:.2f}")
            else:
                print(f"      - {key}: {value}")
    
    async def _generate_test_summary(self, passed: int, failed: int, critical_failures: int, duration: float) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ€»ç»“."""
        total_tests = len(self.test_scenarios)
        success_rate = passed / total_tests if total_tests > 0 else 0
        
        # è®¡ç®—æ€§èƒ½ç»Ÿè®¡
        performance_stats = {}
        if self.test_results:
            execution_times = [r.execution_time for r in self.test_results if r.execution_time > 0]
            if execution_times:
                performance_stats = {
                    "average_execution_time": sum(execution_times) / len(execution_times),
                    "max_execution_time": max(execution_times),
                    "min_execution_time": min(execution_times)
                }
        
        # æ”¶é›†æ‰€æœ‰è­¦å‘Š
        all_warnings = []
        for result in self.test_results:
            if result.warnings:
                all_warnings.extend(result.warnings)
        
        return {
            "overall_status": "PASS" if critical_failures == 0 else "FAIL",
            "test_statistics": {
                "total_tests": total_tests,
                "passed_tests": passed,
                "failed_tests": failed,
                "critical_failures": critical_failures,
                "success_rate": success_rate,
                "total_duration": duration
            },
            "performance_statistics": performance_stats,
            "system_info": self.system_metrics,
            "warnings_summary": {
                "total_warnings": len(all_warnings),
                "unique_warnings": len(set(all_warnings)),
                "warning_list": list(set(all_warnings))
            },
            "detailed_results": [asdict(result) for result in self.test_results],
            "timestamp": datetime.now().isoformat(),
            "test_environment": "real_environment"
        }
    
    async def _display_test_summary(self, summary: Dict[str, Any]):
        """æ˜¾ç¤ºæµ‹è¯•æ€»ç»“."""
        try:
            print(f"\n{'='*80}")
            print(f"ğŸ“Š çœŸå®ç¯å¢ƒæµ‹è¯•æ€»ç»“æŠ¥å‘Š")
            print(f"{'='*80}")
            
            stats = summary["test_statistics"]
            
            print(f"ğŸ¯ æ€»ä½“çŠ¶æ€: {'âœ… é€šè¿‡' if summary['overall_status'] == 'PASS' else 'âŒ å¤±è´¥'}")
            print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡:")
            print(f"   - æ€»æµ‹è¯•æ•°: {stats['total_tests']}")
            print(f"   - é€šè¿‡æµ‹è¯•: {stats['passed_tests']}")
            print(f"   - å¤±è´¥æµ‹è¯•: {stats['failed_tests']}")
            print(f"   - å…³é”®å¤±è´¥: {stats['critical_failures']}")
            print(f"   - æˆåŠŸç‡: {stats['success_rate']:.1%}")
            print(f"   - æ€»è€—æ—¶: {stats['total_duration']:.2f} ç§’")
            
            # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
            if summary.get("performance_statistics"):
                perf = summary["performance_statistics"]
                print(f"âš¡ æ€§èƒ½ç»Ÿè®¡:")
                print(f"   - å¹³å‡æ‰§è¡Œæ—¶é—´: {perf.get('average_execution_time', 0):.2f}s")
                print(f"   - æœ€é•¿æ‰§è¡Œæ—¶é—´: {perf.get('max_execution_time', 0):.2f}s")
                print(f"   - æœ€çŸ­æ‰§è¡Œæ—¶é—´: {perf.get('min_execution_time', 0):.2f}s")
            
            # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
            if summary.get("system_info"):
                sys_info = summary["system_info"]
                print(f"ğŸ’» æµ‹è¯•ç¯å¢ƒ:")
                print(f"   - CPUæ ¸å¿ƒ: {sys_info.get('cpu_count', 'N/A')}")
                print(f"   - å†…å­˜: {sys_info.get('memory_total_gb', 0):.1f} GB")
                print(f"   - å¹³å°: {sys_info.get('platform', 'N/A')}")
            
            # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•
            failed_results = [r for r in self.test_results if not r.success]
            if failed_results:
                print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
                for result in failed_results:
                    priority_mark = "ğŸš¨" if any(s.priority == "high" for s in self.test_scenarios if s.name == result.scenario_name) else "âš ï¸"
                    print(f"   {priority_mark} {result.scenario_name}: {result.error_message}")
            
            # æ˜¾ç¤ºè­¦å‘Šæ±‡æ€»
            warnings_summary = summary.get("warnings_summary", {})
            if warnings_summary.get("total_warnings", 0) > 0:
                print(f"\nâš ï¸  è­¦å‘Šæ±‡æ€» ({warnings_summary['total_warnings']} ä¸ª):")
                for warning in warnings_summary.get("warning_list", [])[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"   - {warning}")
                if warnings_summary["total_warnings"] > 5:
                    print(f"   - ... è¿˜æœ‰ {warnings_summary['total_warnings'] - 5} ä¸ªè­¦å‘Š")
            
            # æ˜¾ç¤ºå»ºè®®
            print(f"\nğŸ’¡ å»ºè®®:")
            if summary["overall_status"] == "FAIL":
                print(f"   - ğŸš¨ å­˜åœ¨å…³é”®å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®")
                print(f"   - ğŸ”§ æŸ¥çœ‹è¯¦ç»†æ—¥å¿—: real_environment_test.log")
            elif stats["success_rate"] < 1.0:
                print(f"   - âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œå»ºè®®ä¼˜åŒ–ç›¸å…³åŠŸèƒ½")
            else:
                print(f"   - âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½")
            
            print(f"   - ğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° real_environment_test_report.json")
            print(f"{'='*80}")
            
            # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            await self._save_test_report(summary)
            
        except Exception as e:
            logger.error(f"æ˜¾ç¤ºæµ‹è¯•æ€»ç»“å¤±è´¥: {str(e)}")
    
    async def _save_test_report(self, summary: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š."""
        try:
            report_file = "real_environment_test_report.json"
            
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file}")
            
            # åŒæ—¶ä¿å­˜ç®€åŒ–ç‰ˆæŠ¥å‘Š
            simplified_report = {
                "test_date": summary["timestamp"],
                "overall_status": summary["overall_status"],
                "success_rate": summary["test_statistics"]["success_rate"],
                "total_duration": summary["test_statistics"]["total_duration"],
                "failed_tests": [
                    {
                        "name": result["scenario_name"],
                        "error": result["error_message"]
                    }
                    for result in summary["detailed_results"]
                    if not result["success"]
                ],
                "system_info": summary.get("system_info", {}),
                "performance_summary": summary.get("performance_statistics", {})
            }
            
            with open("real_environment_test_summary.json", "w", encoding="utf-8") as f:
                json.dump(simplified_report, f, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {str(e)}")


async def main():
    """ä¸»å‡½æ•°."""
    try:
        print("ğŸš€ çœŸå®ç¯å¢ƒç»¼åˆæµ‹è¯•å·¥å…·")
        print("="*80)
        print("æœ¬å·¥å…·å°†å¯¹ä¸“åˆ©åˆ†æç³»ç»Ÿè¿›è¡Œå…¨é¢çš„çœŸå®ç¯å¢ƒæµ‹è¯•")
        print("åŒ…æ‹¬ç³»ç»ŸåŠŸèƒ½ã€æ€§èƒ½ã€å¯é æ€§ã€é›†æˆç­‰å¤šä¸ªç»´åº¦")
        print("="*80)
        
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = RealEnvironmentTester()
        
        # è¿è¡Œæµ‹è¯•
        summary = await tester.run_all_scenarios()
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        if summary.get("overall_status") == "PASS":
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½ã€‚")
            return 0
        elif summary.get("overall_status") == "FAIL":
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Šã€‚")
            return 1
        else:
            print("\nğŸ’¥ æµ‹è¯•æ‰§è¡Œå‡ºç°é”™è¯¯ã€‚")
            return 2
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æµ‹è¯•å·²è¢«ç”¨æˆ·ä¸­æ–­")
        return 0
    except Exception as e:
        logger.error(f"æµ‹è¯•å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")
        print(f"âŒ æµ‹è¯•å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}")
        return 2


if __name__ == "__main__":
    # è¿è¡ŒçœŸå®ç¯å¢ƒæµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code)